{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p>Field sequencing outside a lab environment has been made possible by Oxford Nanopore technologies. Combining the portable sequencer called 'MinION Mk1B' and a powerfull laptop (incl. GPU power) one is able to perform in field sequencing and real-time data analysis. The idea of this website is to setup a portable field sequencing workflow covering following topics:  </p> <ul> <li>GPU system setup  </li> <li>Installing software (including ONT sequencing software)  </li> <li>Using <code>BLAST</code>and different databases for taxonomical ID  </li> <li>Wet lab approach for barcoding applications</li> <li>Demo of our field sequencing test at Botanic Garden Ghent  </li> </ul> <p>This is a step by step guide that will empower you to set up a compute workstation and learn all necessary protocols for both wet and dry lab.</p>"},{"location":"Tutorial-ONTdata/","title":"Tutorial ONTdata","text":""},{"location":"Tutorial-ONTdata/#intro","title":"Intro","text":"<p>At UGent Botanic Garden we organized a field sequencing experiment. The setup was to mimick field conditions i.e. sampling fungi, DNA extraction and PCR on the bentolab followed by live sequencing and basecalling on the mk1B from Oxford nanopore. Below you'll find an outline of the data analysis part of this setup including downloadlinks to the data icluding runcommands, so you can rerun the analysis and practice at your own pace.  </p>"},{"location":"Tutorial-ONTdata/#the-data","title":"The data","text":"<p>The original data file consists of reads for 27 accessions collected from the area around Campus Sterra at UGhent. All the QC examples below are taken from this data set. If you want to test the workflow I have provided a toy data set with reads for 3 accessions. You can download the toy-data here: <pre><code>wget https://github.com/passelma42/Field-Sequencing/raw/main/toy-data.tar.gz\n</code></pre></p> <p>Notes</p> <p>The example is ~20MB and contains reads for 2 samples: Barcode41 and Barcode63. Analysis time was 30 minutes on i7 Intel, 8 core, 16Gb Ram computer.</p>"},{"location":"Tutorial-ONTdata/#the-sequencing-run","title":"The Sequencing Run","text":"<p>Method: Rapid Barcoding SQK-RBK114.96 Data Analysis: wf-amplicon EPI2ME </p> <p>Notes</p> <p>The data and output described below was generated on the full data set. Because this is too large, I cannot share it on this platform. When you run the toy-data though you'll get for the analyzed barcodes the same output as you'll find them in the below plots.  </p> <p>We chose the RBK114 kit because this kit is best applicable in the field if you don't have access to a freezer or fridge. The 'Field sequencing kit' from nanopore is legacy at this point (2024) and was replaced by the Rapid Sequencing kits. In this post you can find results of a stability experiment of the kit kept at ambient temperature.</p> <p>The sequencing data folder </p> <pre><code>20240506_1258_MN35631_ASX408_61292243/\n        \u251c\u2500\u2500 barcode_alignment_ASX408_61292243_dc2466ae.tsv\n        \u251c\u2500\u2500 fastq_fail\n        \u251c\u2500\u2500 fastq_pass\n        \u251c\u2500\u2500 fastq_pass.tar.gz\n        \u251c\u2500\u2500 final_summary_ASX408_61292243_dc2466ae.txt\n        \u251c\u2500\u2500 other_reports\n        \u251c\u2500\u2500 pod5_fail\n        \u251c\u2500\u2500 pod5_pass\n        \u251c\u2500\u2500 pore_activity_ASX408_61292243_dc2466ae.csv\n        \u251c\u2500\u2500 report_ASX408_20240506_1302_61292243.json\n        \u251c\u2500\u2500 report_ASX408_20240506_1302_61292243.md\n        \u251c\u2500\u2500 sample_sheet_ASX408_20240506_1302_61292243.csv\n        \u251c\u2500\u2500 sequencing_summary_ASX408_61292243_dc2466ae.txt\n        \u2514\u2500\u2500 throughput_ASX408_61292243_dc2466ae.csv\n</code></pre> <p>Fastq_pass This folder is where you'll find your sequence reads. Basecalling at SUP means that all reads in this folder are q10 or higher.  </p> <pre><code>20240506_1258_MN35631_ASX408_61292243/fastq_pass/\n            \u251c\u2500\u2500 barcode41\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_0.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_1.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_10.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_11.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_12.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_13.fastq.gz\n            \u251c\u2500\u2500 barcode42\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_0.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_1.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_10.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_11.fastq.gz\n</code></pre>"},{"location":"Tutorial-ONTdata/#qc-sequence-run","title":"QC sequence run","text":"<p>Validating your sequencerun requires you to look at a lot of data. Max throughput of a minion flowcell is 50Gbases! Not hard to imagine that doing this manually is not the way to go. For this job we need dedicated sotware! There are different tools for visualizing HTS data. Nanopack is such a tool, opzimized for longread data. For validating the sequence run we can use the sequencing_summary_ASX408_61292243_dc2466ae.txt file which is generated by Minknow.  </p>"},{"location":"Tutorial-ONTdata/#tools-and-commands","title":"Tools and Commands","text":"<p>Tool: Nanopack Nanoplot Command<pre><code>NanoPlot --summary sequencing_summary_ASX408_61292243_dc2466ae.txt -o summary-plots\n</code></pre> Output </p> <p>NanoPlot gives you a set of .png plots, .txt statistics and also a handy interactive Report.html file. We'll take a closer look at some of the output to get grips on what we have sequenced.  </p> Metric Value Active channels 88.0 Mean read length 527.2 Mean read quality 12.2 Median read length 478.0 Median read quality 15.3 Number of reads 499,919.0 Read length N50 594.0 STDEV read length 2,745.3 Total bases 263,532,800.0 &gt;Q5 492,143 (98.4%), 252.2Mb &gt;Q7 472,784 (94.6%), 238.4Mb &gt;Q10 434,979 (87.0%), 218.4Mb &gt;Q12 392,967 (78.6%), 198.9Mb &gt;Q15 266,363 (53.3%), 137.8Mb <p>Number of reads over time </p> <p>Active pores over time </p> <p>Quality VS sequencing speed </p> <p>Time vs Speed </p> <p>Quality VS length log scaled (min 200bp - max 1000bp) </p> <p>Summary From these plots and stats we can confirm that we have nearly 500.000 reads with 87% of the reads Q10 and higher. Be aware that the RB114 kit performs tagmentation to add barcodes and adapers hence the 'low' mean read length. FYI our amplicons designed with ITS1f-ITS are about 900bp of length.  </p>"},{"location":"Tutorial-ONTdata/#the-data-analysis","title":"The Data Analysis","text":""},{"location":"Tutorial-ONTdata/#qc-data","title":"QC data","text":"<p>In the previous chapter we performed a QC on the sequencerun to validate the success of failure of the minion sequencerun. Now it is time to have a deeper look because we want to know how well our samples have performed. For this experiment we did ITS pcr on 27 samples collected at campus Sterre at UGhent. These samples have been assigned a barcode during the libraryprep and multiplexed into 1 single library to be sequenced. This means the data has to be demultiplexed, each unique Barcode sequence is retreived bioinformatically and assigned to a designated barcode folder (see topology above). Equimolar pooling of our barcodes during the libraryprep allows for an equal amount of reads per barcode. At least in theory. In real life, 'Numbers tell the table', we are going to measure this. We will again use the nanopack tool but now instead of the NanoPlot command we'll use the NanoComp command. In this way all the barcode stats will lign up in a nice overview. We will also need to prep the data a little bit and run this command in a 'for loop' script. We don't want to tire ourselves too much. If this doesn't ring a bell, don't care too much, just follow the instructions below.</p> <p>Tool: Nanopack Script: NanoComp-allfastq.sh </p> <ol> <li>Download the script. The script will loop over your barcode folders containing the demultiplexed fastq files. Download and test the script<pre><code>$ wget https://raw.githubusercontent.com/passelma42/scrippets/main/nanocomp-allfastq.sh\n$ chmod u+x nanocomp-allfastq.sh        # Set run permissions for the user\n$ ./nanocomp-allfastq.sh -h             # Display help function\n</code></pre></li> <li>Run NanoComp over your samples The script will concatenate all fastq files per barcode folder and run nanocomp. After the analysis has finished you'll find a .nanocompout/ folder inside the input folder you issued in the command below (under the -d flag). Command<pre><code>./nanocomp-allfastq.sh -d ./fastq_pass/ -c\n</code></pre> Output<pre><code>(nanopack) passelma@grover:~/Analyses/wf-amplicon/ONT-field2-greenhouse/benin-field2-greenhouse-run/testrunQC/fastq_pass/.nanocompout$ tree\n.\n\u251c\u2500\u2500 NanoComp-report.html\n\u251c\u2500\u2500 NanoComp_20240823_1521.log\n\u251c\u2500\u2500 NanoComp_N50.html\n\u251c\u2500\u2500 NanoComp_N50.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Normalized.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Normalized.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Weighted.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Weighted.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Normalized.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Normalized.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Weighted.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Weighted.png\n\u251c\u2500\u2500 NanoComp_lengths_violin.html\n\u251c\u2500\u2500 NanoComp_lengths_violin.png\n\u251c\u2500\u2500 NanoComp_log_length_violin.html\n\u251c\u2500\u2500 NanoComp_log_length_violin.png\n\u251c\u2500\u2500 NanoComp_number_of_reads.html\n\u251c\u2500\u2500 NanoComp_number_of_reads.png\n\u251c\u2500\u2500 NanoComp_quals_violin.html\n\u251c\u2500\u2500 NanoComp_quals_violin.png\n\u251c\u2500\u2500 NanoComp_total_throughput.html\n\u251c\u2500\u2500 NanoComp_total_throughput.png\n\u2514\u2500\u2500 NanoStats.txt\n</code></pre> NanoComp Number of reads per barcode Amongst other plots and reports we will have a look at number of reads per barcode. Although all samples have been equimolary pooled, we can already see that BC063 is underrepresented. If only this won't give us coverage issues in downstream applications :-o ?? Take a closer look at the file NanoComp-report.html for more details on other stats.  NanoComp_quals_violin In this violin plot we can confirm all our reads per barcode are of good quality. All the reads are Q10 and above. Except again BC63 is acting up... . </li> </ol>"},{"location":"Tutorial-ONTdata/#build-a-consensus","title":"Build a consensus","text":"<p>Now that we verified that we have sufficient reads per barcode and they are of good quality, it is time to start with our downstream analysis. It is time to get to buisiness! Off course there are many workflows to have a go at this type of data, but in this tutorial we will be showcasing the EPI2ME tool called wf-amplicon.. All necessary info on how to setup the necessary software and tools to run this workflow can be found on the 'installation' page of this website. In this section we will walk you through the process on how use the tool to build your consensus sequences from your raw nanopore reads.  </p> <p>Tool: wf-amplicon </p> <p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo. Running the tool in 'variant' mode it is possible to run more than one target (in this case genespecific amplicon) for the same species. But for this tutorial we will run in the de novo mode.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p> <p>Run the workflow The workflow can handle different folder layouts. If you followed the above sections of this tutorial, we are working with structure (iii).  data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre></p> <ol> <li> <p>Prepare a sample sheet This is a comma delimited file with 3 (or 4 if you run in variant mode) sections. The sample sheet is a CSV file with, minimally, columns named barcode and alias. Extra columns are allowed. In variant calling mode, a ref column can be added to tell the workflow which reference sequences should be used for which sample Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode01,BC01,test_sample,ref1\nbarcode02,BC02,test_sample\nbarcode03,BC03,test_sample,ref2\n</code></pre></p> </li> <li> <p>Prepare reference file when running variant mode When using a reference file called reference.fa it need to look like this. Each ref sequence would the represent a genespecific consensus sequence.  This reference will then be used to fish for sequences in your reads to catch and separate per gene. For instance if you amplified ITS and rpb2, mix them per sample and used this as a template to build your library, this tool could give you 2 consensus sequences per species: one ITS consensus and one rpb2 consensus. Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> </li> <li>Run the wf-amplicon on your samples The command below will run a singularity container in a nextflow run. The container beeing epi2me-labs/wf-amplicon and the command flags issued by the nextflow tool. Rember you have some prepping to do to get your system up and running to be able to run EPI2ME workflows. But believe me, this way is more easier than installing the tools yourself and writing designated scripts to stitch together all data input and oututs to feed into the next program. Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\\n    -profile singulairy\n</code></pre><pre><code>--fastq: Path to Input folder which holds fastq files or barcode directories  \n--reference: Including this line invoces running in variant mode, delete if you want to run in de novo mode  \n--sample_sheet: Path to sample sheet file  \n-c: path to config file, see 'Installation' section of this website  \n-profile: We are running the container under the singularity flag, not docker because docker doesn't play nice with HPC\n</code></pre> </li> </ol> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Make sure to delete the ref column in the sample sheet and leave out the --reference flag from the command in that case.  </p>"},{"location":"Tutorial-ONTdata/#get-your-data-out-of-here","title":"Get your data out of here","text":"<p>Once the workflow has finished, the output can be found in a folder called output-wf-apmlicon. Folder layout<pre><code>.\n\u251c\u2500\u2500 output-wf-amplicon\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 barcode41\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 alignments\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned.sorted.bam\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 aligned.sorted.bam.bai\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 consensus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 consensus.fastq\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 barcode42\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 alignments\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned.sorted.bam\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 aligned.sorted.bam.bai\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 consensus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 consensus.fastq\n</code></pre> Offcourse it would be too much to ask if the consensus sequence is given to us in a nice way i.e. having a sensible discriptive name like barcode41.consensus.fastq. Or something similar. \"Alas poor Yorick\". We'll have to fidle ourselves. Script: rename-consensus.sh This script will digg into each individual barcode folder and rename the consensus.fastq file accordingly and finally copy it to output-wf-ampmicon folder. Isn't that neat!  </p> <ol> <li>Download the script and set runpermissions <pre><code>wget https://raw.githubusercontent.com/passelma42/scrippets/main/rename-consensus.sh\nchmod u+x rename-consensus.sh       # Set run permissions for the user\n</code></pre></li> <li>Execute the script <pre><code>./rename-consensus.sh\n</code></pre></li> </ol> <p>Warning</p> <p>Verify you run the script from the output-wf-amplicon folder  </p> <p>Layout folder after rename script<pre><code>/output-wf-amlicon-nosubsample$ tree -L 1\n    .\n    \u251c\u2500\u2500 barcode41\n    \u251c\u2500\u2500 barcode41.consensus.fastq\n    \u251c\u2500\u2500 barcode42\n    \u251c\u2500\u2500 barcode42.consensus.fastq\n</code></pre> This final output gives you one consensus fastq per barcode which now you can use for blast or other alignment tools, phylogenetics,... .  </p>"},{"location":"blast/","title":"Blast","text":""},{"location":"blast/#blast-cli","title":"Blast-CLI","text":""},{"location":"blast/#intro","title":"Intro","text":"<p>The BLAST algorithm, which stands for Basic Local Alignment Search Tool, is a widely used and highly effective algorithm in bioinformatics and computational biology. It is used to compare biological sequences, such as DNA, RNA, or protein sequences, to identify regions of similarity or homology. The primary goal of the BLAST algorithm is to find local alignments between a query sequence and a database of sequences, which can help researchers identify genes, functional domains, and evolutionary relationships between sequences. Blast can be run at the NCBI website. Or a Command Line Interface can be downloaded to your own system.</p> <p>Here are the key components and steps of the BLAST algorithm:  </p> <ol> <li>Query Sequence: BLAST starts with a query sequence provided by the user, which is the sequence you want to compare against a database of other sequences.  </li> <li>Database: BLAST searches against a database containing a collection of sequences. This can be a database of known genes, genomes, proteins, or any other relevant biological sequences.  </li> <li>Scoring System: BLAST uses a scoring system to assign a numerical score to alignments between the query sequence and sequences in the database. Common scoring systems include the substitution matrix (e.g., BLOSUM or PAM matrices) for protein sequences and scoring schemes like match/mismatch scores and gap penalties for nucleotide sequences.  </li> <li>Seed Search: BLAST starts by identifying short, exact matches (seeds) between the query sequence and sequences in the database. These seeds serve as starting points for potential alignments.  </li> <li>Extension: Once seeds are identified, BLAST extends these seed matches in both directions along the sequences, using dynamic programming techniques to find the optimal alignment that maximizes the alignment score. The algorithm also considers gap penalties to account for insertions and deletions.  </li> <li>Scoring and Filtering: BLAST scores the extended alignments based on the chosen scoring system. It then applies a statistical significance threshold (usually based on E-values) to filter out alignments that could occur by chance.  </li> <li>Reporting Hits: BLAST reports the significant alignments, known as \"hits,\" to the user. These hits represent regions of similarity between the query sequence and sequences in the database.  </li> </ol> <p>BLAST is highly configurable, allowing users to adjust parameters such as the scoring system, gap penalties, and statistical significance thresholds to customize the search based on their specific needs. There are different versions of BLAST, including BLASTn (for nucleotide sequences), BLASTp (for protein sequences), BLASTx (translating nucleotide sequences to proteins), and more, each tailored to different types of sequence data.  </p> <p>Overall, BLAST is an essential tool for sequence comparison and is widely used in genomics, proteomics, and other areas of biological research. It enables researchers to quickly and effectively identify similarities between sequences and gain insights into the functional and evolutionary relationships of biological molecules.  </p> <p>Check out the NCBI-blast page for more information (and online applications)</p>"},{"location":"blast/#local-blast","title":"Local Blast","text":"<p>For more info take a look at the Download section @ NCBI website.</p>"},{"location":"blast/#installation","title":"Installation","text":"<p>Running blast form the webinterface has it's limitations. If you want to include your blast analysis in your own pipelines, Blast+ is the way to go. You can download the Blast+ executables and install them on your own machine.  </p> <ol> <li>Download the latest executable</li> <li>Unpack the download file: <pre><code>tar xzvf yourfile.tar.gz\n</code></pre> NOTE: It is always good pracktice to install bioinformatic software (if installed from source) to a designated bioinfo folder i.e. /usr/local/bionfo.           From here you can create a symlink to a folder in your $PATH f.i. /usr/local/bin</li> </ol>"},{"location":"blast/#database-download","title":"Database Download","text":"<p>BLAST databases are updated daily and may be downloaded via FTP. Database sets may be retrieved automatically with update_blastdb.pl, which is part of the BLAST+ suite. (run this command in working directory, probably best in same directory at your $PATH of the database.)</p> <p>Please refer to the BLAST database documentation for more details.</p> <ol> <li>Pre-fromatted BLAST db are archived here</li> <li>Download all numbered files for your database of choice using the basename (eg nt.000.tar.gz =&gt; \"nt\"):       Each of these files represents a subset (volume) of that database,       and all of them are needed to reconstitute the database.</li> <li>Extract: <pre><code>for file in *.gz; do tar -xzvf \"$file\"; done\n</code></pre></li> <li>After extraction, there is no need to concatenate the resulting files:       Call the database with the base name, for nr database files, use \"-db nt\".</li> </ol> <p>NOTE: For easy download, use the update_blastdb.pl script from the blast+ package.      Run following command in your db folder: <pre><code>perl update_blastdb.pl --decompress nt # or any other prefix for you db of coice\n</code></pre> NOTE: if the ncbi server runs slow, the update command could rise connection issues.   Sometimes it is easier to download manually from the ftp db website: <pre><code>wget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz\nwget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz.md5\n</code></pre></p>"},{"location":"blast/#build-local-db","title":"Build Local DB","text":"<p>If you want to build your own database based on fasta sequences take a look here for more details. Example<pre><code>$ makeblastdb -in test.fsa -parse_seqids -blastdb_version 5 -taxid_map test_map.txt -title \"Cookbook demo\" -dbtype prot\n\n\nBuilding a new DB, current time: 02/06/2019 17:08:14\nNew DB name:   test.fsa\nNew DB title:  Cookbook demo\nSequence type: Protein\nKeep MBits: T\nMaximum file size: 1000000000B\nAdding sequences from FASTA; added 6 sequences in 0.00222588 seconds.\n$\n</code></pre></p>"},{"location":"blast/#configuration","title":"Configuration","text":"<p>To set the env for blastn you can make a configuration file named .ncbirc (on Unix-like platforms) or ncbi.ini (on Windows) in your home directory  </p> <pre><code>touch ~/.ncbirc &amp;&amp;\necho [BLAST] &gt;&gt; ~/.ncbirc\necho [BLASTDB]=/path/to/databases &gt;&gt; ~/.ncbirc\n</code></pre> <p>More available parameters can be set in the config file. More detailed info on configurateion can be found here </p>"},{"location":"blast/#command-line","title":"Command line","text":""},{"location":"blast/#quick-run","title":"Quick run","text":"<pre><code>blastn \u2013db nt \u2013query nt.fsa \u2013out results.out\n</code></pre>"},{"location":"blast/#personalized-outputformat","title":"Personalized outputformat","text":"<pre><code>blastn -db nt \\ # nt is the name (prefix) you gave your db\n  -query /path/to/fastafiles \\\n  -out /path/to/output/out \\\n    -max_target_seqs 5 \\\n  -outfmt \"\"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore staxids sscinames sskingdoms qcovs\" \\\n  -num_trheads $(THREADS)\n</code></pre> <ul> <li> <p><code>1. qseqid</code>: query or source (gene) sequence id</p> </li> <li> <p><code>2. sseqid</code>: subject or target (reference genome) sequence id</p> </li> <li> <p><code>3. pident</code>: percentage of identical positions</p> </li> <li> <p><code>4. length</code>: alignment length (sequence overlap)</p> </li> <li> <p><code>5. mismatch</code>: number of mismatches</p> </li> <li> <p><code>6. gapopen</code>: number of gap openings</p> </li> <li> <p><code>7. qstart</code>: start of alignment in query</p> </li> <li> <p><code>8. qend</code>: end of alignment in query</p> </li> <li> <p><code>9. sstart</code>: start of alignment in subject</p> </li> <li> <p><code>10. send</code>: end of alignment in subject</p> </li> <li> <p><code>11. evalue</code>: expect value</p> </li> <li> <p><code>12. bitscore</code>: bit score</p> </li> </ul>"},{"location":"blast/#setup-blast-hpc-ugent","title":"Setup Blast @ HPC-UGent","text":"<p>Blast modules are available on the HPC.  Databases on the other hand are installed 'localy. ALERT!: don't install databases locally, they take up a lot of space. Please contact Pieter if you need a database which is not installed yet.</p>"},{"location":"blast/#db-location","title":"DB location","text":"<p>A dedicated folder is available for our VO where we can 'locally' make pre installed databases available for us all.  If you need another database, please avoid installing it into your own account folder, it will take up a lot of space and others who are part of our VO cannot access.  </p> <p>Location: /data/gent/vo/001/gvo00142/data_share_group/databases_blast/  Configuration db: Create a ncbirc file (see above for instructions) and save in your home folder (/user/gent/433/yourvscnumber)  </p> <p>Alternative: add this line to your .bashrc file:  </p> <pre><code>export BLASTDB=/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre>"},{"location":"blast/#updates","title":"Updates","text":"<p>Instllation dates are always part of the folder where you can find the databasefiles, if you feel it is not up to date, give a shout out to Pieter or make an update yourself if you know how to. When you need another type of db wchich you frequently use and others might benefit from, ask for a local installation.  </p>"},{"location":"blast/#usage","title":"Usage","text":"<ol> <li>Make sure your env value for $BLASTDB is set correctly. <pre><code>vsc43352@node3206:~$echo $BLASTDB                                                    \n/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre></li> <li>Use ml spider blast to list the installed blast modules</li> <li>Use the latest module installed in your jobscript</li> <li>run blast following the blast module (for instpiration look to section Quick run above)      or RTFM.</li> </ol>"},{"location":"github/","title":"Github","text":"<p>For a more in depth overview go to the official documentation.  </p>"},{"location":"github/#1-setting-up-a-local-git-repository","title":"1. Setting Up a Local Git Repository","text":"<ol> <li> <p>Initialize a new Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\n</code></pre> <ul> <li><code>cd /path/to/your/project</code>: Change directory to your project folder.</li> <li><code>git init</code>: Initialize a new Git repository in the current directory.</li> </ul> </li> <li> <p>Add files to the repository:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all files in the current directory for the next commit.</li> </ul> </li> <li> <p>Commit the files:</p> <pre><code>git commit -m \"Initial commit\"\n</code></pre> <ul> <li><code>git commit -m \"Initial commit\"</code>: Commit the staged files with a message.</li> </ul> </li> </ol>"},{"location":"github/#2-authenticating-using-ssh-key","title":"2. Authenticating Using SSH Key","text":"<ol> <li> <p>Generate an SSH key pair (if you don't have one):</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <ul> <li><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"</code>: Generate a new SSH key with RSA encryption.</li> </ul> </li> <li> <p>Add the SSH key to your SSH agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <ul> <li><code>eval \"$(ssh-agent -s)\"</code>: Start the SSH agent.</li> <li><code>ssh-add ~/.ssh/id_rsa</code>: Add your SSH private key to the SSH agent.</li> </ul> </li> <li> <p>Add the SSH key to your GitHub/GitLab/Bitbucket account:</p> <ul> <li> <p>Copy the SSH key to your clipboard:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> </li> <li> <p>Go to your Git hosting service and add the SSH key to your account settings.</p> </li> </ul> </li> </ol>"},{"location":"github/#3-pushing-the-local-repository-to-a-remote-repository","title":"3. Pushing the Local Repository to a Remote Repository","text":"<ol> <li> <p>Add the remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\n</code></pre> <ul> <li><code>git remote add origin git@github.com:username/repo.git</code>: Add a remote repository with the alias <code>origin</code>.</li> </ul> </li> <li> <p>Push the local repository to the remote repository:</p> <pre><code>git push -u origin main\n</code></pre> <ul> <li><code>git push -u origin main</code>: Push the local <code>main</code> branch to the <code>origin</code> remote repository and set it as the default upstream branch.</li> </ul> </li> </ol>"},{"location":"github/#4-making-changes-and-syncing-them","title":"4. Making Changes and Syncing Them","text":"<ol> <li> <p>Make changes to your files.</p> </li> <li> <p>Stage the changes:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all changed files for the next commit.</li> </ul> </li> <li> <p>Commit the changes:</p> <pre><code>git commit -m \"Describe your changes\"\n</code></pre> <ul> <li><code>git commit -m \"Describe your changes\"</code>: Commit the staged changes with a descriptive message.</li> </ul> </li> <li> <p>Push the changes to the remote repository:</p> <pre><code>git push\n</code></pre> <ul> <li><code>git push</code>: Push the committed changes to the remote repository.</li> </ul> </li> </ol>"},{"location":"github/#5-keeping-your-local-repository-in-sync-with-the-remote","title":"5. Keeping Your Local Repository in Sync with the Remote","text":"<ol> <li> <p>Fetch the latest changes from the remote repository:</p> <pre><code>git fetch origin\n</code></pre> <ul> <li><code>git fetch origin</code>: Fetch the latest changes from the <code>origin</code> remote repository.</li> </ul> </li> <li> <p>Merge the fetched changes into your local branch:</p> <pre><code>git merge origin/main\n</code></pre> <ul> <li><code>git merge origin/main</code>: Merge the fetched <code>main</code> branch into your local <code>main</code> branch.</li> </ul> </li> <li> <p>Pull the latest changes (fetch + merge):</p> <pre><code>git pull\n</code></pre> <ul> <li><code>git pull</code>: Fetch and merge the latest changes from the remote repository into your local branch.</li> </ul> </li> </ol>"},{"location":"github/#full-command-summary","title":"Full Command Summary","text":"<ol> <li> <p>Initialize Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> </li> <li> <p>Generate and add SSH key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub\n# Add the key to your Git hosting account settings\n</code></pre> </li> <li> <p>Add and push to remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\ngit push -u origin main\n</code></pre> </li> <li> <p>Make changes and push:</p> <pre><code># Make changes to your files\ngit add .\ngit commit -m \"Describe your changes\"\ngit push\n</code></pre> </li> <li> <p>Sync with remote repository:</p> <pre><code>git pull\n</code></pre> </li> </ol> <p>By following these steps and using the commands provided, you can effectively manage your local and remote Git repositories.</p>"},{"location":"installation/","title":"Installation","text":"<p>EPI2ME Labs maintains a collection of bioinformatics workflows tailored to Oxford Nanopore Technologies long-read sequencing data. They are curated and actively maintained by experts in long-read sequence analysis. Read all about EPI2ME workflows here.  </p>"},{"location":"installation/#installation-guide","title":"Installation guide","text":""},{"location":"installation/#java","title":"JAVA","text":"<pre><code>sudo apt-get update\nsudo apt-get install default-jre #install java on ubuntu\n</code></pre>"},{"location":"installation/#nextflow","title":"NEXTFLOW","text":"<p>Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages.  </p> <p>More information can be found on the NEXTFLOW WEBSITE. Download and install nextflow<pre><code>curl -s https://get.nextflow.io | bash # install nextflow\n</code></pre></p>"},{"location":"installation/#go","title":"GO","text":"<p>Singularity 3.0 is written primarily in Go, and you will need Go installed to compile it from source.  </p> <p>More information can be found on the Singularity installation guide website.  </p> <pre><code># Install\n    export VERSION=1.11 OS=linux ARCH=amd64 &amp;&amp; \\\n    wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    rm go$VERSION.$OS-$ARCH.tar.gz\n\n# setup GO environment\n    echo 'export GOPATH=${HOME}/go' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    source ~/.bashrc\n\n# install dep for dependency resolution\n    go get -u github.com/golang/dep/cmd/dep\n</code></pre>"},{"location":"installation/#singularity","title":"Singularity","text":"<p>Singularity is a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Singularity on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system and system.  </p> <p>More information can be found on the Singularity introduction page.  </p> <p>Installation <pre><code>export VERSION=3.0.3 &amp;&amp; # adjust this as necessary \\\n    mkdir -p $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    cd $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz &amp;&amp; \\\n    tar -xzf singularity-${VERSION}.tar.gz &amp;&amp; \\\n    cd ./singularity &amp;&amp; \\\n    ./mconfig\n</code></pre> Compile <pre><code>./mconfig &amp;&amp; \\\n    make -C ./builddir &amp;&amp; \\\n    sudo make -C ./builddir install\n</code></pre> Singularity cache folder </p> <p>Save storage space on your computer. When you run the singularity container (wf-amplicon for instance), a work directory will be created in your project folder. The wf-amplicon uses different containers without you noticing. All container images will be downloaded each time you run this wf. And this takes up space on your HD. </p> <p>To avoid this, you can create a cache folder where singularity can store these .img files and use these instead of downloding them each time you run the wf. The cache foler can then be specified in a nextflow.config file.  OR you can do as the error message expalains is to set the NXF_SINGULARITY_CACHEDI variable in you .bashrc file. Both options will be explained below.  </p> <p>OPTION 1: create nextflow.config file Follow these steps to do just this:  </p> <ol> <li> <p>Run the workflow for the 1st time     Run the workflow for the firs time and don't bother about this warning:</p> <p>Warning</p> <p>If the cache folder is not set you get this warning: WARN: Singularity cache directory has not been defined -- Remote image will be stored in the path: /home/path-to/work/singularity -- Use the environment variable NXF_SINGULARITY_CACHEDIR to specify a different location </p> </li> <li> <p>Locate the .img files     The wf-amplicon will create a <code>./work/singularity</code> folder in your projcet folder.      <pre><code>pieter@UGENT-PC:~/Analyses/wf-amplicon/work\n$ ls\n0f  14  24  2d  3f  46  49  54  5b  63  6d  77  7b  80  85  93  9b  9f  a1  a4  af  b5  bc  cc  d6  e5  e8  singularity\n11  1e  2b  3c  40  47  4c  56  5d  6b  71  78  7f  82  89  96  9c  a0  a3  ac  b0  b6  c8  d0  db  e6  ee\n(base)\npieter@UGENT-PC~/Analyses/wf-amplicon/work\n$ ls singularity/\nontresearch-medaka-sha61a9438d745a78030738352e084445a2db3daa2a.img       ontresearch-wf-common-sha91452ece4f647f62b32dac3a614635a6f0d7f8b5.img\nontresearch-wf-amplicon-sha3b6a08c2ed47efd85b17b98762951fffdeee5ba9.img\n</code></pre>     In this folder you'll find the <code>.img</code> files you need later.  </p> </li> <li> <p>Copy the <code>.img</code> files to a local folder     To avoid this create cache folder to store downloaded images when running epi2me labs. For each new analysis you run with epi2me make sure to put the image in this folder. <pre><code>mkdir /path/to/singularity-img\ncp ~/Analyses/wf-amplicon/work/singularity/*.img /path/to/singularity-img # this will copy all img files to your folder created in previou step, don't forget to personalize the paths  \n</code></pre> Create nextflow config file Now it is time to configure nextflow which is used to stitch all the singularity containers from wf-amplicon together. We will create a file that will be checked running each wf and redirects singularity to find the images stored locally rather than download them each time. More info on nextflow configuration file. Process: <pre><code>mkdir /home/sequencing/nextflow-config # /home/sequencing is in this case an example of your user's home directory. User is called *sequencer* in this example.  \ntouch /home/sequencing/nextflow-config/nextflow.config\n</code></pre> nextflow.config<pre><code>singularity { \n    enabled = true\n    cacheDir = /path/to/singularity-img\n} \n</code></pre></p> </li> </ol> <p>OPTION2: Set the environmental variable  </p> <p>The warning message indicates that the Singularity cache directory has not been set correctly, even though you specified it in your custom Nextflow configuration. To resolve this, you can explicitly set the Singularity cache directory using the <code>NXF_SINGULARITY_CACHEDIR</code> environment variable.</p> <p>Steps to Set the Singularity Cache Directory </p> <ol> <li>Set the Environment Variable:</li> <li>You can set the environment variable in your shell before running Nextflow. For example:      <pre><code>export NXF_SINGULARITY_CACHEDIR=/home/pieter/singularity-img\n</code></pre></li> <li>Set the Environment Variable Permanently (Optional):</li> <li>To make this change permanent, you can add the <code>export</code> command to your shell\u2019s configuration file (e.g., <code>~/.bashrc</code> or <code>~/.bash_profile</code> for bash):      <pre><code>echo \"export NXF_SINGULARITY_CACHEDIR=/home/pieter/singularity-img\" &gt;&gt; ~/.bashrc\n</code></pre></li> <li> <p>Reload your shell configuration:      <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>Verify the Configuration:</p> </li> <li>Run the pipeline again to ensure the warning disappears:      <pre><code>nextflow run epi2me-labs/wf-amplicon --fastq ./fastq --sample_sheet ./sample-sheet.csv --porechop --discard_middle --out_dir output-wf-amlicon-toydata-bis\n</code></pre></li> </ol> <p>This should direct Nextflow to use the specified cache directory for Singularity images, avoiding the warning and ensuring images are stored in your preferred location.</p>"},{"location":"installation/#wf-amplicon","title":"wf-amplicon","text":"<p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p>"},{"location":"installation/#run-the-workflow","title":"Run the workflow","text":"<p>data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre> Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\ #this can be omitted if you set the variable in your .bashrc\n    -profile singulairy\n</code></pre> Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode41,BC41,test_sample,ref1\nbarcode42,BC42,test_sample\nbarcode43,BC43,test_sample,ref2\n</code></pre> Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Also make sure to delete the ref column in that case.</p>"},{"location":"installation/#links","title":"Links","text":"<ul> <li>Installation EPI2ME</li> <li>Installation Nextflow </li> <li>Installation Singularity </li> <li>Installation Docker</li> <li>Installation GO</li> </ul>"},{"location":"linuxcommands/","title":"Linuxcommands","text":"<p>Copying data from one location to another on a Linux system can be done using several commands depending on your specific needs. Here are a few common methods:</p>"},{"location":"linuxcommands/#copy-data","title":"Copy data","text":""},{"location":"linuxcommands/#using-cp-command","title":"Using <code>cp</code> Command","text":"<p>The <code>cp</code> (copy) command is the most basic way to copy files and directories.</p>"},{"location":"linuxcommands/#copy-a-single-file","title":"Copy a Single File","text":"<pre><code>cp /path/to/source/file /path/to/destination/\n</code></pre> <p>Example: <pre><code>cp /home/user/file.txt /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#copy-a-directory","title":"Copy a Directory","text":"<p>To copy a directory and its contents, use the <code>-r</code> (recursive) option:</p> <pre><code>cp -r /path/to/source/directory /path/to/destination/\n</code></pre> <p>Example: <pre><code>cp -r /home/user/folder /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#using-rsync-command","title":"Using <code>rsync</code> Command","text":"<p>The <code>rsync</code> command is more advanced and powerful, often used for synchronizing directories and copying large amounts of data efficiently.</p>"},{"location":"linuxcommands/#basic-rsync-usage","title":"Basic <code>rsync</code> Usage","text":"<pre><code>rsync -av /path/to/source/ /path/to/destination/\n</code></pre> <ul> <li><code>-a</code>: Archive mode, preserves permissions, times, symbolic links, etc.</li> <li><code>-v</code>: Verbose, shows the progress of the transfer.</li> </ul> <p>Example: <pre><code>rsync -av /home/user/folder/ /home/user/Documents/folder/\n</code></pre></p>"},{"location":"linuxcommands/#using-mv-command","title":"Using <code>mv</code> Command","text":"<p>If you want to move (rather than copy) files or directories, use the <code>mv</code> command:</p>"},{"location":"linuxcommands/#move-a-single-file","title":"Move a Single File","text":"<pre><code>mv /path/to/source/file /path/to/destination/\n</code></pre> <p>Example: <pre><code>mv /home/user/file.txt /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#move-a-directory","title":"Move a Directory","text":"<pre><code>mv /path/to/source/directory /path/to/destination/\n</code></pre> <p>Example: <pre><code>mv /home/user/folder /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#examples-in-detail","title":"Examples in Detail","text":""},{"location":"linuxcommands/#copying-multiple-files","title":"Copying Multiple Files","text":"<p>You can copy multiple files by specifying them one after the other:</p> <pre><code>cp /home/user/file1.txt /home/user/file2.txt /home/user/Documents/\n</code></pre>"},{"location":"linuxcommands/#copying-all-files-with-a-specific-extension","title":"Copying All Files with a Specific Extension","text":"<p>Use wildcards to copy all files with a certain extension:</p> <pre><code>cp /home/user/*.txt /home/user/Documents/\n</code></pre>"},{"location":"linuxcommands/#additional-tips","title":"Additional Tips","text":"<ul> <li>Use <code>cp -i</code> to prompt before overwriting files.</li> <li>Use <code>cp -u</code> to copy only when the source file is newer than the destination file or when the destination file is missing.</li> <li>Use <code>cp -p</code> to preserve the file attributes like mode, ownership, and timestamps.</li> </ul>"},{"location":"linuxcommands/#combining-commands","title":"Combining Commands","text":"<p>You can combine these commands with other Linux utilities for more complex tasks. For example, using <code>find</code> to copy files based on specific criteria:</p> <pre><code>find /home/user/source/ -name \"*.txt\" -exec cp {} /home/user/destination/ \\;\n</code></pre> <p>This command finds all <code>.txt</code> files in the source directory and copies them to the destination directory.</p> <p>These commands and options should cover most of your needs for copying data on a Linux system. If you have specific requirements or run into issues, feel free to ask for more details.</p>"},{"location":"nanopore-tools/","title":"Nanopore Tools","text":"<p>All nanopore proprietary software can be found in the software &amp; Download section on the website of nanopore.   </p>"},{"location":"nanopore-tools/#sequencing-minkow","title":"Sequencing: Minkow","text":"<p>The basic setup for sequencing in the software package called <code>Minknow</code>. Minknow can both operate the sequencing device as well as running real time basecalling. The installation guide provided in the the software &amp; Download section gives you an overview of all necessary commands to issue for installing the software.  </p>"},{"location":"nanopore-tools/#default-installation-directories","title":"Default installation directories","text":"<ol> <li>For the MinKNOW software: <code>/opt/ont/minknow</code> </li> <li>For the MinKNOW user interface: <code>/opt/ont/minknow-ui</code> </li> <li>Location of the reads folder: <code>/var/lib/minknow/data</code> </li> <li>Location of the log files: The MinKNOW logs are located in <code>/var/log/minknow</code> The Dorado basecaller logs are located in <code>/var/log/dorado</code> </li> </ol>"},{"location":"nanopore-tools/#run-minknow-as-root","title":"Run Minknow as root","text":"<p>When running minknow you'll notice that when issuing another path for your reads folder, minkow will issue an error. Redirecting output from the standard <code>/var/lib/minknow/data</code> might be necessary when you run out of diskspace or simply when you prefer to output your data elsewhere. To enable this, you'll need to give the <code>minknow.service</code> writing privileges i.e. make this root. Make minknow.service root<pre><code>        $ sudo service minknow stop\n        $ sudo perl -i -pe 's/(User|Group)=minknow/$1=root/'/lib/systemd/system/minknow.service\n        $ sudo systemctl daemon-reload\n        $ sudo service minknow start\n</code></pre></p>"},{"location":"nanopore-tools/#basecalling-dorado","title":"Basecalling: Dorado","text":"<p>Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads. The tool can be installed as a stand alone tool which provides more functionalities than basecalling in Minknow. You can find the Dorado github repo here.  </p>"},{"location":"nanopore-tools/#dorado-features","title":"Dorado features","text":"<ol> <li>Download model modules are not packed in the software <pre><code>dordado download --list\n</code></pre></li> <li>Basecaller <pre><code>dorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ &gt; output.bam\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-fastq &gt; output.fastq\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-sam &gt; output.sam\n</code></pre></li> <li>Aligner <pre><code>dorado basecaller -x cuda:0 --reference .your-reference.fasta dna_r10.4.1_e8_400bps_fast@v4.1.0 ./pod5 &gt; aligned.bam\n</code></pre></li> <li>Sequencing summary generation <pre><code>dorado summary ./reads.bam &gt; sequencing_summary.txt\n</code></pre></li> </ol>"},{"location":"system-build/","title":"System setup","text":""},{"location":"system-build/#operating-system","title":"Operating system","text":"<p>Release: Ubuntu 22.04.4 LTS (Jammy Jellyfish) Follow this link to learn all you need to know on how to install ubuntu.</p>"},{"location":"system-build/#gpu-cuda","title":"GPU &amp; CUDA","text":"<p>GPU computational power increased ONT basecalling speed and enabled real-time data analysis. Upon writing this documentation dorado 0.7 is the current basecaller which is also incorperated in the sequencing operating software Minknow. Before we get to the installation of dedicated sequencing software and other data analys packages we will focus on installing a GPU and the CUDA toolkit.  </p>"},{"location":"system-build/#installation-on-ubuntu","title":"Installation on Ubuntu","text":"<ul> <li>Add graphics drivers ppa repo <pre><code>sudo add-apt-repository ppa:graphics-drivers/ppa\n</code></pre></li> <li>Install Ubuntu drivers app <pre><code>sudo apt install ubuntu-drivers-common\n</code></pre></li> <li>Check available GPUs <pre><code>ubuntu-drivers devices\n</code></pre> Example output<pre><code>== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==\nmodalias : pci:v000010DEd000027E0sv00001025sd0000166Cbc03sc00i00\nvendor   : NVIDIA Corporation\ndriver   : nvidia-driver-545-open - distro non-free\ndriver   : nvidia-driver-555-open - third-party non-free\ndriver   : nvidia-driver-550 - third-party non-free\ndriver   : nvidia-driver-550-open - third-party non-free\ndriver   : nvidia-driver-545 - distro non-free\ndriver   : nvidia-driver-535 - third-party non-free\ndriver   : nvidia-driver-555 - third-party non-free recommended\ndriver   : nvidia-driver-535-server - distro non-free\ndriver   : nvidia-driver-535-open - distro non-free\ndriver   : nvidia-driver-535-server-open - distro non-free\ndriver   : nvidia-driver-525 - third-party non-free\ndriver   : xserver-xorg-video-nouveau - distro free builtin\n</code></pre></li> <li>Install latest Nvidia driver (change <code>555</code> with latest version available in your case) <pre><code>sudo apt install nvidia-driver-525\n</code></pre></li> <li>Reboot your computer  </li> <li>Check if Nvidia driver and CUDA is available after reboot <pre><code>nvidia-smi\n</code></pre> Example output<pre><code>+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4080 ...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   45C    P8               1W /  90W |   1032MiB / 12282MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n</code></pre></li> </ul> <p>This output tells us we have a NVIDIA GeForce RTX 4080 running (12GB virtual RAM) and CUDA Version 12.3 has been co-installed.  </p>"},{"location":"system-build/#purge-nvidia","title":"Purge Nvidia","text":"<p>On some occasions you'll need to perform a fresh installation of nvidia drivers. It is important to purge all currently installed NVIDIA/CUDA before installing anew. Here is a step by step guide on how to do this:  </p> <ul> <li>List all packages The dpkg -l command is used in Debian-based Linux distributions to list all the installed packages. This command provides detailed information about each package, including its status, name, version, and a brief description. <pre><code>dpkg -l\n</code></pre> The output of dpkg -l, the columns represent:  </li> </ul> <ol> <li>Package status (e.g., \"ii\" for installed, \"un\" for uninstalled, etc.)  </li> <li>Package name  </li> <li>Version  </li> <li>Architecture  </li> <li>Description  </li> </ol> <ul> <li>List and purge all NVIDIA/CUDA  </li> </ul> <p><pre><code>sudo apt-get purge $(dpkg -l | grep '^ii.*nvidia'| awk '{print $2}')\n</code></pre> Description<pre><code>dpkg -l | grep '^ii.*nvidia'    # This command lists all installed packages (output of dpkg -l) that have \"nvidia\" in their names and are in the \"installed\" state (lines starting with \"ii\").  \n\nawk '{print $2}'                # This command uses awk to print the second column (package names) from the output of the previous command.  \n\nsudo apt-get purge $(...)       # This command uses command substitution to execute the inner command and pass its output (list of package names) as arguments to apt-get purge, which removes the specified packages.  \n</code></pre></p> <p>.</p>"},{"location":"system-build/#system-software","title":"System Software","text":"<p>On Ubuntu system applications can be installed using the graphical user interface (GUI). More information on how to install software on Ubuntu click here.  </p>"},{"location":"system-build/#bioinformatic-software","title":"Bioinformatic software","text":"<p>Usually bioinformatic software are opensource packages with installation instructions described on Github repositories. First thing you can do is get yourself a github account and get familiarized with github. Executable scripts are often found in the <code>/bin</code>folder, or can be installed in <code>/usr/local/bin</code> or in a dedicated bioinfo folder <code>/usr/local/bioinf/</code>. In the next section I will explain how to install the basecalling tool Dorado in the <code>/bioinf</code>folder and put the script on <code>PATH</code>.  </p> <ul> <li>Goto DORADO GITHUB </li> <li>Download the relevant installer for your platform (like described in the instructions)  </li> <li>Extract the archive to the desired location Extract and put on PATH<pre><code>1. mkdir /usr/local/bioinf/\n2. cd /usr/local/bioinf/\n3. wget https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.7.2-linux-x64.tar.gz\n4. tar -xzvf dorado-0.7.2-linux-x64.tar.gz\n    # after unzipping, the main folder will contain a /bin and /lib folder.\n5. cd bin  \n6. ls\n    # /usr/local/bioinf/dorado/bin/dorado =&gt; this is the Executable file \n7. sudo ln -s /usr/local/bioinf/dorado/bin/dorado /usr/local/bin/dorado\n    # this will symlink the dorado script to /usr/local/bin/dorado\n8. echo 'export PATH=$PATH:/usr/local/bin' | sudo tee -a /etc/profile &amp;&amp; export PATH=$PATH:/usr/local/bin\n    # put in path if not by default, you can check by `echo $PATH`\n</code></pre> Following these instructions you should have downloaded the dorado script and it's libraries in the designated <code>/usr/local/bioinf</code> folder. To make the executable file accessible for all users you need to put it in PATH.</li> </ul>"},{"location":"system-build/#python-environments","title":"Python environments","text":"<p>When you need to install packages using the command <code>pip</code> it is adviced to do so in a separate python environment to avoid issues with the already existing python installation supporting your operating system. Detailed information on python venv can be found here.  </p>"},{"location":"system-build/#conda","title":"Conda","text":"<p>Yet another packaging and environmanagement tool is Conda. A lot of bioinformatic software has been made available in dedicated conda environments. Read all there is to know on how to use conda here. </p>"}]}