{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p>Field sequencing outside a lab environment has been made possible by Oxford Nanopore technologies. Combining the portable sequencer called 'MinION Mk1B' and a powerfull laptop (incl. GPU power) one is able to perform in field sequencing and real-time data analysis. The idea of this website is to setup a portable field sequencing workflow covering following topics:  </p> <ul> <li>GPU system setup  </li> <li>Installing software (including ONT sequencing software)  </li> <li>Using <code>BLAST</code>and different databases for taxonomical ID  </li> <li>Wet lab approach for barcoding applications</li> <li>Demo of our field sequencing test at Botanic Garden Ghent  </li> </ul> <p>This is a step by step guide that will empower you to set up a compute workstation and learn all necessary protocols for both wet and dry lab.</p>"},{"location":"demo-field2.0/","title":"Demo field2.0","text":""},{"location":"demo-field2.0/#sequencing-basecalling","title":"Sequencing &amp; Basecalling","text":"<p>Method: Rapid Barcoding SQK-RBK114.96 Dorado model: dna_r10.4.1_e8.2_260bps_sup@v4.1.0</p>"},{"location":"demo-field2.0/#qc-data","title":"QC Data","text":""},{"location":"demo-field2.0/#commands","title":"Commands","text":"<p>Tool: Nanopack Nanoplot Command<pre><code>NanoPlot --summary sequencing_summary.txt -o summary-plots\n</code></pre></p>"},{"location":"demo-field2.0/#output","title":"Output","text":"Metric Value Active channels 88.0 Mean read length 527.2 Mean read quality 12.2 Median read length 478.0 Median read quality 15.3 Number of reads 499,919.0 Read length N50 594.0 STDEV read length 2,745.3 Total bases 263,532,800.0 &gt;Q5 492,143 (98.4%), 252.2Mb &gt;Q7 472,784 (94.6%), 238.4Mb &gt;Q10 434,979 (87.0%), 218.4Mb &gt;Q12 392,967 (78.6%), 198.9Mb &gt;Q15 266,363 (53.3%), 137.8Mb"},{"location":"demo-field2.0/#number-of-reads-vs-active-pores","title":"Number of reads VS active pores","text":"<p>Number of reads over time </p> <p>Active pores over time </p>"},{"location":"demo-field2.0/#quality-vs-sequencing-speed","title":"Quality VS sequencing speed","text":"<p>Time vs Quality </p> <p>Time vs Speed </p>"},{"location":"demo-field2.0/#quality-vs-length","title":"Quality VS length","text":"<p>Quality VS length (min 200bp - max 1000bp) </p>"},{"location":"demo-field2.0/#data-processing-create-consensus","title":"Data Processing: Create Consensus","text":""},{"location":"demo-field2.0/#wf-amplicon","title":"wf-amplicon","text":"<p>Tool: wf-amplicon </p> <p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p>"},{"location":"demo-field2.0/#run-the-workflow","title":"Run the workflow","text":"<p>data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre> Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\\n    -profile singulairy\n</code></pre> Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode41,BC41,test_sample,ref1\nbarcode42,BC42,test_sample\nbarcode43,BC43,test_sample,ref2\n</code></pre> Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Also make sure to delete the ref column in that case. </p>"},{"location":"demo-field2.0/#blast-cli","title":"Blast-CLI","text":""},{"location":"demo-field2.0/#intro","title":"Intro","text":"<p>The BLAST algorithm, which stands for Basic Local Alignment Search Tool, is a widely used and highly effective algorithm in bioinformatics and computational biology. It is used to compare biological sequences, such as DNA, RNA, or protein sequences, to identify regions of similarity or homology. The primary goal of the BLAST algorithm is to find local alignments between a query sequence and a database of sequences, which can help researchers identify genes, functional domains, and evolutionary relationships between sequences. Blast can be run at the NCBI website. Or a Command Line Interface can be downloaded to your own system.</p> <p>Here are the key components and steps of the BLAST algorithm:  </p> <ol> <li>Query Sequence: BLAST starts with a query sequence provided by the user, which is the sequence you want to compare against a database of other sequences.  </li> <li>Database: BLAST searches against a database containing a collection of sequences. This can be a database of known genes, genomes, proteins, or any other relevant biological sequences.  </li> <li>Scoring System: BLAST uses a scoring system to assign a numerical score to alignments between the query sequence and sequences in the database. Common scoring systems include the substitution matrix (e.g., BLOSUM or PAM matrices) for protein sequences and scoring schemes like match/mismatch scores and gap penalties for nucleotide sequences.  </li> <li>Seed Search: BLAST starts by identifying short, exact matches (seeds) between the query sequence and sequences in the database. These seeds serve as starting points for potential alignments.  </li> <li>Extension: Once seeds are identified, BLAST extends these seed matches in both directions along the sequences, using dynamic programming techniques to find the optimal alignment that maximizes the alignment score. The algorithm also considers gap penalties to account for insertions and deletions.  </li> <li>Scoring and Filtering: BLAST scores the extended alignments based on the chosen scoring system. It then applies a statistical significance threshold (usually based on E-values) to filter out alignments that could occur by chance.  </li> <li>Reporting Hits: BLAST reports the significant alignments, known as \"hits,\" to the user. These hits represent regions of similarity between the query sequence and sequences in the database.  </li> </ol> <p>BLAST is highly configurable, allowing users to adjust parameters such as the scoring system, gap penalties, and statistical significance thresholds to customize the search based on their specific needs. There are different versions of BLAST, including BLASTn (for nucleotide sequences), BLASTp (for protein sequences), BLASTx (translating nucleotide sequences to proteins), and more, each tailored to different types of sequence data.  </p> <p>Overall, BLAST is an essential tool for sequence comparison and is widely used in genomics, proteomics, and other areas of biological research. It enables researchers to quickly and effectively identify similarities between sequences and gain insights into the functional and evolutionary relationships of biological molecules.  </p> <p>Check out the NCBI-blast page for more information (and online applications)</p>"},{"location":"demo-field2.0/#local-blast","title":"Local Blast","text":"<p>For more info take a look at the Download section @ NCBI website.</p>"},{"location":"demo-field2.0/#installation","title":"Installation","text":"<p>Running blast form the webinterface has it's limitations. If you want to include your blast analysis in your own pipelines, Blast+ is the way to go. You can download the Blast+ executables and install them on your own machine.  </p> <ol> <li>Download the latest executable</li> <li>Unpack the download file: <pre><code>tar xzvf yourfile.tar.gz\n</code></pre> NOTE: It is always good pracktice to install bioinformatic software (if installed from source) to a designated bioinfo folder i.e. /usr/local/bionfo.           From here you can create a symlink to a folder in your $PATH f.i. /usr/local/bin</li> </ol>"},{"location":"demo-field2.0/#database-download","title":"Database Download","text":"<p>BLAST databases are updated daily and may be downloaded via FTP. Database sets may be retrieved automatically with update_blastdb.pl, which is part of the BLAST+ suite. (run this command in working directory, probably best in same directory at your $PATH of the database.)</p> <p>Please refer to the BLAST database documentation for more details.</p> <ol> <li>Pre-fromatted BLAST db are archived here</li> <li>Download all numbered files for your database of choice using the basename (eg nt.000.tar.gz =&gt; \"nt\"):       Each of these files represents a subset (volume) of that database,       and all of them are needed to reconstitute the database.</li> <li>Extract: <pre><code>for file in *.gz; do tar -xzvf \"$file\"; done\n</code></pre></li> <li>After extraction, there is no need to concatenate the resulting files:       Call the database with the base name, for nr database files, use \"-db nt\".</li> </ol> <p>NOTE: For easy download, use the update_blastdb.pl script from the blast+ package.      Run following command in your db folder: <pre><code>perl update_blastdb.pl --decompress nt # or any other prefix for you db of coice\n</code></pre> NOTE: if the ncbi server runs slow, the update command could rise connection issues.   Sometimes it is easier to download manually from the ftp db website: <pre><code>wget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz\nwget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz.md5\n</code></pre></p>"},{"location":"demo-field2.0/#build-local-db","title":"Build Local DB","text":"<p>If you want to build your own database based on fasta sequences take a look here for more details. Example<pre><code>$ makeblastdb -in test.fsa -parse_seqids -blastdb_version 5 -taxid_map test_map.txt -title \"Cookbook demo\" -dbtype prot\n\n\nBuilding a new DB, current time: 02/06/2019 17:08:14\nNew DB name:   test.fsa\nNew DB title:  Cookbook demo\nSequence type: Protein\nKeep MBits: T\nMaximum file size: 1000000000B\nAdding sequences from FASTA; added 6 sequences in 0.00222588 seconds.\n$\n</code></pre></p>"},{"location":"demo-field2.0/#configuration","title":"Configuration","text":"<p>To set the env for blastn you can make a configuration file named .ncbirc (on Unix-like platforms) or ncbi.ini (on Windows) in your home directory  </p> <pre><code>touch ~/.ncbirc &amp;&amp;\necho [BLAST] &gt;&gt; ~/.ncbirc\necho [BLASTDB]=/path/to/databases &gt;&gt; ~/.ncbirc\n</code></pre> <p>More available parameters can be set in the config file. More detailed info on configurateion can be found here </p>"},{"location":"demo-field2.0/#command-line","title":"Command line","text":""},{"location":"demo-field2.0/#quick-run","title":"Quick run","text":"<pre><code>blastn \u2013db nt \u2013query nt.fsa \u2013out results.out\n</code></pre>"},{"location":"demo-field2.0/#personalized-outputformat","title":"Personalized outputformat","text":"<pre><code>blastn -db nt \\ # nt is the name (prefix) you gave your db\n  -query /path/to/fastafiles \\\n  -out /path/to/output/out \\\n    -max_target_seqs 5 \\\n  -outfmt \"\"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore staxids sscinames sskingdoms qcovs\" \\\n  -num_trheads $(THREADS)\n</code></pre> <ul> <li> <p><code>1. qseqid</code>: query or source (gene) sequence id</p> </li> <li> <p><code>2. sseqid</code>: subject or target (reference genome) sequence id</p> </li> <li> <p><code>3. pident</code>: percentage of identical positions</p> </li> <li> <p><code>4. length</code>: alignment length (sequence overlap)</p> </li> <li> <p><code>5. mismatch</code>: number of mismatches</p> </li> <li> <p><code>6. gapopen</code>: number of gap openings</p> </li> <li> <p><code>7. qstart</code>: start of alignment in query</p> </li> <li> <p><code>8. qend</code>: end of alignment in query</p> </li> <li> <p><code>9. sstart</code>: start of alignment in subject</p> </li> <li> <p><code>10. send</code>: end of alignment in subject</p> </li> <li> <p><code>11. evalue</code>: expect value</p> </li> <li> <p><code>12. bitscore</code>: bit score</p> </li> </ul>"},{"location":"demo-field2.0/#setup-blast-hpc-ugent","title":"Setup Blast @ HPC-UGent","text":"<p>Blast modules are available on the HPC.  Databases on the other hand are installed 'localy. ALERT!: don't install databases locally, they take up a lot of space. Please contact Pieter if you need a database which is not installed yet.</p>"},{"location":"demo-field2.0/#db-location","title":"DB location","text":"<p>A dedicated folder is available for our VO where we can 'locally' make pre installed databases available for us all.  If you need another database, please avoid installing it into your own account folder, it will take up a lot of space and others who are part of our VO cannot access.  </p> <p>Location: /data/gent/vo/001/gvo00142/data_share_group/databases_blast/  Configuration db: Create a ncbirc file (see above for instructions) and save in your home folder (/user/gent/433/yourvscnumber)  </p> <p>Alternative: add this line to your .bashrc file:  </p> <pre><code>export BLASTDB=/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre>"},{"location":"demo-field2.0/#updates","title":"Updates","text":"<p>Instllation dates are always part of the folder where you can find the databasefiles, if you feel it is not up to date, give a shout out to Pieter or make an update yourself if you know how to. When you need another type of db wchich you frequently use and others might benefit from, ask for a local installation.  </p>"},{"location":"demo-field2.0/#usage","title":"Usage","text":"<ol> <li>Make sure your env value for $BLASTDB is set correctly. <pre><code>vsc43352@node3206:~$echo $BLASTDB                                                    \n/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre></li> <li>Use ml spider blast to list the installed blast modules</li> <li>Use the latest module installed in your jobscript</li> <li>run blast following the blast module (for instpiration look to section Quick run above)      or RTFM.</li> </ol>"},{"location":"github/","title":"Github","text":"<p>For a more in depth overview go to the official documentation.  </p>"},{"location":"github/#1-setting-up-a-local-git-repository","title":"1. Setting Up a Local Git Repository","text":"<ol> <li> <p>Initialize a new Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\n</code></pre> <ul> <li><code>cd /path/to/your/project</code>: Change directory to your project folder.</li> <li><code>git init</code>: Initialize a new Git repository in the current directory.</li> </ul> </li> <li> <p>Add files to the repository:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all files in the current directory for the next commit.</li> </ul> </li> <li> <p>Commit the files:</p> <pre><code>git commit -m \"Initial commit\"\n</code></pre> <ul> <li><code>git commit -m \"Initial commit\"</code>: Commit the staged files with a message.</li> </ul> </li> </ol>"},{"location":"github/#2-authenticating-using-ssh-key","title":"2. Authenticating Using SSH Key","text":"<ol> <li> <p>Generate an SSH key pair (if you don't have one):</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <ul> <li><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"</code>: Generate a new SSH key with RSA encryption.</li> </ul> </li> <li> <p>Add the SSH key to your SSH agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <ul> <li><code>eval \"$(ssh-agent -s)\"</code>: Start the SSH agent.</li> <li><code>ssh-add ~/.ssh/id_rsa</code>: Add your SSH private key to the SSH agent.</li> </ul> </li> <li> <p>Add the SSH key to your GitHub/GitLab/Bitbucket account:</p> <ul> <li> <p>Copy the SSH key to your clipboard:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> </li> <li> <p>Go to your Git hosting service and add the SSH key to your account settings.</p> </li> </ul> </li> </ol>"},{"location":"github/#3-pushing-the-local-repository-to-a-remote-repository","title":"3. Pushing the Local Repository to a Remote Repository","text":"<ol> <li> <p>Add the remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\n</code></pre> <ul> <li><code>git remote add origin git@github.com:username/repo.git</code>: Add a remote repository with the alias <code>origin</code>.</li> </ul> </li> <li> <p>Push the local repository to the remote repository:</p> <pre><code>git push -u origin main\n</code></pre> <ul> <li><code>git push -u origin main</code>: Push the local <code>main</code> branch to the <code>origin</code> remote repository and set it as the default upstream branch.</li> </ul> </li> </ol>"},{"location":"github/#4-making-changes-and-syncing-them","title":"4. Making Changes and Syncing Them","text":"<ol> <li> <p>Make changes to your files.</p> </li> <li> <p>Stage the changes:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all changed files for the next commit.</li> </ul> </li> <li> <p>Commit the changes:</p> <pre><code>git commit -m \"Describe your changes\"\n</code></pre> <ul> <li><code>git commit -m \"Describe your changes\"</code>: Commit the staged changes with a descriptive message.</li> </ul> </li> <li> <p>Push the changes to the remote repository:</p> <pre><code>git push\n</code></pre> <ul> <li><code>git push</code>: Push the committed changes to the remote repository.</li> </ul> </li> </ol>"},{"location":"github/#5-keeping-your-local-repository-in-sync-with-the-remote","title":"5. Keeping Your Local Repository in Sync with the Remote","text":"<ol> <li> <p>Fetch the latest changes from the remote repository:</p> <pre><code>git fetch origin\n</code></pre> <ul> <li><code>git fetch origin</code>: Fetch the latest changes from the <code>origin</code> remote repository.</li> </ul> </li> <li> <p>Merge the fetched changes into your local branch:</p> <pre><code>git merge origin/main\n</code></pre> <ul> <li><code>git merge origin/main</code>: Merge the fetched <code>main</code> branch into your local <code>main</code> branch.</li> </ul> </li> <li> <p>Pull the latest changes (fetch + merge):</p> <pre><code>git pull\n</code></pre> <ul> <li><code>git pull</code>: Fetch and merge the latest changes from the remote repository into your local branch.</li> </ul> </li> </ol>"},{"location":"github/#full-command-summary","title":"Full Command Summary","text":"<ol> <li> <p>Initialize Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> </li> <li> <p>Generate and add SSH key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub\n# Add the key to your Git hosting account settings\n</code></pre> </li> <li> <p>Add and push to remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\ngit push -u origin main\n</code></pre> </li> <li> <p>Make changes and push:</p> <pre><code># Make changes to your files\ngit add .\ngit commit -m \"Describe your changes\"\ngit push\n</code></pre> </li> <li> <p>Sync with remote repository:</p> <pre><code>git pull\n</code></pre> </li> </ol> <p>By following these steps and using the commands provided, you can effectively manage your local and remote Git repositories.</p>"},{"location":"installation/","title":"Installation","text":"<p>EPI2ME Labs maintains a collection of bioinformatics workflows tailored to Oxford Nanopore Technologies long-read sequencing data. They are curated and actively maintained by experts in long-read sequence analysis. Read all about EPI2ME workflows here.  </p>"},{"location":"installation/#installation-guide","title":"Installation guide","text":""},{"location":"installation/#java","title":"JAVA","text":"<pre><code>sudo apt-get update\nsudo apt-get install default-jre #install java on ubuntu\n</code></pre>"},{"location":"installation/#nextflow","title":"NEXTFLOW","text":"<pre><code>curl -s https://get.nextflow.io | bash # install nextflow\n</code></pre>"},{"location":"installation/#go","title":"GO","text":"<pre><code># Install\n    export VERSION=1.11 OS=linux ARCH=amd64 &amp;&amp; \\\n    wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    rm go$VERSION.$OS-$ARCH.tar.gz\n\n# setup GO environment\n    echo 'export GOPATH=${HOME}/go' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    source ~/.bashrc\n\n# install dep for dependency resolution\n    go get -u github.com/golang/dep/cmd/dep\n</code></pre>"},{"location":"installation/#singularity","title":"Singularity","text":"<p>Installation <pre><code>export VERSION=3.0.3 &amp;&amp; # adjust this as necessary \\\n    mkdir -p $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    cd $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz &amp;&amp; \\\n    tar -xzf singularity-${VERSION}.tar.gz &amp;&amp; \\\n    cd ./singularity &amp;&amp; \\\n    ./mconfig\n</code></pre> Compile <pre><code>./mconfig &amp;&amp; \\\n    make -C ./builddir &amp;&amp; \\\n    sudo make -C ./builddir install\n</code></pre> Singularity cache folder If folder is not set you get this warning:     WARN: Singularity cache directory has not been defined -- Remote image will be stored in the path: /home/sequencing/Analisys/wf-amplicon-demo/work/singularity -- Use the environment variable NXF_SINGULARITY_CACHEDIR to specify a different location</p> <p>To avoid this create cache folder to store downloaded images when running epi2me labs. For each new analysis you run with epi2me make sure to put the image in this folder. <pre><code>mkdir /path/to/singularity-cache-dir\n</code></pre> Create nextflow config file This file will be checked running each wf and redirects singularity to find the images stored locally rather than download them each time. More info on nextflow configuration file. Process: <pre><code>mkdir /home/sequencing/nextflow-config\ntouch /home/sequencing/nextflow-config/nextflow.config\n</code></pre> nextflow.config<pre><code>singularity { \n    enabled = true\n    cacheDir = /path/to/singularity-cache-dir\n} \n</code></pre></p>"},{"location":"installation/#wf-amplicon","title":"wf-amplicon","text":"<p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p>"},{"location":"installation/#run-the-workflow","title":"Run the workflow","text":"<p>data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre> Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\\n    -profile singulairy\n</code></pre> Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode41,BC41,test_sample,ref1\nbarcode42,BC42,test_sample\nbarcode43,BC43,test_sample,ref2\n</code></pre> Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Also make sure to delete the ref column in that case.</p>"},{"location":"installation/#links","title":"Links","text":"<ul> <li>Installation EPI2ME</li> <li>Installation Nextflow </li> <li>Installation Singularity </li> <li>Installation Docker</li> <li>Installation GO</li> </ul>"},{"location":"linuxcommands/","title":"Linuxcommands","text":"<p>Copying data from one location to another on a Linux system can be done using several commands depending on your specific needs. Here are a few common methods:</p>"},{"location":"linuxcommands/#copy-data","title":"Copy data","text":""},{"location":"linuxcommands/#using-cp-command","title":"Using <code>cp</code> Command","text":"<p>The <code>cp</code> (copy) command is the most basic way to copy files and directories.</p>"},{"location":"linuxcommands/#copy-a-single-file","title":"Copy a Single File","text":"<pre><code>cp /path/to/source/file /path/to/destination/\n</code></pre> <p>Example: <pre><code>cp /home/user/file.txt /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#copy-a-directory","title":"Copy a Directory","text":"<p>To copy a directory and its contents, use the <code>-r</code> (recursive) option:</p> <pre><code>cp -r /path/to/source/directory /path/to/destination/\n</code></pre> <p>Example: <pre><code>cp -r /home/user/folder /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#using-rsync-command","title":"Using <code>rsync</code> Command","text":"<p>The <code>rsync</code> command is more advanced and powerful, often used for synchronizing directories and copying large amounts of data efficiently.</p>"},{"location":"linuxcommands/#basic-rsync-usage","title":"Basic <code>rsync</code> Usage","text":"<pre><code>rsync -av /path/to/source/ /path/to/destination/\n</code></pre> <ul> <li><code>-a</code>: Archive mode, preserves permissions, times, symbolic links, etc.</li> <li><code>-v</code>: Verbose, shows the progress of the transfer.</li> </ul> <p>Example: <pre><code>rsync -av /home/user/folder/ /home/user/Documents/folder/\n</code></pre></p>"},{"location":"linuxcommands/#using-mv-command","title":"Using <code>mv</code> Command","text":"<p>If you want to move (rather than copy) files or directories, use the <code>mv</code> command:</p>"},{"location":"linuxcommands/#move-a-single-file","title":"Move a Single File","text":"<pre><code>mv /path/to/source/file /path/to/destination/\n</code></pre> <p>Example: <pre><code>mv /home/user/file.txt /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#move-a-directory","title":"Move a Directory","text":"<pre><code>mv /path/to/source/directory /path/to/destination/\n</code></pre> <p>Example: <pre><code>mv /home/user/folder /home/user/Documents/\n</code></pre></p>"},{"location":"linuxcommands/#examples-in-detail","title":"Examples in Detail","text":""},{"location":"linuxcommands/#copying-multiple-files","title":"Copying Multiple Files","text":"<p>You can copy multiple files by specifying them one after the other:</p> <pre><code>cp /home/user/file1.txt /home/user/file2.txt /home/user/Documents/\n</code></pre>"},{"location":"linuxcommands/#copying-all-files-with-a-specific-extension","title":"Copying All Files with a Specific Extension","text":"<p>Use wildcards to copy all files with a certain extension:</p> <pre><code>cp /home/user/*.txt /home/user/Documents/\n</code></pre>"},{"location":"linuxcommands/#additional-tips","title":"Additional Tips","text":"<ul> <li>Use <code>cp -i</code> to prompt before overwriting files.</li> <li>Use <code>cp -u</code> to copy only when the source file is newer than the destination file or when the destination file is missing.</li> <li>Use <code>cp -p</code> to preserve the file attributes like mode, ownership, and timestamps.</li> </ul>"},{"location":"linuxcommands/#combining-commands","title":"Combining Commands","text":"<p>You can combine these commands with other Linux utilities for more complex tasks. For example, using <code>find</code> to copy files based on specific criteria:</p> <pre><code>find /home/user/source/ -name \"*.txt\" -exec cp {} /home/user/destination/ \\;\n</code></pre> <p>This command finds all <code>.txt</code> files in the source directory and copies them to the destination directory.</p> <p>These commands and options should cover most of your needs for copying data on a Linux system. If you have specific requirements or run into issues, feel free to ask for more details.</p>"},{"location":"nanopore-tools/","title":"Nanopore Tools","text":"<p>All nanopore proprietary software can be found in the software &amp; Download section on the website of nanopore.   </p>"},{"location":"nanopore-tools/#sequencing-minkow","title":"Sequencing: Minkow","text":"<p>The basic setup for sequencing in the software package called <code>Minknow</code>. Minknow can both operate the sequencing device as well as running real time basecalling. The installation guide provided in the the software &amp; Download section gives you an overview of all necessary commands to issue for installing the software.  </p>"},{"location":"nanopore-tools/#default-installation-directories","title":"Default installation directories","text":"<ol> <li>For the MinKNOW software: <code>/opt/ont/minknow</code> </li> <li>For the MinKNOW user interface: <code>/opt/ont/minknow-ui</code> </li> <li>Location of the reads folder: <code>/var/lib/minknow/data</code> </li> <li>Location of the log files: The MinKNOW logs are located in <code>/var/log/minknow</code> The Dorado basecaller logs are located in <code>/var/log/dorado</code> </li> </ol>"},{"location":"nanopore-tools/#run-minknow-as-root","title":"Run Minknow as root","text":"<p>When running minknow you'll notice that when issuing another path for your reads folder, minkow will issue an error. Redirecting output from the standard <code>/var/lib/minknow/data</code> might be necessary when you run out of diskspace or simply when you prefer to output your data elsewhere. To enable this, you'll need to give the <code>minknow.service</code> writing privileges i.e. make this root. Make minknow.service root<pre><code>        $ sudo service minknow stop\n        $ sudo perl -i -pe 's/(User|Group)=minknow/$1=root/'/lib/systemd/system/minknow.service\n        $ sudo systemctl daemon-reload\n        $ sudo service minknow start\n</code></pre></p>"},{"location":"nanopore-tools/#basecalling-dorado","title":"Basecalling: Dorado","text":"<p>Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads. The tool can be installed as a stand alone tool which provides more functionalities than basecalling in Minknow. You can find the Dorado github repo here.  </p>"},{"location":"nanopore-tools/#dorado-features","title":"Dorado features","text":"<ol> <li>Download model modules are not packed in the software <pre><code>dordado download --list\n</code></pre></li> <li>Basecaller <pre><code>dorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ &gt; output.bam\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-fastq &gt; output.fastq\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-sam &gt; output.sam\n</code></pre></li> <li>Aligner <pre><code>dorado basecaller -x cuda:0 --reference .your-reference.fasta dna_r10.4.1_e8_400bps_fast@v4.1.0 ./pod5 &gt; aligned.bam\n</code></pre></li> <li>Sequencing summary generation <pre><code>dorado summary ./reads.bam &gt; sequencing_summary.txt\n</code></pre></li> </ol>"},{"location":"system-build/","title":"System setup","text":""},{"location":"system-build/#operating-system","title":"Operating system","text":"<p>Release: Ubuntu 22.04.4 LTS (Jammy Jellyfish) Follow this link to learn all you need to know on how to install ubuntu.</p>"},{"location":"system-build/#gpu-cuda","title":"GPU &amp; CUDA","text":"<p>GPU computational power increased ONT basecalling speed and enabled real-time data analysis. Upon writing this documentation dorado 0.7 is the current basecaller which is also incorperated in the sequencing operating software Minknow. Before we get to the installation of dedicated sequencing software and other data analys packages we will focus on installing a GPU and the CUDA toolkit.  </p>"},{"location":"system-build/#installation-on-ubuntu","title":"Installation on Ubuntu","text":"<ul> <li>Add graphics drivers ppa repo <pre><code>sudo add-apt-repository ppa:graphics-drivers/ppa\n</code></pre></li> <li>Install Ubuntu drivers app <pre><code>sudo apt install ubuntu-drivers-common\n</code></pre></li> <li>Check available GPUs <pre><code>ubuntu-drivers devices\n</code></pre> Example output<pre><code>== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==\nmodalias : pci:v000010DEd000027E0sv00001025sd0000166Cbc03sc00i00\nvendor   : NVIDIA Corporation\ndriver   : nvidia-driver-545-open - distro non-free\ndriver   : nvidia-driver-555-open - third-party non-free\ndriver   : nvidia-driver-550 - third-party non-free\ndriver   : nvidia-driver-550-open - third-party non-free\ndriver   : nvidia-driver-545 - distro non-free\ndriver   : nvidia-driver-535 - third-party non-free\ndriver   : nvidia-driver-555 - third-party non-free recommended\ndriver   : nvidia-driver-535-server - distro non-free\ndriver   : nvidia-driver-535-open - distro non-free\ndriver   : nvidia-driver-535-server-open - distro non-free\ndriver   : nvidia-driver-525 - third-party non-free\ndriver   : xserver-xorg-video-nouveau - distro free builtin\n</code></pre></li> <li>Install latest Nvidia driver (change <code>555</code> with latest version available in your case) <pre><code>sudo apt install nvidia-driver-525\n</code></pre></li> <li>Reboot your computer  </li> <li>Check if Nvidia driver and CUDA is available after reboot <pre><code>nvidia-smi\n</code></pre> Example output<pre><code>+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4080 ...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   45C    P8               1W /  90W |   1032MiB / 12282MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n</code></pre></li> </ul> <p>This output tells us we have a NVIDIA GeForce RTX 4080 running (12GB virtual RAM) and CUDA Version 12.3 has been co-installed.  </p>"},{"location":"system-build/#purge-nvidia","title":"Purge Nvidia","text":"<p>On some occasions you'll need to perform a fresh installation of nvidia drivers. It is important to purge all currently installed NVIDIA/CUDA before installing anew. Here is a step by step guide on how to do this:  </p> <ul> <li>List all packages The dpkg -l command is used in Debian-based Linux distributions to list all the installed packages. This command provides detailed information about each package, including its status, name, version, and a brief description. <pre><code>dpkg -l\n</code></pre> The output of dpkg -l, the columns represent:  </li> </ul> <ol> <li>Package status (e.g., \"ii\" for installed, \"un\" for uninstalled, etc.)  </li> <li>Package name  </li> <li>Version  </li> <li>Architecture  </li> <li>Description  </li> </ol> <ul> <li>List and purge all NVIDIA/CUDA  </li> </ul> <p><pre><code>sudo apt-get purge $(dpkg -l | grep '^ii.*nvidia'| awk '{print $2}')\n</code></pre> Description<pre><code>dpkg -l | grep '^ii.*nvidia'    # This command lists all installed packages (output of dpkg -l) that have \"nvidia\" in their names and are in the \"installed\" state (lines starting with \"ii\").  \n\nawk '{print $2}'                # This command uses awk to print the second column (package names) from the output of the previous command.  \n\nsudo apt-get purge $(...)       # This command uses command substitution to execute the inner command and pass its output (list of package names) as arguments to apt-get purge, which removes the specified packages.  \n</code></pre></p> <p>.</p>"},{"location":"system-build/#system-software","title":"System Software","text":"<p>On Ubuntu system applications can be installed using the graphical user interface (GUI). More information on how to install software on Ubuntu click here.  </p>"},{"location":"system-build/#bioinformatic-software","title":"Bioinformatic software","text":"<p>Usually bioinformatic software are opensource packages with installation instructions described on Github repositories. First thing you can do is get yourself a github account and get familiarized with github. Executable scripts are often found in the <code>/bin</code>folder, or can be installed in <code>/usr/local/bin</code> or in a dedicated bioinfo folder <code>/usr/local/bioinf/</code>. In the next section I will explain how to install the basecalling tool Dorado in the <code>/bioinf</code>folder and put the script on <code>PATH</code>.  </p> <ul> <li>Goto DORADO GITHUB </li> <li>Download the relevant installer for your platform (like described in the instructions)  </li> <li>Extract the archive to the desired location Extract and put on PATH<pre><code>1. mkdir /usr/local/bioinf/\n2. cd /usr/local/bioinf/\n3. wget https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.7.2-linux-x64.tar.gz\n4. tar -xzvf dorado-0.7.2-linux-x64.tar.gz\n    # after unzipping, the main folder will contain a /bin and /lib folder.\n5. cd bin  \n6. ls\n    # /usr/local/bioinf/dorado/bin/dorado =&gt; this is the Executable file \n7. sudo ln -s /usr/local/bioinf/dorado/bin/dorado /usr/local/bin/dorado\n    # this will symlink the dorado script to /usr/local/bin/dorado\n8. echo 'export PATH=$PATH:/usr/local/bin' | sudo tee -a /etc/profile &amp;&amp; export PATH=$PATH:/usr/local/bin\n    # put in path if not by default, you can check by `echo $PATH`\n</code></pre> Following these instructions you should have downloaded the dorado script and it's libraries in the designated <code>/usr/local/bioinf</code> folder. To make the executable file accessible for all users you need to put it in PATH.</li> </ul>"},{"location":"system-build/#python-environments","title":"Python environments","text":"<p>When you need to install packages using the command <code>pip</code> it is adviced to do so in a separate python environment to avoid issues with the already existing python installation supporting your operating system. Detailed information on python venv can be found here.  </p>"},{"location":"system-build/#conda","title":"Conda","text":"<p>Yet another packaging and environmanagement tool is Conda. A lot of bioinformatic software has been made available in dedicated conda environments. Read all there is to know on how to use conda here. </p>"}]}