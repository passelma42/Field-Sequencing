{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p>Field sequencing outside a lab environment has been made possible by Oxford Nanopore technologies. Combining the portable sequencer called 'MinION Mk1B' and a powerfull laptop (incl. GPU power) one is able to perform in field sequencing and real-time data analysis. The idea of this website is to setup a portable field sequencing workflow covering following topics:  </p> <ul> <li>GPU system setup  </li> <li>Installing software (including ONT sequencing software)  </li> <li>Using <code>BLAST</code>and different databases for taxonomical ID  </li> <li>Wet lab approach for barcoding applications</li> <li>Demo of our field sequencing test at Botanic Garden Ghent  </li> </ul> <p>This is a step by step guide that will empower you to set up a compute workstation and learn all necessary protocols for both wet and dry lab.</p>"},{"location":"Tutorial-NanoPack/","title":"Tutorial NanoPack","text":"<p>Nanopack is a tool set of different long read processing and analysis tools. Below you'll find elaborate exercises explaining all the different faeatures of this toolset. The publischer also provided a test-data set which can be downloaded in preparation of this tutorial.</p> <p>Tool: https://github.com/wdecoster/nanopack Download test data: https://github.com/wdecoster/nanotest Publication: NanoPack: visualizing and processing long-read sequencing data </p> <p>Wouter De Coster, Svenn D\u2019Hert, Darrin T Schultz, Marc Cruts, Christine Van Broeckhoven, NanoPack: visualizing and processing long-read sequencing dta, Bioinformatics, Volume 34, Issue 15, August 2018, Pages 2666\u20132669, https://doi.org/10.1093/bioinformatics/bty149  </p>"},{"location":"Tutorial-NanoPack/#preparations","title":"Preparations","text":""},{"location":"Tutorial-NanoPack/#installation","title":"Installation","text":"<p>Please follow the installation instructions on the nanopack github website for installing all the individual tools and modules. If you are using conda on your local machine for installing scientific software you can use a dedicated <code>nanopack.yml</code> file and follow the instructions below.  </p> <p>Notes</p> <p>For more information on how to install miniconda on your system have a look here: https://docs.anaconda.com/miniconda/ </p> <p>Following these instructions should craete a dedicated environment and install all necessary dependencies:  </p> <ol> <li>Download environment file     <pre><code>wget https://raw.githubusercontent.com/passelma42/conda-envs-VM/main/nanopack.yml\n</code></pre></li> <li>Create the environment and install all dependencies <pre><code>conda env create -f nanopack.yml\n</code></pre></li> <li>Activate the env <pre><code>conda activate Nanopack\n</code></pre></li> <li>Verify installation <pre><code>conda list\n</code></pre></li> </ol>"},{"location":"Tutorial-NanoPack/#download-test-data","title":"Download test data","text":"<p>Clone the git repo. Example<pre><code>git clone git@github.com:wdecoster/nanotest.git\n</code></pre></p> <ul> <li><code>NanoStat</code>: https://github.com/wdecoster/nanotest</li> <li><code>Cramino</code>: https://github.com/wdecoster/cramino/tree/master/test-data </li> <li><code>Chopper</code>: https://github.com/wdecoster/chopper/tree/master/test-data </li> </ul>"},{"location":"Tutorial-NanoPack/#nanostat","title":"NanoStat","text":"<p>Tool for basic statistics. </p> <p>Exercise 1 Generating Basic Statistics from a FASTQ File. You are provided with a compressed FASTQ file (<code>reads.fastq.gz</code>). Generate a statistics report and save the output in a folder called <code>statreports</code> and filnema <code>nanostats.out</code>.</p> <p>Command: <pre><code>NanoStat --fastq reads.fastq.gz --outdir statreports -n nanostats.out\n</code></pre></p> <p>Objective: - Learn how to calculate and store basic statistics from a FASTQ file. - Explore the statistical metrics NanoStat provides for sequencing data.</p> <p>Exercise 2: Working with Summary Files You are given a sequencing summary files (<code>sequencing_summary.txt</code>). Generate a report from this summary files.</p> <p>Command: <pre><code>NanoStat --summary sequencing_summary.txt --outdir statreports -n nanostats-seqsummary.out\n</code></pre></p> <p>Objective: - Learn how to calculate statistics from multiple summary files and filter based on specific read types (1D2 in this case). - Understand how NanoStat handles multiple input files and the read type filtering option.</p> <p>Exercise 3: Calculating Statistics for BAM Files You are working with multiple BAM files (<code>alignment.bam</code>). Generate statistics for aligned reads and print the results to the terminal.</p> <p>Command: <pre><code>NanoStat --bam alignment.bam\n</code></pre></p> <p>Objective: - Explore how NanoStat calculates statistics from BAM files containing aligned reads. - Understand the metrics provided for aligned data and how to handle BAM files.</p> <p>Exercise 4: Exporting Statistics as TSV You are provided with a FASTA file (<code>reads.fasta.gz</code>). Generate a statistics report and export it as a TSV (Tab-Separated Values) file.</p> <p>Command: <pre><code>NanoStat --fasta reads.fasta.gz --tsv --outdir tsv_report -n nanostat-tsv.out\n</code></pre></p> <p>Objective: - Learn how to generate and export statistics in TSV format, which can be useful for downstream analysis. - Explore how to work with FASTA files as input data.</p>"},{"location":"Tutorial-NanoPack/#chopper","title":"Chopper","text":"<p>A rust implementation combining NanoLyse and NanoFilt into one faster tool for filtering, trimming, and removing contaminants. </p> <p>Download test data: https://github.com/wdecoster/chopper/tree/master/test-data</p> <p>Exercise 1: Basic Quality Filtering You are provided with a FASTQ file (<code>reads.fastq</code>) and need to filter out reads with a Phred average quality score below 20. Use the default settings for other parameters.</p> <p>Command: <pre><code>chopper -q 10 -i ./nanotest/reads.fastq.gz &gt; Q10_filtered.reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on minimum quality score. - Understand the impact of quality filtering on read data. - Get statistics from your reads before and after filtering and compare.</p> <p>Exercise 2: Length-Based Filtering You have a FASTQ file (<code>reads.fastq</code>) and need to filter out reads shorter than 100 base pairs and longer than 1000 base pairs. Use the default settings for other parameters.</p> <p>Command: <pre><code>chopper -l 10000 --maxlength 250000 -i reads.fastq &gt; length_filtered_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on length. - Understand how length-based filtering affects the dataset.</p> <p>Exercise 3: Quality and Length Filtering with Trimming You are working with a FASTQ file (<code>reads.fastq</code>). Set a minimum quality score of 30 and filter reads between 200 and 800 base pairs in length. Trim 10 nucleotides from both the start and end of each read.</p> <p>Command: <pre><code>chopper -q 12 -l 200 --maxlength 250000 --headcrop 10 --tailcrop 10 -i reads.fastq.gz &gt; filtered_trimmed_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to combine quality and length filtering with trimming. - Explore the effects of trimming on read length and quality.</p> <p>Exercise 4: Contaminant Filtering You have a FASTQ file (<code>test.fastq</code>) and a contaminant FASTA file (<code>random_contam.fa</code>). Filter out reads that match contaminants using Chopper.</p> <p>Command: <pre><code>chopper -c chopper-random_contam.fa -i chopper.test.fastq &gt; chopper_clean_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on contamination against a FASTA file. - Understand the process of removing contaminant sequences.</p> <p>Exercise 5: Inverse Filtering You are provided with a FASTQ file (<code>reads.fastq</code>) and need to keep only the reads that do not match the contaminants in <code>contaminants.fasta</code>.</p> <p>Command: <pre><code>chopper -c chopper-random_contam.fa --inverse -i chopper.test.fastq &gt; chopper_clean_reads_inverse.fastq\n</code></pre></p> <p>Objective: - Learn how to use inverse filtering to retain non-contaminant reads. - Understand the difference between standard and inverse filtering.</p> <p>Exercise 6: GC Content Filtering You have a FASTQ file (<code>reads.fastq</code>) and want to filter out reads with GC content outside the range of 30% to 60%.</p> <p>Command: <pre><code>chopper --mingc 0.3 --maxgc 0.6 -i testGC.fastq &gt; chopper_gc_filtered_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on GC content. - Understand the implications of GC content filtering on read data.</p> <p>Exercise 7 - Optional: Multi-threaded Processing When you have a large FASTQ file (<code>large_reads.fastq</code>) and want to speed up processing by using 8 threads. Filter reads with a minimum Phred quality score of 25 and a length between 100 and 1000 base pairs.</p> <p>Notes</p> <p>You can check the number of threads available on your system in various ways depending on your OS. For Linux type the command <code>lscpu</code>.</p> <p>Command: <pre><code>chopper -q 25 -l 100 --maxlength 1000 -t 8 -i large_reads.fastq &gt; filtered_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to utilize multi-threading for faster processing. - Explore the effect of parallel processing on large datasets.</p>"},{"location":"Tutorial-NanoPack/#nanocomp","title":"NanoComp","text":"<p>A tool for comparing multiple runs on read length and quality based on reads (fastq), alignments (bam) or albacore summary files. </p> <p>Exercise 1: Comparing BAM Files Compare multiple BAM files generated from different sequencing runs to assess read length distribution and other metrics.</p> <p>Command: <pre><code>NanoComp --bam alignment.bam small-test-phased.bam --outdir compare_bams --plot box --names run1 run2\n</code></pre> Make sure you downloaded the test data in the Download test data section.  </p> <p>Objective: - You will be able to visualize and interpret differences in read length distribution across sequencing runs.</p> <p>Exercise 2: Filtering FASTQ Files by Read Length Compare read lengths of FASTQ files while filtering out reads that fall outside a specific length range.  </p> <p>Command:   <pre><code>NanoComp --fastq reads.fastq.gz chopper.test.fastq --minlength 1000 --maxlength 50000 --outdir compare_filtered --plot violin\n</code></pre></p> <p>Objectives: - Learn how filtering by read length can influence data visualization and results. - Compare without filtering.</p> <p>Exercise 4: Using Different Plot Types Explore different visualization methods by generating various types of plots. - Command:   <pre><code>NanoComp --fastq reads.fastq.gz chopper.test.fastq --names run1 run2 --plot ridge --outdir compare_plots\n</code></pre></p> <p>Objectives: - Learn how different visualization techniques can highlight various aspects of sequencing data. - Generate different plots (ridge plot, violin plot, box plot) by adjusting the <code>--plot</code> parameter.</p> <p>Use Case: Generating a TSV Summary of Multiple Sequencing Files Objective: Create a summary TSV file with statistics for multiple sequencing files. - Command:   <pre><code>  NanoComp --fastq ./nanotest/reads.fastq.gz ./nanotest/chopper.test.fastq --names run1 run2  --tsv_stats --outdir tvs_summary\n</code></pre> - Objective: - Learn how to generate tabular summaries that can be used in downstream analysis or reports. This can be particularly useful when comparing several datasets in a systematic way. - To view tab delimited files issue this commant: <code>column -s $'\\t' -t &lt; NanoStats.txt</code> - Get quick info out of NanoStats on longest read and it's Q score: <code>column -s $'\\t' -t &lt; NanoStats.txt | grep \"longest_read_(with_Q):1 \"</code></p>"},{"location":"Tutorial-NanoPack/#cramino","title":"Cramino","text":"<p>A rust replacement for NanoStat - much quicker summary creation of BAM or CRAM files. </p> <p>Download test data: https://github.com/wdecoster/cramino/tree/master/test-data</p> <p>Exercise 1: Extract Basic QC Metrics from a BAM File You are provided with a BAM file (<code>alignment.bam</code>). Use Cramino to extract the basic QC metrics and use 4 parallel decompression threads (default). </p> <p>Command: <pre><code>cramino alignment.bam\n</code></pre></p> <p>Objective: - Learn how to extract quality control metrics from a BAM file. - Explore the default output provided by Cramino.</p> <p>Exercise 2: Generate Read Length Histograms You are given a BAM file (<code>alignment.bam</code>) and need to generate histograms of the read lengths.</p> <p>Command: <pre><code>cramino alignment.bam --hist\n</code></pre></p> <p>Objective: - Learn how to generate histograms of read lengths using Cramino. - Visualize the distribution of read lengths within the dataset.</p> <p>Exercise 3: Filter Reads by Minimum Length You are provided with a CRAM file (<code>alignment.cram</code>) and only want to include reads that are at least 500 base pairs long.  </p> <p>Command: <pre><code>cramino alignment.cram --min-read-len 500\n</code></pre></p> <p>Objective: - Learn how to filter reads by minimum length when extracting QC metrics. - Understand how length filtering affects the output. - Compare histograms with and without filtering</p>"},{"location":"Tutorial-NanoPack/#nanoplot","title":"NanoPlot","text":"<p>A tool for visualizing read data. </p> <p>Exercise 1: Plot Read Length Distribution from FASTQ Data.</p> <p>Command: <pre><code>NanoPlot --fastq chopper.test.fastq -o output_ex1/\n</code></pre></p> <p>Objective: Learn how to visualize the distribution of read lengths from a FASTQ file using NanoPlot.</p> <p>Exercise 2: Analyzing BAM File Alignments. This exercise will teach you how to visualize the read lengths for aligned reads from a BAM file using NanoPlot.  </p> <p>Command: <pre><code>NanoPlot --bam small-test-phased.bam --alength -o output_ex2/ --format pdf\n</code></pre> Objective: Visualize the aligned read lengths from BAM files.</p> <p>Exercise 3: Logarithmic Transformation of Read Lengths In this exercise, you'll explore how NanoPlot can transform read length distributions using a logarithmic scale.  </p> <p>Command: <pre><code>NanoPlot --fastq chopper.test.fastq --loglength -o output_ex3/\n</code></pre></p> <p>Objective: Learn how to apply logarithmic scaling to read length distributions.</p> <p>Exercise 4: Plotting Quality Scores This exercise focuses on generating plots to visualize the quality scores of sequencing reads.  </p> <p>Command: <pre><code>NanoPlot --fastq filtered_trimmed_reads.fastq --percentqual -o output_ex4/ --format png\n</code></pre></p> <p>Objective: Visualize quality scores from sequencing reads.  </p> <p>Exercise 5: Downsampling Large Datasets In this exercise, you will use NanoPlot to downsample a large FASTQ file to reduce the dataset size for faster plotting.  </p> <p>Command: <pre><code>NanoPlot --fastq reads.fastq.gz --downsample 10000 -o output_ex5/\n</code></pre></p> <p>Objective: Explore the downsampling feature of NanoPlot to work with large datasets.</p> <p>Exercise 6: Filtering Reads by Quality Learn how to filter reads by their average quality scores before plotting.  </p> <p>Command: <pre><code>NanoPlot --fastq Q10_filtered.reads.fastq --minqual 15 -o output_ex6/\n</code></pre></p> <p>Objective: Filter reads based on average quality scores and visualize the results.</p>"},{"location":"Tutorial-NanoPack/#plot-twists","title":"Plot twists","text":"<p>Wouter is like the David Lynch of plots. Please find below a short description on each plot and what it tells you.  </p> <p>Weighted Histogram of Read Lengths </p> <ul> <li>Description: This plot displays the distribution of read lengths, where each read is assigned a weight based on its abundance, quality, or other criteria. It helps to visualize which lengths contribute most significantly to the dataset.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Weighted frequency (number of reads multiplied by their weight).</li> <li>Example: Imagine a sequencing run where 500 reads are 150 bp long and 50 reads are 10,000 bp long. In a weighted histogram, the long reads would have a high weight due to their length, leading to a taller bar for the 10,000 bp category even though there are fewer of them. This shows that while there are many short reads, the long reads contribute significantly to the total base pair yield.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_1","title":"Tutorial NanoPack","text":"<p>Weighted Histogram of Read Lengths After Log Transformation </p> <ul> <li>Description: Similar to the previous plot, this histogram displays read lengths after applying a logarithmic transformation. Log transformation is useful for visualizing data that spans several orders of magnitude.</li> <li>X-axis: Log-transformed read length (log10 of the length).</li> <li>Y-axis: Weighted frequency.</li> <li>Example: If your dataset contains reads ranging from 100 bp to 10,000 bp, the log transformation will allow you to visualize the distribution more clearly. In this case, short reads (100-200 bp) might dominate the histogram, but you can still see the contribution of long reads (which would appear much smaller on a linear scale).  </li> </ul>"},{"location":"Tutorial-NanoPack/#_2","title":"Tutorial NanoPack","text":"<p>Non-Weighted Histogram of Read Lengths </p> <ul> <li>Description: This plot shows the distribution of read lengths without applying any weighting, simply counting the number of reads for each length category.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Frequency (number of reads).</li> <li>Example: If you have 700 reads of 150 bp and 300 reads of 300 bp, the histogram will show a higher bar for the 150 bp length. This representation is straightforward but doesn\u2019t account for the potential importance of longer reads.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_3","title":"Tutorial NanoPack","text":"<p>Non-Weighted Histogram of Read Lengths After Log Transformation </p> <ul> <li>Description: A non-weighted histogram that uses log transformation on read lengths to accommodate large variances in length.</li> <li>X-axis: Log-transformed read length (log10 of the length).</li> <li>Y-axis: Frequency.</li> <li>Example: This histogram could show that a large number of reads are concentrated around the short lengths (e.g., 100-500 bp, appearing as a single peak on a log scale) while still revealing how many long reads exist. The log transformation helps mitigate the influence of extremely long reads that might skew the data on a linear scale.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_4","title":"Tutorial NanoPack","text":"<p>Yield by Length </p> <ul> <li>Description: This plot illustrates how much sequencing data (yield) is generated for each read length category, providing insight into which lengths contribute most to the overall sequencing effort.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Yield (total base pairs sequenced).</li> <li>Example: If the sequencing effort produced many short reads but only a few long reads that cover a significant amount of bases, the yield might be higher for long reads despite their lower count. This plot would visually show that longer reads contribute more to total yield, helping researchers decide on read length strategies.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_5","title":"Tutorial NanoPack","text":"<p>Read Lengths vs. Average Read Quality Plot </p> <ul> <li>Description: This plot compares the lengths of reads against their average quality scores (typically represented as Phred scores). It provides insights into how read length affects quality.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Average read quality score (e.g., Phred score).</li> <li>Example: If you find that shorter reads tend to have higher average quality (e.g., many are Q30 or higher), while longer reads fall to Q20 or lower, this might indicate that the longer reads are more prone to errors, possibly due to the sequencing technology used.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_6","title":"Tutorial NanoPack","text":"<p>Aligned Read Lengths vs. Sequence Read Length </p> <ul> <li>Description: This plot compares the lengths of reads that have been aligned to a reference genome against their original lengths, highlighting the differences caused by alignment (e.g., clipping).</li> <li>X-axis: Aligned read length (length after alignment).</li> <li>Y-axis: Original read length (sequence read length).</li> <li>Example: Points clustering along the diagonal indicate that reads aligned well without significant clipping. Points above the diagonal suggest soft-clipped reads, while points below might indicate that reads were trimmed significantly or poorly aligned.</li> </ul>"},{"location":"Tutorial-NanoPack/#_7","title":"Tutorial NanoPack","text":"<p>Read Mapping Quality vs. Average Basecall Quality Plot </p> <ul> <li>Description: This plot assesses the relationship between read mapping quality (confidence that the read is correctly aligned) and average base quality (confidence in the accuracy of the base calls).</li> <li>X-axis: Read mapping quality (often denoted as MAPQ score).</li> <li>Y-axis: Average base quality score.</li> <li>Example: A cluster of points in the upper right indicates high-quality reads that map confidently. If many low mapping quality reads also show high average base quality, it might suggest alignment issues, even if the bases are accurate.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_8","title":"Tutorial NanoPack","text":"<p>Read Length vs. Read Mapping Quality </p> <ul> <li>Description: This plot explores how read length correlates with mapping quality, indicating whether longer reads tend to align better or worse than shorter ones.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Read mapping quality (MAPQ score).</li> <li>Example: You might observe that shorter reads cluster in the lower mapping quality range, indicating they map ambiguously, while longer reads often show higher mapping quality, reflecting their capacity to provide unique alignments.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_9","title":"Tutorial NanoPack","text":"<p>Percent Identity vs. Average Base Quality Plot </p> <ul> <li>Description: This plot compares percent identity (how well the read matches the reference genome) against the average quality score of the bases in the read.</li> <li>X-axis: Percent identity (percentage of bases matching the reference).</li> <li>Y-axis: Average base quality score.</li> <li>Example: A trend where high percent identity corresponds with high average base quality indicates that better-quality reads tend to align more accurately. If some low-quality reads also have high percent identity, it might suggest they are mismatches that coincidentally align well.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_10","title":"Tutorial NanoPack","text":"<p>Aligned Length vs. Percent Identity </p> <ul> <li>Description: This plot compares the length of reads that have been aligned to the reference genome against their percent identity, highlighting how alignment length relates to alignment quality.</li> <li>X-axis: Aligned read length (length after alignment).</li> <li>Y-axis: Percent identity (percentage of matches).</li> <li>Example: Points in the upper right show long aligned reads that match well with the reference. If some long reads have low percent identity, it might indicate challenges in aligning due to errors or complex regions in the genome.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_11","title":"Tutorial NanoPack","text":"<p>Dynamic Histogram of Percent Identity </p> <ul> <li>Description: An interactive histogram that shows how the distribution of percent identity changes based on applied filters (like read length or mapping quality). Users can adjust parameters in real-time to observe shifts in the data.</li> <li>X-axis: Percent identity (percentage).</li> <li>Y-axis: Frequency (or weighted frequency).</li> <li>Example: You might start with a general view showing a broad range of percent identities. When you apply a filter to exclude low-quality reads, the histogram updates to show a concentration of higher percent identities, helping you understand the impact of quality filtering on your data.  </li> </ul> <p> </p>"},{"location":"Tutorial-NanoPack/#addendum","title":"Addendum","text":""},{"location":"Tutorial-NanoPack/#pid-vs-mapq","title":"PID VS MAPQ","text":"<p>Percent Identity VS Mapping Quality. Both are important metrics used in analyzing sequencing data. But are NOT the same.</p> <p>Percent Identity (PID) </p> <ul> <li>Definition: Percent Identity refers to the proportion of identical bases between the read and the reference sequence in an alignment. It is calculated as the number of matching bases divided by the total number of aligned bases, often expressed as a percentage.</li> <li>Purpose: PID indicates how similar the aligned read is to the reference, providing a measure of alignment accuracy. A higher PID means the read matches the reference more closely.</li> <li>Example: If a read aligns to the reference with 98% identical bases, the PID is 98%. PID is often used to assess how well the reads align to a reference at the sequence level.</li> </ul> <p>Mapping Quality (MAPQ) </p> <ul> <li>Definition: Mapping Quality is a score assigned by an alignment algorithm (such as BWA, minimap2, etc.) that represents the confidence in the placement of the read in the genome. It quantifies how likely it is that the read is aligned to the correct location. The MAPQ score is usually derived from both the uniqueness of the alignment and the quality of the alignment itself.</li> <li>Purpose: MAPQ reflects the reliability of the alignment in terms of positional accuracy. It considers whether the read could have aligned equally well to other locations in the genome (ambiguity), as well as the general quality of the alignment.</li> <li>Example: A MAPQ score of 60 typically indicates that the read is uniquely aligned with high confidence, while lower MAPQ values (e.g., below 30) suggest that the read may have multiple possible alignment locations or lower confidence in its placement.</li> </ul> <p>Key Differences </p> <p>Scope:  </p> <ul> <li>PID measures the sequence similarity between the read and the reference, focusing on how many bases match.</li> <li>MAPQ measures the confidence in the read's placement in the genome, focusing on alignment ambiguity and positional accuracy.</li> </ul> <p>Calculation:  </p> <ul> <li>PID is calculated from the alignment by dividing matching bases by the total number of aligned bases.</li> <li>MAPQ is calculated by the aligner based on the alignment score and the likelihood that the read could be placed elsewhere.</li> </ul> <p>Interpretation:  </p> <ul> <li>A high PID means the read is very similar to the reference in terms of its base sequence.</li> <li>A high MAPQ means there is high confidence that the read is placed in the correct position in the genome, indicating a unique or highly confident alignment.</li> </ul> <p>Example: </p> <p>A read could have a high PID (e.g., 99% identical to the reference) but a low MAPQ (e.g., 20), which would suggest that while the read sequence closely matches the reference, the read might have multiple valid alignment locations due to repetitive regions in the genome.</p> <p>Both metrics are used together to assess different aspects of sequencing data quality: PID for sequence matching and MAPQ for alignment confidence.</p>"},{"location":"Tutorial-ONTdata/","title":"Tutorial ONTdata","text":""},{"location":"Tutorial-ONTdata/#intro","title":"Intro","text":"<p>At UGent Botanic Garden we organized a field sequencing experiment. The setup was to mimick field conditions i.e. sampling fungi, DNA extraction and PCR on the bentolab followed by live sequencing and basecalling on the mk1B from Oxford nanopore. Below you'll find an outline of the data analysis part of this setup including downloadlinks to the data icluding runcommands, so you can rerun the analysis and practice at your own pace.  </p>"},{"location":"Tutorial-ONTdata/#the-data","title":"The data","text":"<p>The original data file consists of reads for 27 accessions collected from the area around Campus Sterra at UGhent. All the QC examples below are taken from this data set. If you want to test the workflow I have provided a toy data set with reads for 3 accessions. You can download the toy-data here: <pre><code>wget https://github.com/passelma42/Field-Sequencing/raw/main/toy-data.tar.gz\n</code></pre></p> <p>Notes</p> <p>The example is ~20MB and contains reads for 2 samples: Barcode41 and Barcode63. Analysis time was 30 minutes on i7 Intel, 8 core, 16Gb Ram computer.</p>"},{"location":"Tutorial-ONTdata/#the-sequencing-run","title":"The Sequencing Run","text":"<p>Method: Rapid Barcoding SQK-RBK114.96 Data Analysis: wf-amplicon EPI2ME </p> <p>Notes</p> <p>The data and output described below was generated on the full data set. Because this is too large, I cannot share it on this platform. When you run the toy-data though you'll get for the analyzed barcodes the same output as you'll find them in the below plots.  </p> <p>We chose the RBK114 kit because this kit is best applicable in the field if you don't have access to a freezer or fridge. The 'Field sequencing kit' from nanopore is legacy at this point (2024) and was replaced by the Rapid Sequencing kits. In this post you can find results of a stability experiment of the kit kept at ambient temperature.</p> <p>The sequencing data folder </p> <pre><code>20240506_1258_MN35631_ASX408_61292243/\n        \u251c\u2500\u2500 barcode_alignment_ASX408_61292243_dc2466ae.tsv\n        \u251c\u2500\u2500 fastq_fail\n        \u251c\u2500\u2500 fastq_pass\n        \u251c\u2500\u2500 fastq_pass.tar.gz\n        \u251c\u2500\u2500 final_summary_ASX408_61292243_dc2466ae.txt\n        \u251c\u2500\u2500 other_reports\n        \u251c\u2500\u2500 pod5_fail\n        \u251c\u2500\u2500 pod5_pass\n        \u251c\u2500\u2500 pore_activity_ASX408_61292243_dc2466ae.csv\n        \u251c\u2500\u2500 report_ASX408_20240506_1302_61292243.json\n        \u251c\u2500\u2500 report_ASX408_20240506_1302_61292243.md\n        \u251c\u2500\u2500 sample_sheet_ASX408_20240506_1302_61292243.csv\n        \u251c\u2500\u2500 sequencing_summary_ASX408_61292243_dc2466ae.txt\n        \u2514\u2500\u2500 throughput_ASX408_61292243_dc2466ae.csv\n</code></pre> <p>Fastq_pass This folder is where you'll find your sequence reads. Basecalling at SUP means that all reads in this folder are q10 or higher.  </p> <pre><code>20240506_1258_MN35631_ASX408_61292243/fastq_pass/\n            \u251c\u2500\u2500 barcode41\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_0.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_1.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_10.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_11.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_12.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_13.fastq.gz\n            \u251c\u2500\u2500 barcode42\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_0.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_1.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_10.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_11.fastq.gz\n</code></pre>"},{"location":"Tutorial-ONTdata/#qc-sequence-run","title":"QC sequence run","text":"<p>Validating your sequencerun requires you to look at a lot of data. Max throughput of a minion flowcell is 50Gbases! Not hard to imagine that doing this manually is not the way to go. For this job we need dedicated sotware! There are different tools for visualizing HTS data. Nanopack is such a tool, opzimized for longread data. For validating the sequence run we can use the sequencing_summary_ASX408_61292243_dc2466ae.txt file which is generated by Minknow.  </p>"},{"location":"Tutorial-ONTdata/#tools-and-commands","title":"Tools and Commands","text":"<p>Tool: Nanopack Nanoplot Command<pre><code>NanoPlot --summary sequencing_summary_ASX408_61292243_dc2466ae.txt -o summary-plots\n</code></pre> Output </p> <p>NanoPlot gives you a set of .png plots, .txt statistics and also a handy interactive Report.html file. We'll take a closer look at some of the output to get grips on what we have sequenced.  </p> Metric Value Active channels 88.0 Mean read length 527.2 Mean read quality 12.2 Median read length 478.0 Median read quality 15.3 Number of reads 499,919.0 Read length N50 594.0 STDEV read length 2,745.3 Total bases 263,532,800.0 &gt;Q5 492,143 (98.4%), 252.2Mb &gt;Q7 472,784 (94.6%), 238.4Mb &gt;Q10 434,979 (87.0%), 218.4Mb &gt;Q12 392,967 (78.6%), 198.9Mb &gt;Q15 266,363 (53.3%), 137.8Mb <p>Number of reads over time </p> <p>Active pores over time </p> <p>Quality VS sequencing speed </p> <p>Time vs Speed </p> <p>Quality VS length log scaled (min 200bp - max 1000bp) </p> <p>Summary From these plots and stats we can confirm that we have nearly 500.000 reads with 87% of the reads Q10 and higher. Be aware that the RB114 kit performs tagmentation to add barcodes and adapers hence the 'low' mean read length. FYI our amplicons designed with ITS1f-ITS are about 900bp of length.  </p>"},{"location":"Tutorial-ONTdata/#the-data-analysis","title":"The Data Analysis","text":""},{"location":"Tutorial-ONTdata/#qc-data","title":"QC data","text":"<p>In the previous chapter we performed a QC on the sequencerun to validate the success of failure of the minion sequencerun. Now it is time to have a deeper look because we want to know how well our samples have performed. For this experiment we did ITS pcr on 27 samples collected at campus Sterre at UGhent. These samples have been assigned a barcode during the libraryprep and multiplexed into 1 single library to be sequenced. This means the data has to be demultiplexed, each unique Barcode sequence is retreived bioinformatically and assigned to a designated barcode folder (see topology above). Equimolar pooling of our barcodes during the libraryprep allows for an equal amount of reads per barcode. At least in theory. In real life, 'Numbers tell the table', we are going to measure this. We will again use the nanopack tool but now instead of the NanoPlot command we'll use the NanoComp command. In this way all the barcode stats will lign up in a nice overview. We will also need to prep the data a little bit and run this command in a 'for loop' script. We don't want to tire ourselves too much. If this doesn't ring a bell, don't care too much, just follow the instructions below.</p> <p>Tool: Nanopack Script: NanoComp-allfastq.sh </p> <ol> <li>Download the script. The script will loop over your barcode folders containing the demultiplexed fastq files. Download and test the script<pre><code>$ wget https://raw.githubusercontent.com/passelma42/scrippets/main/nanocomp-allfastq.sh\n$ chmod u+x nanocomp-allfastq.sh        # Set run permissions for the user\n$ ./nanocomp-allfastq.sh -h             # Display help function\n</code></pre></li> <li>Run NanoComp over your samples The script will concatenate all fastq files per barcode folder and run nanocomp. After the analysis has finished you'll find a .nanocompout/ folder inside the input folder you issued in the command below (under the -d flag). Command<pre><code>./nanocomp-allfastq.sh -d ./fastq_pass/ -c\n</code></pre> Output<pre><code>(nanopack) passelma@grover:~/Analyses/wf-amplicon/ONT-field2-greenhouse/benin-field2-greenhouse-run/testrunQC/fastq_pass/.nanocompout$ tree\n.\n\u251c\u2500\u2500 NanoComp-report.html\n\u251c\u2500\u2500 NanoComp_20240823_1521.log\n\u251c\u2500\u2500 NanoComp_N50.html\n\u251c\u2500\u2500 NanoComp_N50.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Normalized.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Normalized.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Weighted.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Weighted.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Normalized.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Normalized.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Weighted.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Weighted.png\n\u251c\u2500\u2500 NanoComp_lengths_violin.html\n\u251c\u2500\u2500 NanoComp_lengths_violin.png\n\u251c\u2500\u2500 NanoComp_log_length_violin.html\n\u251c\u2500\u2500 NanoComp_log_length_violin.png\n\u251c\u2500\u2500 NanoComp_number_of_reads.html\n\u251c\u2500\u2500 NanoComp_number_of_reads.png\n\u251c\u2500\u2500 NanoComp_quals_violin.html\n\u251c\u2500\u2500 NanoComp_quals_violin.png\n\u251c\u2500\u2500 NanoComp_total_throughput.html\n\u251c\u2500\u2500 NanoComp_total_throughput.png\n\u2514\u2500\u2500 NanoStats.txt\n</code></pre> NanoComp Number of reads per barcode Amongst other plots and reports we will have a look at number of reads per barcode. Although all samples have been equimolary pooled, we can already see that BC063 is underrepresented. If only this won't give us coverage issues in downstream applications :-o ?? Take a closer look at the file NanoComp-report.html for more details on other stats.  NanoComp_quals_violin In this violin plot we can confirm all our reads per barcode are of good quality. All the reads are Q10 and above. Except again BC63 is acting up... . </li> </ol>"},{"location":"Tutorial-ONTdata/#build-a-consensus","title":"Build a consensus","text":"<p>Now that we verified that we have sufficient reads per barcode and they are of good quality, it is time to start with our downstream analysis. It is time to get to buisiness! Off course there are many workflows to have a go at this type of data, but in this tutorial we will be showcasing the EPI2ME tool called wf-amplicon.. All necessary info on how to setup the necessary software and tools to run this workflow can be found on the 'installation' page of this website. In this section we will walk you through the process on how use the tool to build your consensus sequences from your raw nanopore reads.  </p> <p>Tool: wf-amplicon </p> <p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo. Running the tool in 'variant' mode it is possible to run more than one target (in this case genespecific amplicon) for the same species. But for this tutorial we will run in the de novo mode.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p> <p>Run the workflow The workflow can handle different folder layouts. If you followed the above sections of this tutorial, we are working with structure (iii).  data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre></p> <ol> <li> <p>Prepare a sample sheet This is a comma delimited file with 3 (or 4 if you run in variant mode) sections. The sample sheet is a CSV file with, minimally, columns named barcode and alias. Extra columns are allowed. In variant calling mode, a ref column can be added to tell the workflow which reference sequences should be used for which sample Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode01,BC01,test_sample,ref1\nbarcode02,BC02,test_sample\nbarcode03,BC03,test_sample,ref2\n</code></pre></p> </li> <li> <p>Prepare reference file when running variant mode When using a reference file called reference.fa it need to look like this. Each ref sequence would the represent a genespecific consensus sequence.  This reference will then be used to fish for sequences in your reads to catch and separate per gene. For instance if you amplified ITS and rpb2, mix them per sample and used this as a template to build your library, this tool could give you 2 consensus sequences per species: one ITS consensus and one rpb2 consensus. Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> </li> <li>Run the wf-amplicon on your samples The command below will run a singularity container in a nextflow run. The container beeing epi2me-labs/wf-amplicon and the command flags issued by the nextflow tool. Rember you have some prepping to do to get your system up and running to be able to run EPI2ME workflows. But believe me, this way is more easier than installing the tools yourself and writing designated scripts to stitch together all data input and oututs to feed into the next program. Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\\n    -profile singulairy\n</code></pre><pre><code>--fastq: Path to Input folder which holds fastq files or barcode directories  \n--reference: Including this line invoces running in variant mode, delete if you want to run in de novo mode  \n--sample_sheet: Path to sample sheet file  \n-c: path to config file, see 'Installation' section of this website  \n-profile: We are running the container under the singularity flag, not docker because docker doesn't play nice with HPC\n</code></pre> </li> </ol> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Make sure to delete the ref column in the sample sheet and leave out the --reference flag from the command in that case.  </p>"},{"location":"Tutorial-ONTdata/#get-your-data-out-of-here","title":"Get your data out of here","text":"<p>Once the workflow has finished, the output can be found in a folder called output-wf-apmlicon. Folder layout<pre><code>.\n\u251c\u2500\u2500 output-wf-amplicon\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 barcode41\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 alignments\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned.sorted.bam\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 aligned.sorted.bam.bai\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 consensus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 consensus.fastq\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 barcode42\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 alignments\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned.sorted.bam\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 aligned.sorted.bam.bai\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 consensus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 consensus.fastq\n</code></pre> Offcourse it would be too much to ask if the consensus sequence is given to us in a nice way i.e. having a sensible discriptive name like barcode41.consensus.fastq. Or something similar. \"Alas poor Yorick\". We'll have to fidle ourselves. Script: rename-consensus.sh This script will digg into each individual barcode folder and rename the consensus.fastq file accordingly and finally copy it to output-wf-ampmicon folder. Isn't that neat!  </p> <ol> <li>Download the script and set runpermissions <pre><code>wget https://raw.githubusercontent.com/passelma42/scrippets/main/rename-consensus.sh\nchmod u+x rename-consensus.sh       # Set run permissions for the user\n</code></pre></li> <li>Execute the script <pre><code>./rename-consensus.sh\n</code></pre></li> </ol> <p>Warning</p> <p>Verify you run the script from the output-wf-amplicon folder  </p> <p>Layout folder after rename script<pre><code>/output-wf-amlicon-nosubsample$ tree -L 1\n    .\n    \u251c\u2500\u2500 barcode41\n    \u251c\u2500\u2500 barcode41.consensus.fastq\n    \u251c\u2500\u2500 barcode42\n    \u251c\u2500\u2500 barcode42.consensus.fastq\n</code></pre> This final output gives you one consensus fastq per barcode which now you can use for blast or other alignment tools, phylogenetics,... .  </p>"},{"location":"blast/","title":"Blast","text":""},{"location":"blast/#blast-cli","title":"Blast-CLI","text":""},{"location":"blast/#intro","title":"Intro","text":"<p>The BLAST algorithm, which stands for Basic Local Alignment Search Tool, is a widely used and highly effective algorithm in bioinformatics and computational biology. It is used to compare biological sequences, such as DNA, RNA, or protein sequences, to identify regions of similarity or homology. The primary goal of the BLAST algorithm is to find local alignments between a query sequence and a database of sequences, which can help researchers identify genes, functional domains, and evolutionary relationships between sequences. Blast can be run at the NCBI website. Or a Command Line Interface can be downloaded to your own system.</p> <p>Here are the key components and steps of the BLAST algorithm:  </p> <ol> <li>Query Sequence: BLAST starts with a query sequence provided by the user, which is the sequence you want to compare against a database of other sequences.  </li> <li>Database: BLAST searches against a database containing a collection of sequences. This can be a database of known genes, genomes, proteins, or any other relevant biological sequences.  </li> <li>Scoring System: BLAST uses a scoring system to assign a numerical score to alignments between the query sequence and sequences in the database. Common scoring systems include the substitution matrix (e.g., BLOSUM or PAM matrices) for protein sequences and scoring schemes like match/mismatch scores and gap penalties for nucleotide sequences.  </li> <li>Seed Search: BLAST starts by identifying short, exact matches (seeds) between the query sequence and sequences in the database. These seeds serve as starting points for potential alignments.  </li> <li>Extension: Once seeds are identified, BLAST extends these seed matches in both directions along the sequences, using dynamic programming techniques to find the optimal alignment that maximizes the alignment score. The algorithm also considers gap penalties to account for insertions and deletions.  </li> <li>Scoring and Filtering: BLAST scores the extended alignments based on the chosen scoring system. It then applies a statistical significance threshold (usually based on E-values) to filter out alignments that could occur by chance.  </li> <li>Reporting Hits: BLAST reports the significant alignments, known as \"hits,\" to the user. These hits represent regions of similarity between the query sequence and sequences in the database.  </li> </ol> <p>BLAST is highly configurable, allowing users to adjust parameters such as the scoring system, gap penalties, and statistical significance thresholds to customize the search based on their specific needs. There are different versions of BLAST, including BLASTn (for nucleotide sequences), BLASTp (for protein sequences), BLASTx (translating nucleotide sequences to proteins), and more, each tailored to different types of sequence data.  </p> <p>Overall, BLAST is an essential tool for sequence comparison and is widely used in genomics, proteomics, and other areas of biological research. It enables researchers to quickly and effectively identify similarities between sequences and gain insights into the functional and evolutionary relationships of biological molecules.  </p> <p>Check out the NCBI-blast page for more information (and online applications)</p>"},{"location":"blast/#local-blast","title":"Local Blast","text":"<p>For more info take a look at the Download section @ NCBI website.</p>"},{"location":"blast/#installation","title":"Installation","text":"<p>Running blast form the webinterface has it's limitations. If you want to include your blast analysis in your own pipelines, Blast+ is the way to go. You can download the Blast+ executables and install them on your own machine.  </p> <ol> <li>Download the latest executable</li> <li>Unpack the download file: <pre><code>tar xzvf yourfile.tar.gz\n</code></pre> NOTE: It is always good pracktice to install bioinformatic software (if installed from source) to a designated bioinfo folder i.e. /usr/local/bionfo.           From here you can create a symlink to a folder in your $PATH f.i. /usr/local/bin</li> </ol>"},{"location":"blast/#database-download","title":"Database Download","text":"<p>BLAST databases are updated daily and may be downloaded via FTP. Database sets may be retrieved automatically with update_blastdb.pl, which is part of the BLAST+ suite. (run this command in working directory, probably best in same directory at your $PATH of the database.)</p> <p>Please refer to the BLAST database documentation for more details.</p> <ol> <li>Pre-fromatted BLAST db are archived here</li> <li>Download all numbered files for your database of choice using the basename (eg nt.000.tar.gz =&gt; \"nt\"):       Each of these files represents a subset (volume) of that database,       and all of them are needed to reconstitute the database.</li> <li>Extract: <pre><code>for file in *.gz; do tar -xzvf \"$file\"; done\n</code></pre></li> <li>After extraction, there is no need to concatenate the resulting files:       Call the database with the base name, for nr database files, use \"-db nt\".</li> </ol> <p>NOTE: For easy download, use the update_blastdb.pl script from the blast+ package.      Run following command in your db folder: <pre><code>perl update_blastdb.pl --decompress nt # or any other prefix for you db of coice\n</code></pre> NOTE: if the ncbi server runs slow, the update command could rise connection issues.   Sometimes it is easier to download manually from the ftp db website: <pre><code>wget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz\nwget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz.md5\n</code></pre></p>"},{"location":"blast/#build-local-db","title":"Build Local DB","text":"<p>If you want to build your own database based on fasta sequences take a look here for more details. Example<pre><code>$ makeblastdb -in test.fsa -parse_seqids -blastdb_version 5 -taxid_map test_map.txt -title \"Cookbook demo\" -dbtype prot\n\n\nBuilding a new DB, current time: 02/06/2019 17:08:14\nNew DB name:   test.fsa\nNew DB title:  Cookbook demo\nSequence type: Protein\nKeep MBits: T\nMaximum file size: 1000000000B\nAdding sequences from FASTA; added 6 sequences in 0.00222588 seconds.\n$\n</code></pre></p>"},{"location":"blast/#configuration","title":"Configuration","text":"<p>To set the env for blastn you can make a configuration file named .ncbirc (on Unix-like platforms) or ncbi.ini (on Windows) in your home directory  </p> <pre><code>touch ~/.ncbirc &amp;&amp;\necho [BLAST] &gt;&gt; ~/.ncbirc\necho [BLASTDB]=/path/to/databases &gt;&gt; ~/.ncbirc\n</code></pre> <p>More available parameters can be set in the config file. More detailed info on configurateion can be found here </p>"},{"location":"blast/#command-line","title":"Command line","text":""},{"location":"blast/#quick-run","title":"Quick run","text":"<pre><code>blastn \u2013db nt \u2013query nt.fsa \u2013out results.out\n</code></pre>"},{"location":"blast/#personalized-outputformat","title":"Personalized outputformat","text":"<pre><code>blastn -db nt \\ # nt is the name (prefix) you gave your db\n  -query /path/to/fastafiles \\\n  -out /path/to/output/out \\\n    -max_target_seqs 5 \\\n  -outfmt \"\"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore staxids sscinames sskingdoms qcovs\" \\\n  -num_trheads $(THREADS)\n</code></pre> <ul> <li> <p><code>1. qseqid</code>: query or source (gene) sequence id</p> </li> <li> <p><code>2. sseqid</code>: subject or target (reference genome) sequence id</p> </li> <li> <p><code>3. pident</code>: percentage of identical positions</p> </li> <li> <p><code>4. length</code>: alignment length (sequence overlap)</p> </li> <li> <p><code>5. mismatch</code>: number of mismatches</p> </li> <li> <p><code>6. gapopen</code>: number of gap openings</p> </li> <li> <p><code>7. qstart</code>: start of alignment in query</p> </li> <li> <p><code>8. qend</code>: end of alignment in query</p> </li> <li> <p><code>9. sstart</code>: start of alignment in subject</p> </li> <li> <p><code>10. send</code>: end of alignment in subject</p> </li> <li> <p><code>11. evalue</code>: expect value</p> </li> <li> <p><code>12. bitscore</code>: bit score</p> </li> </ul>"},{"location":"blast/#setup-blast-hpc-ugent","title":"Setup Blast @ HPC-UGent","text":"<p>Blast modules are available on the HPC.  Databases on the other hand are installed 'localy. ALERT!: don't install databases locally, they take up a lot of space. Please contact Pieter if you need a database which is not installed yet.</p>"},{"location":"blast/#db-location","title":"DB location","text":"<p>A dedicated folder is available for our VO where we can 'locally' make pre installed databases available for us all.  If you need another database, please avoid installing it into your own account folder, it will take up a lot of space and others who are part of our VO cannot access.  </p> <p>Location: /data/gent/vo/001/gvo00142/data_share_group/databases_blast/  Configuration db: Create a ncbirc file (see above for instructions) and save in your home folder (/user/gent/433/yourvscnumber)  </p> <p>Alternative: add this line to your .bashrc file:  </p> <pre><code>export BLASTDB=/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre>"},{"location":"blast/#updates","title":"Updates","text":"<p>Instllation dates are always part of the folder where you can find the databasefiles, if you feel it is not up to date, give a shout out to Pieter or make an update yourself if you know how to. When you need another type of db wchich you frequently use and others might benefit from, ask for a local installation.  </p>"},{"location":"blast/#usage","title":"Usage","text":"<ol> <li>Make sure your env value for $BLASTDB is set correctly. <pre><code>vsc43352@node3206:~$echo $BLASTDB                                                    \n/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre></li> <li>Use ml spider blast to list the installed blast modules</li> <li>Use the latest module installed in your jobscript</li> <li>run blast following the blast module (for instpiration look to section Quick run above)      or RTFM.</li> </ol>"},{"location":"github/","title":"Github","text":"<p>For a more in depth overview go to the official documentation.  </p>"},{"location":"github/#1-setting-up-a-local-git-repository","title":"1. Setting Up a Local Git Repository","text":"<ol> <li> <p>Initialize a new Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\n</code></pre> <ul> <li><code>cd /path/to/your/project</code>: Change directory to your project folder.</li> <li><code>git init</code>: Initialize a new Git repository in the current directory.</li> </ul> </li> <li> <p>Add files to the repository:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all files in the current directory for the next commit.</li> </ul> </li> <li> <p>Commit the files:</p> <pre><code>git commit -m \"Initial commit\"\n</code></pre> <ul> <li><code>git commit -m \"Initial commit\"</code>: Commit the staged files with a message.</li> </ul> </li> </ol>"},{"location":"github/#2-authenticating-using-ssh-key","title":"2. Authenticating Using SSH Key","text":"<ol> <li> <p>Generate an SSH key pair (if you don't have one):</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <ul> <li><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"</code>: Generate a new SSH key with RSA encryption.</li> </ul> </li> <li> <p>Add the SSH key to your SSH agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <ul> <li><code>eval \"$(ssh-agent -s)\"</code>: Start the SSH agent.</li> <li><code>ssh-add ~/.ssh/id_rsa</code>: Add your SSH private key to the SSH agent.</li> </ul> </li> <li> <p>Add the SSH key to your GitHub/GitLab/Bitbucket account:</p> <ul> <li> <p>Copy the SSH key to your clipboard:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> </li> <li> <p>Go to your Git hosting service and add the SSH key to your account settings.</p> </li> </ul> </li> </ol>"},{"location":"github/#3-pushing-the-local-repository-to-a-remote-repository","title":"3. Pushing the Local Repository to a Remote Repository","text":"<ol> <li> <p>Add the remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\n</code></pre> <ul> <li><code>git remote add origin git@github.com:username/repo.git</code>: Add a remote repository with the alias <code>origin</code>.</li> </ul> </li> <li> <p>Push the local repository to the remote repository:</p> <pre><code>git push -u origin main\n</code></pre> <ul> <li><code>git push -u origin main</code>: Push the local <code>main</code> branch to the <code>origin</code> remote repository and set it as the default upstream branch.</li> </ul> </li> </ol>"},{"location":"github/#4-making-changes-and-syncing-them","title":"4. Making Changes and Syncing Them","text":"<ol> <li> <p>Make changes to your files.</p> </li> <li> <p>Stage the changes:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all changed files for the next commit.</li> </ul> </li> <li> <p>Commit the changes:</p> <pre><code>git commit -m \"Describe your changes\"\n</code></pre> <ul> <li><code>git commit -m \"Describe your changes\"</code>: Commit the staged changes with a descriptive message.</li> </ul> </li> <li> <p>Push the changes to the remote repository:</p> <pre><code>git push\n</code></pre> <ul> <li><code>git push</code>: Push the committed changes to the remote repository.</li> </ul> </li> </ol>"},{"location":"github/#5-keeping-your-local-repository-in-sync-with-the-remote","title":"5. Keeping Your Local Repository in Sync with the Remote","text":"<ol> <li> <p>Fetch the latest changes from the remote repository:</p> <pre><code>git fetch origin\n</code></pre> <ul> <li><code>git fetch origin</code>: Fetch the latest changes from the <code>origin</code> remote repository.</li> </ul> </li> <li> <p>Merge the fetched changes into your local branch:</p> <pre><code>git merge origin/main\n</code></pre> <ul> <li><code>git merge origin/main</code>: Merge the fetched <code>main</code> branch into your local <code>main</code> branch.</li> </ul> </li> <li> <p>Pull the latest changes (fetch + merge):</p> <pre><code>git pull\n</code></pre> <ul> <li><code>git pull</code>: Fetch and merge the latest changes from the remote repository into your local branch.</li> </ul> </li> </ol>"},{"location":"github/#full-command-summary","title":"Full Command Summary","text":"<ol> <li> <p>Initialize Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> </li> <li> <p>Generate and add SSH key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub\n# Add the key to your Git hosting account settings\n</code></pre> </li> <li> <p>Add and push to remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\ngit push -u origin main\n</code></pre> </li> <li> <p>Make changes and push:</p> <pre><code># Make changes to your files\ngit add .\ngit commit -m \"Describe your changes\"\ngit push\n</code></pre> </li> <li> <p>Sync with remote repository:</p> <pre><code>git pull\n</code></pre> </li> </ol> <p>By following these steps and using the commands provided, you can effectively manage your local and remote Git repositories.</p>"},{"location":"installation/","title":"Installation","text":"<p>EPI2ME Labs maintains a collection of bioinformatics workflows tailored to Oxford Nanopore Technologies long-read sequencing data. They are curated and actively maintained by experts in long-read sequence analysis. Read all about EPI2ME workflows here.  </p>"},{"location":"installation/#installation-guide","title":"Installation guide","text":""},{"location":"installation/#java","title":"JAVA","text":"<pre><code>sudo apt-get update\nsudo apt-get install default-jre #install java on ubuntu\n</code></pre>"},{"location":"installation/#nextflow","title":"NEXTFLOW","text":"<p>Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages.  </p> <p>More information can be found on the NEXTFLOW WEBSITE. Download and install nextflow<pre><code>curl -s https://get.nextflow.io | bash # install nextflow\n</code></pre></p>"},{"location":"installation/#go","title":"GO","text":"<p>Singularity 3.0 is written primarily in Go, and you will need Go installed to compile it from source.  </p> <p>More information can be found on the Singularity installation guide website.  </p> <pre><code># Install\n    export VERSION=1.11 OS=linux ARCH=amd64 &amp;&amp; \\\n    wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    rm go$VERSION.$OS-$ARCH.tar.gz\n\n# setup GO environment\n    echo 'export GOPATH=${HOME}/go' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    source ~/.bashrc\n\n# install dep for dependency resolution\n    go get -u github.com/golang/dep/cmd/dep\n</code></pre>"},{"location":"installation/#singularity","title":"Singularity","text":"<p>Singularity is a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Singularity on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system and system.  </p> <p>More information can be found on the Singularity introduction page.  </p> <p>Installation <pre><code>export VERSION=3.0.3 &amp;&amp; # adjust this as necessary \\\n    mkdir -p $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    cd $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz &amp;&amp; \\\n    tar -xzf singularity-${VERSION}.tar.gz &amp;&amp; \\\n    cd ./singularity &amp;&amp; \\\n    ./mconfig\n</code></pre> Compile <pre><code>./mconfig &amp;&amp; \\\n    make -C ./builddir &amp;&amp; \\\n    sudo make -C ./builddir install\n</code></pre> Singularity cache folder </p> <p>Save storage space on your computer. When you run the singularity container (wf-amplicon for instance), a work directory will be created in your project folder. The wf-amplicon uses different containers without you noticing. All container images will be downloaded each time you run this wf. And this takes up space on your HD. </p> <p>To avoid this, you can create a cache folder where singularity can store these .img files and use these instead of downloding them each time you run the wf. The cache foler can then be specified in a nextflow.config file.  OR you can do as the error message expalains is to set the NXF_SINGULARITY_CACHEDI variable in you .bashrc file. Both options will be explained below.  </p> <p>OPTION 1: create nextflow.config file Follow these steps to do just this:  </p> <ol> <li> <p>Run the workflow for the 1st time     Run the workflow for the firs time and don't bother about this warning:</p> <p>Warning</p> <p>If the cache folder is not set you get this warning: WARN: Singularity cache directory has not been defined -- Remote image will be stored in the path: /home/path-to/work/singularity -- Use the environment variable NXF_SINGULARITY_CACHEDIR to specify a different location </p> </li> <li> <p>Locate the .img files     The wf-amplicon will create a <code>./work/singularity</code> folder in your projcet folder.      <pre><code>pieter@UGENT-PC:~/Analyses/wf-amplicon/work\n$ ls\n0f  14  24  2d  3f  46  49  54  5b  63  6d  77  7b  80  85  93  9b  9f  a1  a4  af  b5  bc  cc  d6  e5  e8  singularity\n11  1e  2b  3c  40  47  4c  56  5d  6b  71  78  7f  82  89  96  9c  a0  a3  ac  b0  b6  c8  d0  db  e6  ee\n(base)\npieter@UGENT-PC~/Analyses/wf-amplicon/work\n$ ls singularity/\nontresearch-medaka-sha61a9438d745a78030738352e084445a2db3daa2a.img       ontresearch-wf-common-sha91452ece4f647f62b32dac3a614635a6f0d7f8b5.img\nontresearch-wf-amplicon-sha3b6a08c2ed47efd85b17b98762951fffdeee5ba9.img\n</code></pre>     In this folder you'll find the <code>.img</code> files you need later.  </p> </li> <li> <p>Copy the <code>.img</code> files to a local folder     To avoid this create cache folder to store downloaded images when running epi2me labs. For each new analysis you run with epi2me make sure to put the image in this folder. <pre><code>mkdir /path/to/singularity-img\ncp ~/Analyses/wf-amplicon/work/singularity/*.img /path/to/singularity-img # this will copy all img files to your folder created in previou step, don't forget to personalize the paths  \n</code></pre> Create nextflow config file Now it is time to configure nextflow which is used to stitch all the singularity containers from wf-amplicon together. We will create a file that will be checked running each wf and redirects singularity to find the images stored locally rather than download them each time. More info on nextflow configuration file. Process: <pre><code>mkdir /home/sequencing/nextflow-config # /home/sequencing is in this case an example of your user's home directory. User is called *sequencer* in this example.  \ntouch /home/sequencing/nextflow-config/nextflow.config\n</code></pre> nextflow.config<pre><code>singularity { \n    enabled = true\n    cacheDir = /path/to/singularity-img\n} \n</code></pre></p> </li> </ol> <p>OPTION2: Set the environmental variable  </p> <p>The warning message indicates that the Singularity cache directory has not been set correctly, even though you specified it in your custom Nextflow configuration. To resolve this, you can explicitly set the Singularity cache directory using the <code>NXF_SINGULARITY_CACHEDIR</code> environment variable.</p> <p>Steps to Set the Singularity Cache Directory </p> <ol> <li>Set the Environment Variable:</li> <li>You can set the environment variable in your shell before running Nextflow. For example:      <pre><code>export NXF_SINGULARITY_CACHEDIR=/home/pieter/singularity-img\n</code></pre></li> <li>Set the Environment Variable Permanently (Optional):</li> <li>To make this change permanent, you can add the <code>export</code> command to your shell\u2019s configuration file (e.g., <code>~/.bashrc</code> or <code>~/.bash_profile</code> for bash):      <pre><code>echo \"export NXF_SINGULARITY_CACHEDIR=/home/pieter/singularity-img\" &gt;&gt; ~/.bashrc\n</code></pre></li> <li> <p>Reload your shell configuration:      <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>Verify the Configuration:</p> </li> <li>Run the pipeline again to ensure the warning disappears:      <pre><code>nextflow run epi2me-labs/wf-amplicon --fastq ./fastq --sample_sheet ./sample-sheet.csv --porechop --discard_middle --out_dir output-wf-amlicon-toydata-bis\n</code></pre></li> </ol> <p>This should direct Nextflow to use the specified cache directory for Singularity images, avoiding the warning and ensuring images are stored in your preferred location.</p>"},{"location":"installation/#wf-amplicon","title":"wf-amplicon","text":"<p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p>"},{"location":"installation/#run-the-workflow","title":"Run the workflow","text":"<p>data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre> Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\ #this can be omitted if you set the variable in your .bashrc\n    -profile singulairy\n</code></pre> Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode41,BC41,test_sample,ref1\nbarcode42,BC42,test_sample\nbarcode43,BC43,test_sample,ref2\n</code></pre> Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Also make sure to delete the ref column in that case.</p>"},{"location":"installation/#links","title":"Links","text":"<ul> <li>Installation EPI2ME</li> <li>Installation Nextflow </li> <li>Installation Singularity </li> <li>Installation Docker</li> <li>Installation GO</li> </ul>"},{"location":"linuxcommands/","title":"Essential Linux Commands","text":""},{"location":"linuxcommands/#file-and-directory-operations","title":"File and Directory Operations","text":""},{"location":"linuxcommands/#listing-and-navigating","title":"Listing and Navigating","text":"<ul> <li><code>ls</code>: List the contents of a directory.   <pre><code>ls /path/to/directory\n</code></pre></li> <li><code>cd</code>: Change the current directory.   <pre><code>cd /path/to/directory\n</code></pre></li> <li><code>pwd</code>: Display the current working directory.   <pre><code>pwd\n</code></pre></li> </ul>"},{"location":"linuxcommands/#creating-and-removing","title":"Creating and Removing","text":"<ul> <li><code>mkdir</code>: Create a new directory.   <pre><code>mkdir new_directory\n</code></pre></li> <li><code>rmdir</code>: Remove an empty directory.   <pre><code>rmdir empty_directory\n</code></pre></li> <li><code>rm</code>: Remove files or directories.   <pre><code>rm file_name   # for files\nrm -r directory_name   # for directories\n</code></pre></li> </ul>"},{"location":"linuxcommands/#copying-and-moving","title":"Copying and Moving","text":"<ul> <li><code>cp</code>: Copy files or directories.   <pre><code>cp /path/to/source/file /path/to/destination/\n</code></pre>   Example:   <pre><code>cp /home/user/file.txt /home/user/Documents/\n</code></pre></li> <li>Copy a directory and its contents:   <pre><code>cp -r /path/to/source/directory /path/to/destination/\n</code></pre>   Example:   <pre><code>cp -r /home/user/folder /home/user/Documents/\n</code></pre></li> <li><code>mv</code>: Move or rename files or directories.   <pre><code>mv /path/to/source/file /path/to/destination/\n</code></pre>   Example:   <pre><code>mv /home/user/file.txt /home/user/Documents/\n</code></pre></li> </ul>"},{"location":"linuxcommands/#file-manipulation","title":"File Manipulation","text":"<ul> <li><code>touch</code>: Create an empty file or update the timestamp of an existing file.   <pre><code>touch file_name\n</code></pre></li> <li><code>cat</code>: Concatenate and display the content of files.   <pre><code>cat file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#advanced-copying-with-rsync","title":"Advanced Copying with <code>rsync</code>","text":"<ul> <li><code>rsync</code>: Synchronize directories and copy large amounts of data efficiently.   <pre><code>rsync -av /path/to/source/ /path/to/destination/\n</code></pre></li> <li><code>-a</code>: Archive mode, preserves permissions, times, symbolic links, etc.</li> <li><code>-v</code>: Verbose, shows the progress of the transfer.</li> </ul>"},{"location":"linuxcommands/#file-permissions-and-ownership","title":"File Permissions and Ownership","text":"<ul> <li><code>chmod</code>: Change the permissions of a file or directory.   <pre><code>chmod 755 file_name\n</code></pre></li> <li><code>chown</code>: Change the ownership of a file or directory.   <pre><code>chown user:group file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#file-viewing-and-editing","title":"File Viewing and Editing","text":"<ul> <li><code>nano</code>/<code>vim</code>: Text editors for creating and editing files directly in the terminal.   <pre><code>nano file_name\nvim file_name\n</code></pre></li> <li><code>less</code>: Display the content of a file one screen at a time.   <pre><code>less file_name\n</code></pre></li> <li><code>head</code>: Show the first few lines of a file.   <pre><code>head -n 10 file_name   # shows the first 10 lines\n</code></pre></li> <li><code>tail</code>: Show the last few lines of a file.   <pre><code>tail -n 10 file_name   # shows the last 10 lines\n</code></pre></li> <li><code>grep</code>: Search for a specific pattern within files.   <pre><code>grep \"pattern\" file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#system-information","title":"System Information","text":"<ul> <li><code>uname</code>: Display basic information about the system.   <pre><code>uname -a   # shows all system information\n</code></pre></li> <li><code>df</code>: Show disk space usage.   <pre><code>df -h   # human-readable format\n</code></pre></li> <li><code>du</code>: Display the disk usage of files and directories.   <pre><code>du -sh directory_name\n</code></pre></li> <li><code>free</code>: Show the amount of free and used memory in the system.   <pre><code>free -h\n</code></pre></li> <li><code>top</code>: Display real-time system processes and resource usage.   <pre><code>top\n</code></pre></li> <li><code>ps</code>: Display the currently running processes.   <pre><code>ps aux\n</code></pre></li> </ul>"},{"location":"linuxcommands/#network-operations","title":"Network Operations","text":"<ul> <li><code>ping</code>: Test connectivity to a network host.   <pre><code>ping google.com\n</code></pre></li> <li><code>ifconfig</code>: Display or configure network interfaces.   <pre><code>ifconfig   # use `ip addr` in newer systems\n</code></pre></li> <li><code>netstat</code>: Show network connections, routing tables, and more.   <pre><code>netstat -tuln   # shows listening ports\n</code></pre></li> <li><code>ssh</code>: Connect to another machine securely over the network.   <pre><code>ssh user@hostname\n</code></pre></li> </ul>"},{"location":"linuxcommands/#process-management","title":"Process Management","text":"<ul> <li><code>kill</code>: Send a signal to terminate a process.   <pre><code>kill PID   # where PID is the process ID\n</code></pre></li> <li><code>killall</code>: Terminate all processes by name.   <pre><code>killall process_name\n</code></pre></li> <li><code>bg</code>: Resume a suspended job in the background.   <pre><code>bg job_number\n</code></pre></li> <li><code>fg</code>: Bring a job to the foreground.   <pre><code>fg job_number\n</code></pre></li> </ul>"},{"location":"linuxcommands/#user-and-group-management","title":"User and Group Management","text":"<ul> <li><code>adduser</code>/<code>useradd</code>: Add a new user to the system.   <pre><code>adduser username\nuseradd username\n</code></pre></li> <li><code>passwd</code>: Change a user\u2019s password.   <pre><code>passwd username\n</code></pre></li> <li><code>su</code>: Switch to another user account.   <pre><code>su - username\n</code></pre></li> <li><code>sudo</code>: Execute a command with superuser privileges.   <pre><code>sudo command\n</code></pre></li> </ul>"},{"location":"linuxcommands/#package-management-example-debian-based-systems","title":"Package Management (Example: Debian-based systems)","text":"<ul> <li><code>apt-get update</code>: Update the package lists.   <pre><code>sudo apt-get update\n</code></pre></li> <li><code>apt-get install</code>: Install a new package.   <pre><code>sudo apt-get install package_name\n</code></pre></li> <li><code>apt-get upgrade</code>: Upgrade all installed packages.   <pre><code>sudo apt-get upgrade\n</code></pre></li> </ul>"},{"location":"linuxcommands/#text-manipulation","title":"Text Manipulation","text":"<ul> <li><code>echo</code>: Display a line of text.   <pre><code>echo \"Hello, World!\"\n</code></pre></li> <li><code>grep</code>: Search for text patterns in files.   <pre><code>grep \"text\" file_name\n</code></pre></li> <li><code>sed</code>: Stream editor for filtering and transforming text.   <pre><code>sed 's/old/new/' file_name\n</code></pre></li> <li><code>awk</code>: A powerful text processing language.   <pre><code>awk '{print $1}' file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#expanded-section-on-text-manipulation-for-fasta-fastq-and-bam-files","title":"Expanded Section on Text Manipulation for FASTA, FASTQ, and BAM Files","text":"<p>When working with bioinformatics data formats like FASTA, FASTQ, and BAM files, text manipulation tools like <code>echo</code>, <code>grep</code>, <code>sed</code>, and <code>awk</code> become essential. Below are examples and explanations of how these tools can be used effectively.</p>"},{"location":"linuxcommands/#fasta-files","title":"FASTA Files","text":"<p>FASTA files store nucleotide or protein sequences with a header line starting with <code>&gt;</code> followed by sequence data. </p> <p>Examples: </p> <ul> <li> <p><code>grep</code>: Extract sequence identifiers (headers) from a FASTA file.   <pre><code>grep \"^&gt;\" sequences.fasta\n</code></pre>   This command searches for lines starting with <code>&gt;</code> and outputs them, giving you all the headers in the file.</p> </li> <li> <p><code>awk</code>: Count the number of sequences in a FASTA file.   <pre><code>awk '/^&gt;/ {count++} END {print count}' sequences.fasta\n</code></pre>   This <code>awk</code> script increments a counter for each header line and prints the total number of sequences at the end.</p> </li> <li> <p><code>sed</code>: Remove description from headers in a FASTA file.   <pre><code>sed 's/\\s.*$//' sequences.fasta &gt; cleaned_sequences.fasta\n</code></pre>   This command keeps only the identifier in each header line by deleting everything after the first space.</p> </li> </ul>"},{"location":"linuxcommands/#fastq-files","title":"FASTQ Files","text":"<p>FASTQ files contain sequence data with quality scores, commonly used in next-generation sequencing. </p> <p>Examples: </p> <ul> <li> <p><code>grep</code>: Extract all sequence headers (lines starting with <code>@</code>).   <pre><code>grep \"^@\" sequences.fastq\n</code></pre>   This command will output all sequence headers in a FASTQ file.</p> </li> <li> <p><code>awk</code>: Calculate the total number of reads in a FASTQ file.   <pre><code>awk '{s++} END {print s/4}' sequences.fastq\n</code></pre>   Since each read spans four lines (header, sequence, plus, quality), dividing the total line count by four gives the number of reads.</p> </li> <li> <p><code>sed</code>: Convert sequence headers to uppercase (useful if headers have mixed cases).   <pre><code>sed -e '1~4s/.*/\\U&amp;/' sequences.fastq &gt; uppercased_headers.fastq\n</code></pre>   This command converts the headers (every 4th line, starting from line 1) to uppercase.</p> </li> </ul>"},{"location":"linuxcommands/#bam-files","title":"BAM Files","text":"<p>BAM files are binary versions of SAM files and store aligned sequence data. While binary, text manipulation often applies to the SAM format (BAM converted to SAM via <code>samtools view</code>).</p> <p>Examples: </p> <ul> <li> <p><code>samtools</code> &amp; <code>awk</code>: Extract read names and count the total number of unique reads.   <pre><code>samtools view -h file.bam | awk '{if($1 !~ /^@/) print $1}' | sort | uniq | wc -l\n</code></pre>   This command extracts the read names from a BAM file, sorts them, filters out duplicates, and counts the unique entries.</p> </li> <li> <p><code>grep</code>: Filter reads mapped to a specific chromosome.   <pre><code>samtools view file.bam | grep \"^chr1\" &gt; chr1_reads.sam\n</code></pre>   This filters out reads that are mapped to chromosome 1.</p> </li> <li> <p><code>awk</code>: Extract and count reads with a specific mapping quality.   <pre><code>samtools view file.bam | awk '$5 &gt;= 30' | wc -l\n</code></pre>   This command counts the number of reads with a mapping quality of 30 or higher.</p> </li> </ul>"},{"location":"linuxcommands/#combining-tools-for-advanced-manipulation","title":"Combining Tools for Advanced Manipulation","text":"<p>Combining <code>grep</code>, <code>awk</code>, and <code>sed</code> can provide powerful data extraction and manipulation. Here\u2019s an example:</p> <ul> <li>Extracting and summarizing GC content from FASTA sequences:   <pre><code>awk '/^&gt;/ {if (seqlen){print gc/seqlen*100}; print; gc=0; seqlen=0; next} {gc+=gsub(/[GgCc]/,\"\"); seqlen+=length($0)} END {print gc/seqlen*100}' sequences.fasta\n</code></pre>   This script calculates the GC content for each sequence and prints it after each sequence header.</li> </ul> <p>By mastering these commands, you can efficiently manipulate and analyze bioinformatics data files. Each tool offers specific advantages depending on the task, allowing for streamlined data processing in a command-line environment.</p>"},{"location":"linuxcommands/#archiving-and-compression","title":"Archiving and Compression","text":""},{"location":"linuxcommands/#tar","title":"<code>tar</code>","text":"<p>Archive files into a tarball, which can be compressed using various algorithms.   - Create a tarball:      <pre><code>tar -cvf archive_name.tar directory_name\n</code></pre>   - Create a compressed tarball with gzip:      <pre><code>tar -czvf archive_name.tar.gz directory_name\n</code></pre>   - Create a compressed tarball with bzip2:      <pre><code>tar -cjvf archive_name.tar.bz2 directory_name\n</code></pre>   - Create a compressed tarball with xz:      <pre><code>tar -cJvf archive_name.tar.xz directory_name\n</code></pre>   - Extract a tarball:      <pre><code>tar -xvf archive_name.tar\n</code></pre>   - Extract a gzip-compressed tarball:      <pre><code>tar -xzvf archive_name.tar.gz\n</code></pre>   - Extract a bzip2-compressed tarball:      <pre><code>tar -xjvf archive_name.tar.bz2\n</code></pre>   - Extract an xz-compressed tarball:      <pre><code>tar -xJvf archive_name.tar.xz\n</code></pre></p>"},{"location":"linuxcommands/#gzip","title":"<code>gzip</code>","text":"<p>Compress files using the gzip algorithm.   - Compress a file:      <pre><code>gzip file_name\n</code></pre>   - Decompress a file:      <pre><code>gunzip file_name.gz\n</code></pre>   - Compress a file while keeping the original:      <pre><code>gzip -c file_name &gt; file_name.gz\n</code></pre>   - List the contents of a gzip-compressed file:      <pre><code>zcat file_name.gz\n</code></pre></p>"},{"location":"linuxcommands/#bzip2","title":"<code>bzip2</code>","text":"<p>Compress files using the bzip2 algorithm, which typically achieves better compression than gzip.   - Compress a file:      <pre><code>bzip2 file_name\n</code></pre>   - Decompress a file:      <pre><code>bunzip2 file_name.bz2\n</code></pre>   - Compress a file while keeping the original:      <pre><code>bzip2 -c file_name &gt; file_name.bz2\n</code></pre>   - List the contents of a bzip2-compressed file:      <pre><code>bzcat file_name.bz2\n</code></pre></p>"},{"location":"linuxcommands/#zip","title":"<code>zip</code>","text":"<p>Create a ZIP archive, which can include multiple files and directories.   - Create a ZIP archive:      <pre><code>zip archive_name.zip file1 file2 directory_name\n</code></pre>   - Add files to an existing ZIP archive:      <pre><code>zip archive_name.zip newfile\n</code></pre>   - Extract a ZIP archive:      <pre><code>unzip archive_name.zip\n</code></pre>   - List the contents of a ZIP archive:      <pre><code>unzip -l archive_name.zip\n</code></pre></p>"},{"location":"linuxcommands/#7z","title":"<code>7z</code>","text":"<p>Use the 7-Zip compression tool for high compression ratios and support for many formats.   - Create a 7z archive:      <pre><code>7z a archive_name.7z file1 file2 directory_name\n</code></pre>   - Extract a 7z archive:      <pre><code>7z x archive_name.7z\n</code></pre>   - List the contents of a 7z archive:      <pre><code>7z l archive_name.7z\n</code></pre></p> <p>These tools and commands cover a broad range of archiving and compression needs, from simple file compression to handling more complex multi-file archives.</p>"},{"location":"linuxcommands/#finding-files","title":"Finding Files","text":"<ul> <li><code>find</code>: Search for files in a directory hierarchy.   <pre><code>find /path -name \"file_name\"\n</code></pre></li> <li><code>locate</code>: Quickly find files by name using an indexed database.   <pre><code>locate file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#system-monitoring-and-performance","title":"System Monitoring and Performance","text":"<ul> <li><code>htop</code>: Interactive process viewer (more user-friendly than <code>top</code>).   <pre><code>htop\n</code></pre></li> <li><code>vmstat</code>: Report virtual memory statistics.   <pre><code>vmstat 1   # refreshes every second\n</code></pre></li> <li><code>iostat</code>: Report CPU and I/O statistics.   <pre><code>iostat\n</code></pre></li> </ul>"},{"location":"linuxtutorial/","title":"Linuxtutorial","text":"<p>All credits for this HANDS-ON go to Mag Selwa from KULeuven. Check out the whole repo including slides.</p>"},{"location":"linuxtutorial/#hands-on-1","title":"HANDS-ON 1","text":"Task Hint 1. Open the VM and play Linux (Ubuntu) Show Hint Check the Applications menu. Run the command line (Show applications -&gt; Terminal). 2. Identify if you are a normal user or a superuser Show Hint Try to switch to root (superuser) and report what happens. <code>whoami</code>, <code>su</code> 3. Identify your shell Show Hint <code>echo $SHELL</code> 4. Check the kernel version, system distribution, and glibc version Show Hint <code>uname -r</code>, <code>uname -a</code>, <code>ls -l /lib/i386-linux-gnu/libc.so*</code>"},{"location":"linuxtutorial/#hands-on-2","title":"HANDS-ON 2","text":"Task Hint 1. Start Terminal and display the list of your files and directories Show Hint <code>ls</code> 2. Check more options about the <code>ls</code> command and display all files Show Hint <code>ls --help</code>, <code>man ls</code>, <code>info ls</code>, <code>ls -a</code> 3. Clear the screen Show Hint <code>clear</code> 4. Compare the information given by different kinds of help about <code>pwd</code> command Show Hint <code>whatis pwd</code>, <code>help pwd</code>, <code>man pwd</code>, <code>info pwd</code> 5. Navigate directories: Downloads, Desktop, and back to home Show Hint <code>cd Downloads</code>, <code>ls</code>, <code>cd ../Desktop</code>, <code>ls</code>, <code>cd ../Downloads</code>, <code>cd ~</code>, <code>cd</code> 6. Review a few used commands with arrow and compare with output from history Show Hint <code>history</code> 7. Print the date on the screen and save it to a file Show Hint <code>date</code>, <code>date &gt; date.txt</code>, <code>date &gt;&gt; date.txt</code> 8. Exit the shell Show Hint <code>exit</code> 9. Start terminal again and perform directory operations Show Hint <code>pwd</code>, <code>cd Desktop</code>, <code>pwd</code>, <code>cd ..</code>, <code>pwd</code>, <code>cd /tmp</code>, <code>pwd</code>, <code>cd /home/student</code>, <code>ls /</code> 10. Display more information about <code>date.txt</code> file Show Hint <code>ls -l d*</code>, <code>stat date.txt</code>"},{"location":"linuxtutorial/#hands-on-3","title":"HANDS-ON 3","text":"Task Hint 1. Create and manipulate directories and files Show Hint <code>mkdir CourseLinux</code>, <code>cp -r -v -i /usr/share/icons/ ./CourseLinux</code>, <code>mkdir CourseLinux/test</code>, <code>mkdir CourseLinux/test1</code>, <code>mv CourseLinux/test1 CourseLinux/test2</code>, <code>cp -r -v -i /usr/share/icons/* CourseLinux/test</code>, <code>mv CourseLinux/test/unity-icon-theme/apps/128/photos.svg CourseLinux/test/fl.svg</code>, <code>cp -v -i CourseLinux/test/fl.svg fl2.svg</code>, <code>display fl2.svg</code>, <code>ln -s CourseLinux/test/fl.svg mylink2file</code>, <code>display mylink2file</code>, <code>rm -i CourseLinux/test/fl.svg</code>, <code>display mylink2file</code> 2. Clear the screen and perform file operations Show Hint <code>clear</code>, <code>cd CourseLinux</code>, <code>wget https://raw.githubusercontent.com/hpculeuven/Linux-intro/master/tabel.dat</code>, <code>wget https://raw.githubusercontent.com/hpculeuven/Linux-intro/master/matstats.log</code>, <code>cat tabel.dat</code>, <code>less matstats.log</code>, <code>tail -30 matstats.log</code>, <code>head matstats.log</code>, <code>grep ardaa matstats.log</code>, <code>grep ardbb matstats.log</code>, <code>ls -al | less</code>, <code>ls -al | grep tabel.dat</code> 3. Create a file called <code>test4edit</code> and edit it Show Hint <code>touch test4edit</code>, <code>gedit test4edit</code>, <code>nano test4edit</code>, <code>vi test4edit</code>"},{"location":"linuxtutorial/#hands-on-4","title":"HANDS-ON 4","text":"Task Hint 1. Show the path of gcc command and perform various operations Show Hint <code>which gcc</code>, <code>whereis gcc | grep lib</code>, <code>whoami</code>, <code>echo \"I like Linux\" | tr -d 'I' | tr a-z A-Z</code>, <code>bc</code>, <code>date &gt; date.txt</code>, <code>date &gt;&gt; date.txt</code>, <code>cp date.txt date1.txt</code>, <code>echo \"I like Linux\" &gt;&gt; date1.txt</code>, <code>echo \"And I do not\" &gt;&gt; date.txt</code>, <code>diff date.txt date1.txt</code>, <code>du -kah CourseLinux</code>, <code>wc date.txt</code> 2. Clear the screen, archive and unpack directories Show Hint <code>clear</code>, <code>cd ~/CourseLinux</code>, <code>ls ico*</code>, <code>tar -cvf ico.tar icons/</code>, <code>tar -czvf ico.tar.gz icons/</code>, <code>mkdir newtest</code>, <code>cd newtest</code>, <code>cp ../ico.tar.gz .</code>, <code>tar -xzvf ico.tar.gz</code> 3. Work with permissions and directory contents Show Hint <code>cd ~/CourseLinux</code>, <code>mkdir testfiles</code>, <code>cd testfiles</code>, <code>touch file1</code>, <code>gedit file1</code>, <code>cp file1 file2</code>, <code>chmod u-w file2</code>, <code>ls -la</code>, <code>gedit file2</code>, <code>cd ..</code>, <code>chmod u-w testfiles</code>, <code>ls -la</code>, <code>cp testfiles/file1 testfiles/file3</code>, <code>chmod o-rwx testfiles</code>, <code>ls -la</code>, <code>ls testfiles</code>, <code>chmod u-r testfiles</code>, <code>ls -la</code>, <code>cd testfiles</code>, <code>cd ..</code>, <code>chmod u-x testfiles</code>, <code>ls -la</code>, <code>cd testfiles</code> 4. Manage processes and run applications Show Hint <code>clear</code>, <code>ps -aux</code>, <code>gedit</code>, <code>ps -u student | grep gedit</code>, <code>kill &lt;pid&gt;</code>, <code>gedit &amp;</code>"},{"location":"linuxtutorial/#summary","title":"Summary","text":"<ul> <li>All the Hint sections have been updated with <code>&lt;details&gt;</code> and <code>&lt;summary&gt;</code> tags for collapsible content.</li> <li>Users can click on \"Show Hint\" to reveal the hint and click again to collapse it.</li> </ul> <p>Make sure to rebuild your MkDocs project after updating the Markdown file to see the changes.</p>"},{"location":"nanopore-tools/","title":"Nanopore Tools","text":"<p>All nanopore proprietary software can be found in the software &amp; Download section on the website of nanopore.   </p>"},{"location":"nanopore-tools/#sequencing-minkow","title":"Sequencing: Minkow","text":"<p>The basic setup for sequencing in the software package called <code>Minknow</code>. Minknow can both operate the sequencing device as well as running real time basecalling. The installation guide provided in the the software &amp; Download section gives you an overview of all necessary commands to issue for installing the software.  </p>"},{"location":"nanopore-tools/#default-installation-directories","title":"Default installation directories","text":"<ol> <li>For the MinKNOW software: <code>/opt/ont/minknow</code> </li> <li>For the MinKNOW user interface: <code>/opt/ont/minknow-ui</code> </li> <li>Location of the reads folder: <code>/var/lib/minknow/data</code> </li> <li>Location of the log files: The MinKNOW logs are located in <code>/var/log/minknow</code> The Dorado basecaller logs are located in <code>/var/log/dorado</code> </li> </ol>"},{"location":"nanopore-tools/#run-minknow-as-root","title":"Run Minknow as root","text":"<p>When running minknow you'll notice that when issuing another path for your reads folder, minkow will issue an error. Redirecting output from the standard <code>/var/lib/minknow/data</code> might be necessary when you run out of diskspace or simply when you prefer to output your data elsewhere. To enable this, you'll need to give the <code>minknow.service</code> writing privileges i.e. make this root. Make minknow.service root<pre><code>        $ sudo service minknow stop\n        $ sudo perl -i -pe 's/(User|Group)=minknow/$1=root/'/lib/systemd/system/minknow.service\n        $ sudo systemctl daemon-reload\n        $ sudo service minknow start\n</code></pre></p>"},{"location":"nanopore-tools/#basecalling-dorado","title":"Basecalling: Dorado","text":"<p>Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads. The tool can be installed as a stand alone tool which provides more functionalities than basecalling in Minknow. You can find the Dorado github repo here.  </p>"},{"location":"nanopore-tools/#dorado-features","title":"Dorado features","text":"<ol> <li>Download model modules are not packed in the software <pre><code>dordado download --list\n</code></pre></li> <li>Basecaller <pre><code>dorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ &gt; output.bam\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-fastq &gt; output.fastq\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-sam &gt; output.sam\n</code></pre></li> <li>Aligner <pre><code>dorado basecaller -x cuda:0 --reference .your-reference.fasta dna_r10.4.1_e8_400bps_fast@v4.1.0 ./pod5 &gt; aligned.bam\n</code></pre></li> <li>Sequencing summary generation <pre><code>dorado summary ./reads.bam &gt; sequencing_summary.txt\n</code></pre></li> </ol>"},{"location":"system-build/","title":"System setup","text":""},{"location":"system-build/#operating-system","title":"Operating system","text":"<p>Release: Ubuntu 22.04.4 LTS (Jammy Jellyfish) Follow this link to learn all you need to know on how to install ubuntu.</p>"},{"location":"system-build/#gpu-cuda","title":"GPU &amp; CUDA","text":"<p>GPU computational power increased ONT basecalling speed and enabled real-time data analysis. Upon writing this documentation dorado 0.7 is the current basecaller which is also incorperated in the sequencing operating software Minknow. Before we get to the installation of dedicated sequencing software and other data analys packages we will focus on installing a GPU and the CUDA toolkit.  </p>"},{"location":"system-build/#installation-on-ubuntu","title":"Installation on Ubuntu","text":"<ul> <li>Add graphics drivers ppa repo <pre><code>sudo add-apt-repository ppa:graphics-drivers/ppa\n</code></pre></li> <li>Install Ubuntu drivers app <pre><code>sudo apt install ubuntu-drivers-common\n</code></pre></li> <li>Check available GPUs <pre><code>ubuntu-drivers devices\n</code></pre> Example output<pre><code>== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==\nmodalias : pci:v000010DEd000027E0sv00001025sd0000166Cbc03sc00i00\nvendor   : NVIDIA Corporation\ndriver   : nvidia-driver-545-open - distro non-free\ndriver   : nvidia-driver-555-open - third-party non-free\ndriver   : nvidia-driver-550 - third-party non-free\ndriver   : nvidia-driver-550-open - third-party non-free\ndriver   : nvidia-driver-545 - distro non-free\ndriver   : nvidia-driver-535 - third-party non-free\ndriver   : nvidia-driver-555 - third-party non-free recommended\ndriver   : nvidia-driver-535-server - distro non-free\ndriver   : nvidia-driver-535-open - distro non-free\ndriver   : nvidia-driver-535-server-open - distro non-free\ndriver   : nvidia-driver-525 - third-party non-free\ndriver   : xserver-xorg-video-nouveau - distro free builtin\n</code></pre></li> <li>Install latest Nvidia driver (change <code>555</code> with latest version available in your case) <pre><code>sudo apt install nvidia-driver-525\n</code></pre></li> <li>Reboot your computer  </li> <li>Check if Nvidia driver and CUDA is available after reboot <pre><code>nvidia-smi\n</code></pre> Example output<pre><code>+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4080 ...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   45C    P8               1W /  90W |   1032MiB / 12282MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n</code></pre></li> </ul> <p>This output tells us we have a NVIDIA GeForce RTX 4080 running (12GB virtual RAM) and CUDA Version 12.3 has been co-installed.  </p>"},{"location":"system-build/#purge-nvidia","title":"Purge Nvidia","text":"<p>On some occasions you'll need to perform a fresh installation of nvidia drivers. It is important to purge all currently installed NVIDIA/CUDA before installing anew. Here is a step by step guide on how to do this:  </p> <ul> <li>List all packages The dpkg -l command is used in Debian-based Linux distributions to list all the installed packages. This command provides detailed information about each package, including its status, name, version, and a brief description. <pre><code>dpkg -l\n</code></pre> The output of dpkg -l, the columns represent:  </li> </ul> <ol> <li>Package status (e.g., \"ii\" for installed, \"un\" for uninstalled, etc.)  </li> <li>Package name  </li> <li>Version  </li> <li>Architecture  </li> <li>Description  </li> </ol> <ul> <li>List and purge all NVIDIA/CUDA  </li> </ul> <p><pre><code>sudo apt-get purge $(dpkg -l | grep '^ii.*nvidia'| awk '{print $2}')\n</code></pre> Description<pre><code>dpkg -l | grep '^ii.*nvidia'    # This command lists all installed packages (output of dpkg -l) that have \"nvidia\" in their names and are in the \"installed\" state (lines starting with \"ii\").  \n\nawk '{print $2}'                # This command uses awk to print the second column (package names) from the output of the previous command.  \n\nsudo apt-get purge $(...)       # This command uses command substitution to execute the inner command and pass its output (list of package names) as arguments to apt-get purge, which removes the specified packages.  \n</code></pre></p> <p>.</p>"},{"location":"system-build/#system-software","title":"System Software","text":"<p>On Ubuntu system applications can be installed using the graphical user interface (GUI). More information on how to install software on Ubuntu click here.  </p>"},{"location":"system-build/#bioinformatic-software","title":"Bioinformatic software","text":"<p>Usually bioinformatic software are opensource packages with installation instructions described on Github repositories. First thing you can do is get yourself a github account and get familiarized with github. Executable scripts are often found in the <code>/bin</code>folder, or can be installed in <code>/usr/local/bin</code> or in a dedicated bioinfo folder <code>/usr/local/bioinf/</code>. In the next section I will explain how to install the basecalling tool Dorado in the <code>/bioinf</code>folder and put the script on <code>PATH</code>.  </p> <ul> <li>Goto DORADO GITHUB </li> <li>Download the relevant installer for your platform (like described in the instructions)  </li> <li>Extract the archive to the desired location Extract and put on PATH<pre><code>1. mkdir /usr/local/bioinf/\n2. cd /usr/local/bioinf/\n3. wget https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.7.2-linux-x64.tar.gz\n4. tar -xzvf dorado-0.7.2-linux-x64.tar.gz\n    # after unzipping, the main folder will contain a /bin and /lib folder.\n5. cd bin  \n6. ls\n    # /usr/local/bioinf/dorado/bin/dorado =&gt; this is the Executable file \n7. sudo ln -s /usr/local/bioinf/dorado/bin/dorado /usr/local/bin/dorado\n    # this will symlink the dorado script to /usr/local/bin/dorado\n8. echo 'export PATH=$PATH:/usr/local/bin' | sudo tee -a /etc/profile &amp;&amp; export PATH=$PATH:/usr/local/bin\n    # put in path if not by default, you can check by `echo $PATH`\n</code></pre> Following these instructions you should have downloaded the dorado script and it's libraries in the designated <code>/usr/local/bioinf</code> folder. To make the executable file accessible for all users you need to put it in PATH.</li> </ul>"},{"location":"system-build/#python-environments","title":"Python environments","text":"<p>When you need to install packages using the command <code>pip</code> it is adviced to do so in a separate python environment to avoid issues with the already existing python installation supporting your operating system. Detailed information on python venv can be found here.  </p>"},{"location":"system-build/#conda","title":"Conda","text":"<p>Yet another packaging and environmanagement tool is Conda. A lot of bioinformatic software has been made available in dedicated conda environments. Read all there is to know on how to use conda here. </p>"}]}