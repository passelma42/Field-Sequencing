{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p>Field sequencing outside a lab environment has been made possible by Oxford Nanopore technologies. Combining the portable sequencer called 'MinION Mk1B' and a powerfull laptop (incl. GPU power) one is able to perform in field sequencing and real-time data analysis. The idea of this website is to setup a portable field sequencing workflow covering following topics:  </p> <ul> <li>GPU system setup  </li> <li>Installing software (including ONT sequencing software)  </li> <li>Using <code>BLAST</code>and different databases for taxonomical ID  </li> <li>Wet lab approach for barcoding applications</li> <li>Demo of our field sequencing test at Botanic Garden Ghent  </li> </ul> <p>This is a step by step guide that will empower you to set up a compute workstation and learn all necessary protocols for both wet and dry lab.</p>"},{"location":"Tutorial-Data-Analysis-HPC-old/","title":"Tutorial Data Analysis HPC old","text":"<p>Welcome to the HPC filesystem tutorial. This guide will teach you all the necessary commands and provide you with info on how to work in linux within the context of an HPC environment.  </p> <p>For an extended overview of HPC-documentation I refer to the HPC-DOCS: https://hpcugent.github.io/vsc_user_docs/ </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#1-introduction-to-linux-on-hpc","title":"1.  Introduction to Linux on HPC","text":""},{"location":"Tutorial-Data-Analysis-HPC-old/#11-getting-started","title":"1.1 Getting Started","text":"<p>To get started with the HPC-UGent infrastructure, you need to obtain a VSC account, see HPC manual. Keep in mind that you must keep your private key to yourself! You can look at your public/private key pair as a lock and a key: you give us the lock (your public key), we put it on the door, and then you can use your key to open the door and get access to the HPC infrastructure. Anyone who has your key can use your VSC account! Details on connecting to the HPC infrastructure are available in HPC manual connecting section.  </p> <p>NOTE: If you plan to work in the webbrowser interface only, then you don't need to upload public/private key. For this course, the webbrowse interface only is ok.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#12-the-prompt","title":"1.2 The prompt","text":"<p>The basic interface is the so-called shell prompt, typically ending with <code>$</code> (for bash shells).  </p> <p>NOTE: In the <code>code</code> examples below the <code>$</code> indicates your prompt. This is for information purpose only, you don't need to type this in your commands.  </p> <p>You use the shell by executing commands, and hitting <code>&lt;enter&gt;</code>. For example:</p> <p><pre><code>$ echo hello\nhello\n</code></pre> While typing (long) commands in the terminal, you can go to the start or end of the command line using <code>Ctrl-A</code> or <code>Ctrl-E</code>.</p> <p>To go through previous commands, use <code>&lt;up&gt;</code> and <code>&lt;down&gt;</code>, rather than retyping them.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#13-basic-commands","title":"1.3 Basic Commands","text":"<p>Download example data For this exercise you'll need to connect to the HPC and open a terminal. I've prepared some folders and files on which you can perform following commands.</p> <p>Start by copying the <code>/Linux_Basics_exampledata</code> to your home folder <code>~/</code>.</p> <pre><code>$ cp -r /path/to/ONT-SEQ_PA-Benin2024_Bioinfocourse/Linux_Basics_exampledata ~/  \n</code></pre> <p>Commands &amp; Examples NOTE: Make sure you are in your home folder before you start with the exercises. To go to your home folder use following command: <pre><code>$ cd ~/\n</code></pre> Example 1: Lists files and directories in the current directory. <code>$ ls -l</code></p> <p>Example 2: Changes the current directory. <code>$ cd /user/gent/433/vscXXXX/ONT-SEQ_PA-Benin2024_Bioinfocourse</code> </p> <p>Example 3: Prints the current working directory. <code>$ pwd</code></p> <p>Example 4: Creates a new directory. <code>$ mkdir my_first-folder</code></p> <p>Example 5: Deletes a file. WARNING: There is no trash, files are permanently deleted! <code>$ rm ./old_hello.py</code></p> <p>Example 6: Deletes a directory and its contents recursively. <code>$ rm -r ./old_sequences</code></p> <p>Example 7: Copies files or directories. <code>$ cp samplesheet96.txt ./my_first-folder</code></p> <p>Example 8: Moves or renames files or directories. <code>$ mv ./my_first-folder/samplesheet96.txt ./my_first-folder/samplesheet96_$(date +%Y-%m-%d).txt</code></p> <p>Example 9: Searches for a file by name in a specified path. <code>$ find ~/ -name \"samplesheet96*\"</code></p> <p>Example 10: Displays the contents of a file. <code>$ cat hello.py</code></p> <p>Example 11: Opens a file for viewing one page at a time. Press <code>q</code> to quit. <code>$ less manual_cp.txt</code></p> <p>Example 12: Shows the first 10 lines of a file. <code>$ head manual_cp.txt</code></p> <p>Example 13: Shows the last 10 lines of a file. <code>$ tail manual_cp.txt</code></p> <p>Example 14: Displays detailed information about files, including permissions. <code>$ ls -lh ~/Linux_Basics_exampledata</code></p> <p>Example 15: Shows currently running processes. <code>$ ps</code></p> <p>Example 16: Displays real-time system resource usage. <code>$ top</code></p> <p>Example 17: Interactive process viewer (if installed). <code>$ htop</code></p> <p>Example 18: Prints text to the terminal. <code>$ echo \"Hello, world!\"</code></p> <p>Example 19: Displays the current date and time. <code>$ date</code></p> <p>Example 20: Logs out or closes the terminal. <code>$ exit</code></p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#14-manipulating-files-and-directories","title":"1.4 Manipulating files and directories","text":"<p>Changing permissions: \"chmod\" Each file and directory has particular permissions set on it, which can be queried using ls -l.</p> <p>For example: <pre><code>$ ls -l afile.txt \n-rw-rw-r-- 1 vsc40000 agroup 2929176 Apr 12 13:29 afile.txt \n</code></pre> The -rwxrw-r-- specifies both the type of file (- for files, d for directories (see first character)), and the permissions for user/group/others:</p> <ol> <li>each triple of characters indicates whether the read (r), write (w), execute (x) permission bits are set or not</li> <li>the 1st part rwx indicates that the owner \"vsc40000\" of the file has all the rights</li> <li>the 2nd part rw- indicates the members of the group \"agroup\" only have read/write permissions (not execute)</li> <li>the 3rd part r-- indicates that other users only have read permissions</li> </ol> <p>The default permission settings for new files/directories are determined by the so-called umask setting, and are by default:</p> <ol> <li>read-write permission on files for user/group (no execute), read-only for others (no write/execute)</li> <li>read-write-execute permission for directories on user/group, read/execute-only for others (no write)</li> </ol> <p>Any time you run <code>ls -l</code> you'll see a familiar line of <code>-rwx------</code> or similar combination of the letters <code>r</code>, <code>w</code>, <code>x</code> and <code>-</code> (dashes). These are the permissions for the file or directory. <pre><code>$ ls -l\ntotal 1\n-rw-r--r--. 1 vsc40000 mygroup 4283648 Apr 12 15:13 articleTable.csv\ndrwxr-x---. 2 vsc40000 mygroup 40 Apr 12 15:00 Project_GoldenDragon\n</code></pre></p> <p>Here, we see that <code>articleTable.csv</code> is a file (beginning the line with -) has read and write permission for the user <code>vsc40000</code> (<code>rw-</code>), and read permission for the group <code>mygroup</code> as well as all other users (<code>r--</code> and <code>r--</code>).</p> <p>The next entry is <code>Project_GoldenDragon</code>. We see it is a directory because the line begins with a <code>d</code>. It also has read, write, and execute permission for the <code>vsc40000</code> user (<code>rwx</code>). So that user can look into the directory and add or remove files. Users in the <code>mygroup</code> can also look into the directory and read the files. But they can't add or remove files (<code>r-x</code>). Finally, other users can read files in the directory, but other users have no permissions to look in the directory at all (<code>---</code>).</p> <p>Maybe we have a colleague who wants to be able to add files to the directory. We use <code>chmod</code> to change the modifiers to the directory to let people in the group write to the directory: <pre><code>$ chmod g+w Project_GoldenDragon\n$ ls -l\ntotal 1\n-rw-r--r--. 1 vsc40000 mygroup 4283648 Apr 12 15:13 articleTable.csv\ndrwxrwx---. 2 vsc40000 mygroup 40 Apr 12 15:00 Project_GoldenDragon\n</code></pre></p> <p>The syntax used here is <code>g+x</code> which means group was given write permission. To revoke it again, we use <code>g-w</code>. The other roles are <code>u</code> for user and <code>o</code> for other.</p> <p>You can put multiple changes on the same line: <code>chmod o-rwx,g-rxw,u+rx,u-w somefile</code>. This will take everyone's permission away except the user's ability to read or execute the file.  </p> <p>You can also use the <code>-R</code> flag to affect all the files within a directory, but this is dangerous. It's best to refine your search using find and then pass the resulting list to chmod since it's not usual for all files in a directory structure to have the same permissions.  </p> <p>Example 1: Apply chmod to all files (regular files only) within a directory This will set 644 permissions for all files (not directories) <code>find /path/to/directory -type f -exec chmod 644 {} \\;</code></p> <p>Example 2: Apply chmod to all directories within a directory  This will set 755 permissions for all directories (not files) <code>find /path/to/directory -type d -exec chmod 755 {} \\;</code></p> <p>Example 3: Apply chmod to all files modified in the last 7 days This will search for all files modified in the last 7 days and set 644 permissions <code>find /path/to/directory -type f -mtime -7 -exec chmod 644 {} \\;</code></p> <p>Example 4: Apply chmod to files with a specific extension (e.g., .txt) This will search for all .txt files and set 644 permissions <code>find /path/to/directory -type f -name \"*.txt\" -exec chmod 644 {} \\;</code></p> <p>Example 5: Apply chmod recursively with different permissions for files and directories First, set 644 permissions for all files (regular files) <code>find /path/to/directory -type f -exec chmod 644 {} \\;</code></p> <p>Then, set 755 permissions for all directories <code>find /path/to/directory -type d -exec chmod 755 {} \\;</code></p> <p>1.4.1.1 Exercises: Changing permissions with \"chmod\" </p> <p>Here are some exercises on <code>chmod</code> based on your directory structure. Try each command and observe the changes using <code>ls -l</code> before and after.</p> <p>Exercise 1: Make <code>hello.sh</code> Executable Goal: Grant execute (<code>x</code>) permission to the script so it can run as a program. <pre><code>chmod +x hello.sh\nls -l hello.sh\n</code></pre> Check if the <code>x</code> permission appears for the user (<code>rwxr--r--</code>). Now try running it: <pre><code>./hello.sh\n</code></pre></p> <p>Exercise 2: Remove Read Permissions from <code>manual_cp.txt</code> Goal: Prevent yourself and others from reading the file. <pre><code>chmod a-r manual_cp.txt\nls -l manual_cp.txt\n</code></pre> Now try opening it: <pre><code>cat manual_cp.txt  # Should give a \"Permission denied\" error\n</code></pre> Restore permissions after testing: <pre><code>chmod u+r manual_cp.txt\n</code></pre></p> <p>Exercise 3: Grant Full Access to <code>samplesheet96.txt</code> for Everyone Goal: Allow all users to read, write, and execute <code>samplesheet96.txt</code>. <pre><code>chmod 777 samplesheet96.txt\nls -l samplesheet96.txt\n</code></pre> Now everyone can modify and execute the file. This is not recommended for sensitive files!</p> <p>Exercise 4: Restrict <code>old_hello.py</code> to Read and Write for Owner Only Goal: Make the file private so only the owner can read and modify it. <pre><code>chmod 600 old_hello.py\nls -l old_hello.py\n</code></pre> Now other users cannot read or modify it.</p> <p>Exercise 5: Add a Sticky Bit to <code>sequences</code> Goal: Ensure that only the file owner can delete their own files inside <code>sequences</code>, even if others have write access. <pre><code>chmod +t sequences\nls -ld sequences  # Check for the \"t\" at the end of the permissions (drwxr-xr-t)\n</code></pre> Now, even if other users have write permissions, they cannot delete files they don't own.</p> <p>Exercise 6: Recursively Set Read &amp; Execute for All Users in <code>old_sequences</code> Goal: Ensure all files in <code>old_sequences</code> are readable and executable but not writable by others. <pre><code>chmod -R a+rx old_sequences\nls -l old_sequences/\n</code></pre> Check if all files inside now have <code>r-x</code> for everyone.</p> <p>Exercise 7: Set Group Write Permissions for <code>Tabular-file.tab</code> Goal: Allow your group members to edit the file. <pre><code>chmod g+w Tabular-file.tab\nls -l Tabular-file.tab\n</code></pre> Now, users in the same group (<code>vsc43352</code>) can modify the file.</p> <p>Exercise 8: Remove Execute Permission from <code>hello.py</code> Goal: Prevent accidental execution of a Python script. <pre><code>chmod -x hello.py\nls -l hello.py\n</code></pre> Now, you must explicitly use <code>python3 hello.py</code> instead of <code>./hello.py</code>.</p> <p>Exercise 9: Convert Between Symbolic and Octal Modes </p> <p>Goal: Change <code>samplesheet96_2025-04-02.txt</code> to the same permissions as <code>samplesheet96.txt</code> using octal mode.  </p> <ol> <li>Check the original permissions: <pre><code>ls -l samplesheet96.txt\n</code></pre></li> <li>Use <code>stat</code> to see the octal mode: <pre><code>stat -c \"%a\" samplesheet96.txt\n</code></pre>    Suppose it returns <code>644</code>.</li> <li>Apply the same mode to the new file: <pre><code>chmod 644 samplesheet96_2025-04-02.txt\n</code></pre></li> </ol> <p>Bonus Challenge: Set <code>sequences</code> to 750 Goal: Make <code>sequences</code> accessible only to the owner and group, while others have no access. <pre><code>chmod 750 sequences\nls -ld sequences\n</code></pre> Now only you and your group can access it, while others are blocked.</p> <p>Final Check After completing the exercises, list all files again to see the changes: <pre><code>ls -l\n</code></pre></p> <p>Compressing files (zip, unzip, tar) </p> <p>Files should usually be stored in a compressed file if they're not being used frequently. This means they will use less space and thus you get more out of your quota. Some types of files (e.g., CSV files with a lot of numbers) compress as much as 9:1. The most commonly used compression format on Linux is gzip. To compress a file using gzip, we use: <pre><code>$ ls -lh myfile\n-rw-r--r--. 1 vsc40000 vsc40000 4.1M Dec 2 11:14 myfile\n$ gzip myfile\n$ ls -lh myfile.gz\n-rw-r--r--. 1 vsc40000 vsc40000 1.1M Dec 2 11:14 myfile.gz\n</code></pre> Note: if you zip a file, the original file will be removed. If you unzip a file, the compressed file will be removed. To keep both, we send the data to stdout and redirect it to the target file: <pre><code>$ gzip -c myfile &gt; myfile.gz\n$ gunzip -c myfile.gz &gt; myfile\n</code></pre> \"zip\" and \"unzip\" </p> <p>Windows and macOS seem to favour the zip file format, so it's also important to know how to unpack those. We do this using unzip: <pre><code>$ unzip myfile.zip\n</code></pre> If we would like to make our own zip archive, we use zip: <pre><code>$ zip myfiles.zip myfile1 myfile2 myfile3\n</code></pre> Working with tarballs: \"tar\" </p> <p>Tar stands for \"tape archive\" and is a way to bundle files together in a bigger file.</p> <p>You will normally want to unpack these files more often than you make them. To unpack a <code>.tar</code> file you use: <pre><code>$ tar -xf tarfile.tar\n</code></pre> Often, you will find <code>gzip</code> compressed <code>.tar</code> files on the web. These are called tarballs. You can recognize them by the filename ending in <code>.tar.gz</code>. You can uncompress these using gunzip and then unpacking them using tar. But tar knows how to open them using the <code>-z</code> option: <pre><code>$ tar -zxf tarfile.tar.gz\n$ tar -zxf tarfile.tgz\n</code></pre> Order of arguments </p> <p>Note: Archive programs like <code>zip</code>, <code>tar</code> use arguments in the \"opposite direction\" of copy commands. <pre><code>$ cp source1 source2 source3 target\n$ zip zipfile.zip source1 source2 source3\n$ tar -cf tarfile.tar source1 source2 source3\n</code></pre> If you use <code>tar</code> with the source files first then the first file will be overwritten. You can control the order of arguments of tar if it helps you remember:</p> <p>$ tar -c source1 source2 source3 -f tarfile.tar Exercises: Compressing files (zip, unzip, tar)</p> <p>Here are some zip, unzip, and tar exercises based on your directory structure. Each exercise includes commands and explanations.</p> <p>Exercise 1: Create a <code>.zip</code>file from a local file. Goal: Create a <code>.zip</code> archive containing <code>manual_cp.txt</code>. <pre><code>zip manual_cp.zip manual_cp.txt\nls -l manual_cp.zip\n</code></pre> Now the file is compressed into <code>manual_cp.zip</code>.</p> <p>Exercise 2: Extract <code>manual_cp.zip</code> Goal: Extract the zipped file back to its original form. <pre><code>unzip manual_cp.zip\nls -l\n</code></pre> The extracted file should appear in the directory.</p> <p>Exercise 3: Compress Multiple Files into One ZIP Archive Goal: Create a <code>.zip</code> file containing <code>hello.py</code>, <code>hello.sh</code>, and <code>Tabular-file.tab</code>. <pre><code>zip my_archive.zip hello.py hello.sh Tabular-file.tab\nls -l my_archive.zip\n</code></pre> All three files are stored in <code>my_archive.zip</code>.</p> <p>Exercise 4: Compress the <code>sequences</code> Directory Using ZIP Goal: Create a <code>.zip</code> archive of the <code>sequences</code> directory. <pre><code>zip -r sequences.zip sequences\nls -l sequences.zip\n</code></pre> The <code>-r</code> flag ensures that all files inside the directory are included.</p> <p>Exercise 5: Extract <code>sequences.zip</code> Goal: Extract the entire directory from the <code>.zip</code> archive. <pre><code>unzip sequences.zip\nls -l\n</code></pre> The <code>sequences</code> directory is restored.</p> <p>Exercise 6: Create a Tarball (<code>.tar</code>) Without Compression Goal: Archive multiple files into a <code>.tar</code> file without compression. <pre><code>tar -cf archive.tar hello.py hello.sh Tabular-file.tab\nls -l archive.tar\n</code></pre> The <code>.tar</code> file now contains all three files but is not compressed.</p> <p>Exercise 7: Create a Gzipped Tarball (<code>.tar.gz</code>) Goal: Archive and compress the <code>old_sequences</code> directory. <pre><code>tar -czf old_sequences.tar.gz old_sequences\nls -l old_sequences.tar.gz\n</code></pre> The <code>-z</code> flag enables gzip compression, creating a smaller file.</p> <p>Exercise 8: Extract a <code>.tar.gz</code> File Goal: Extract the <code>old_sequences.tar.gz</code> archive. <pre><code>tar -xzf old_sequences.tar.gz\nls -l\n</code></pre> The <code>old_sequences</code> directory is restored.</p> <p>Exercise 9: List Contents of a <code>.tar.gz</code> File Without Extracting Goal: View files inside <code>old_sequences.tar.gz</code> without extracting. <pre><code>tar -tzf old_sequences.tar.gz\n</code></pre> This shows all files inside the tarball.</p> <p>Exercise 10: Extract Only <code>samplesheet96.txt</code> From <code>archive.tar</code> Goal: Extract a single file from a tar archive. <pre><code>tar -xf archive.tar samplesheet96.txt\nls -l samplesheet96.txt\n</code></pre> Only <code>samplesheet96.txt</code> is extracted.</p> <p>Bonus Challenge: Tar a Directory and Extract It to Another Location Goal: Archive <code>sequences</code> and extract it elsewhere. <pre><code>tar -czf sequences.tar.gz sequences\nmkdir test_restore\ntar -xzf sequences.tar.gz -C test_restore\nls test_restore/\n</code></pre> The <code>sequences</code> directory is extracted into <code>test_restore</code> instead of the current directory.</p> <p>Final Check After completing the exercises, list your directory contents: <pre><code>ls -l\n</code></pre></p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#15-software-packages-on-hpc","title":"1.5 Software packages on HPC","text":"<p>All software on the HPC is available through <code>modules</code> which need to be activated first before you can start using them. Follow the steps below to learn how to work with these commands.  </p> <p>Exercise 1: Use the command <code>module</code> to list all activated modules </p> <ol> <li>Open a terminal and an interactive session! (see \"02-Day1-HPC-intro_2025.ppx\" if you don't remember how to do this)</li> <li>Check which modules are currently loaded/activated on your system. The command \u00b4ml\u00b4 you'll see below is short for <code>module</code>. <pre><code>ml list\n</code></pre> OUTPUT<pre><code>Currently Loaded Modules:\n  1) env/vsc/doduo (S)   2) env/slurm/doduo (S)   3) env/software/doduo (S)   4) cluster/doduo (S)\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n</code></pre> Note: This tells you that you are working on the doduo cluster and all basic modules are loaded but no software packages are yet activated </li> </ol> <p>Exercise 2: Use the command <code>ml spider</code> to see if your favourite software package is available </p> <p>Run following command to check if a nanpack module is available. <pre><code>ml spider nanopack\n</code></pre></p> <p>OUTPUT<pre><code>-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  NanoPack: NanoPack/20230602-foss-2023a\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Collection of long read processing and analysis tools.\n\n\n    You will need to load all module(s) on any one of the lines below before the \"NanoPack/20230602-foss-2023a\" module is available to load.\n\n      env/software/accelgor\n      env/software/doduo\n      env/software/donphan\n      env/software/gallade\n      env/software/joltik\n      env/software/litleo\n      env/software/shinx\n\n    Help:\n      Description\n      ===========\n      Collection of long read processing and analysis tools.\n\n\n      More information\n      ================\n       - Homepage: https://daler.github.io/pybedtools\n</code></pre> Note: Here you see the full name of the package \"NanoPack/20230602-foss-2023a\" and all the cluster on which you can activate this module/software package </p> <ol> <li>Activate the nanopack module <pre><code>ml NanoPack/20230602-foss-2023a\n</code></pre></li> <li>Check if the module is loaded   </li> </ol> <pre><code>vsc43352@gligar09:~$ml list\n</code></pre> <p>OUTPUT<pre><code>Currently Loaded Modules:\n  1) env/vsc/doduo                    (S)  26) foss/2023a                                 51) statsmodels/0.14.1-gfbf-2023a            76) Tkinter/3.11.3-GCCcore-12.3.0\n  2) env/slurm/doduo                  (S)  27) bzip2/1.0.8-GCCcore-12.3.0                 52) gzip/1.12-GCCcore-12.3.0                 77) NASM/2.16.01-GCCcore-12.3.0\n  3) env/software/doduo               (S)  28) ncurses/6.4-GCCcore-12.3.0                 53) lz4/1.9.4-GCCcore-12.3.0                 78) libjpeg-turbo/2.1.5.1-GCCcore-12.3.0\n  4) cluster/doduo                    (S)  29) libreadline/8.2-GCCcore-12.3.0             54) zstd/1.5.5-GCCcore-12.3.0                79) jbigkit/2.1-GCCcore-12.3.0\n  5) GCCcore/12.3.0                        30) Tcl/8.6.13-GCCcore-12.3.0                  55) ICU/73.2-GCCcore-12.3.0                  80) libdeflate/1.18-GCCcore-12.3.0\n  6) zlib/1.2.13-GCCcore-12.3.0            31) SQLite/3.42.0-GCCcore-12.3.0               56) Boost/1.82.0-GCC-12.3.0                  81) LibTIFF/4.5.0-GCCcore-12.3.0\n  7) binutils/2.40-GCCcore-12.3.0          32) libffi/3.4.4-GCCcore-12.3.0                57) snappy/1.1.10-GCCcore-12.3.0             82) giflib/5.2.1-GCCcore-12.3.0\n  8) GCC/12.3.0                            33) Python/3.11.3-GCCcore-12.3.0               58) RapidJSON/1.1.0-20230928-GCCcore-12.3.0  83) libwebp/1.3.1-GCCcore-12.3.0\n  9) numactl/2.0.16-GCCcore-12.3.0         34) gfbf/2023a                                 59) Abseil/20230125.3-GCCcore-12.3.0         84) OpenJPEG/2.5.0-GCCcore-12.3.0\n 10) XZ/5.4.2-GCCcore-12.3.0               35) cffi/1.15.1-GCCcore-12.3.0                 60) RE2/2023-08-01-GCCcore-12.3.0            85) LittleCMS/2.15-GCCcore-12.3.0\n 11) libxml2/2.11.4-GCCcore-12.3.0         36) cryptography/41.0.1-GCCcore-12.3.0         61) utf8proc/2.8.0-GCCcore-12.3.0            86) Pillow/10.0.0-GCCcore-12.3.0\n 12) libpciaccess/0.17-GCCcore-12.3.0      37) virtualenv/20.23.1-GCCcore-12.3.0          62) Arrow/14.0.1-gfbf-2023a                  87) Qhull/2020.2-GCCcore-12.3.0\n 13) hwloc/2.9.1-GCCcore-12.3.0            38) Python-bundle-PyPI/2023.06-GCCcore-12.3.0  63) Kaleido/0.2.1-GCCcore-12.3.0             88) matplotlib/3.7.2-gfbf-2023a\n 14) OpenSSL/3                             39) pybind11/2.11.1-GCCcore-12.3.0             64) NanoPlot/1.43.0-foss-2023a               89) joypy/0.2.6-foss-2023a\n 15) libevent/2.1.12-GCCcore-12.3.0        40) SciPy-bundle/2023.07-gfbf-2023a            65) nodejs/18.17.1-GCCcore-12.3.0            90) NanoComp/1.24.0-foss-2023a\n 16) UCX/1.14.1-GCCcore-12.3.0             41) nanomath/1.4.0-foss-2023a                  66) plotly-orca/1.3.1-GCCcore-12.3.0         91) libyaml/0.2.5-GCCcore-12.3.0\n 17) PMIx/4.2.4-GCCcore-12.3.0             42) Biopython/1.83-foss-2023a                  67) libpng/1.6.39-GCCcore-12.3.0             92) PyYAML/6.0-GCCcore-12.3.0\n 18) UCC/1.2.0-GCCcore-12.3.0              43) cURL/8.0.1-GCCcore-12.3.0                  68) Brotli/1.0.9-GCCcore-12.3.0              93) Pillow-SIMD/9.5.0-GCCcore-12.3.0\n 19) OpenMPI/4.1.5-GCC-12.3.0              44) Pysam/0.22.0-GCC-12.3.0                    69) freetype/2.13.0-GCCcore-12.3.0           94) tornado/6.3.2-GCCcore-12.3.0\n 20) OpenBLAS/0.3.23-GCC-12.3.0            45) nanoget/1.19.3-foss-2023a                  70) expat/2.5.0-GCCcore-12.3.0               95) bokeh/3.2.2-foss-2023a\n 21) FlexiBLAS/3.3.1-GCC-12.3.0            46) NanoStat/1.6.0-foss-2023a                  71) util-linux/2.39-GCCcore-12.3.0           96) nanoQC/0.9.4-foss-2023a\n 22) FFTW/3.3.10-GCC-12.3.0                47) NanoFilt/2.8.0-foss-2023a                  72) fontconfig/2.14.2-GCCcore-12.3.0         97) kyber/0.4.0-GCC-12.3.0\n 23) gompi/2023a                           48) minimap2/2.26-GCCcore-12.3.0               73) xorg-macros/1.20.0-GCCcore-12.3.0        98) NanoPack/20230602-foss-2023a\n 24) FFTW.MPI/3.3.10-gompi-2023a           49) NanoLyse/1.2.1-foss-2023a                  74) X11/20230603-GCCcore-12.3.0\n 25) ScaLAPACK/2.2.0-gompi-2023a-fb        50) plotly.py/5.16.0-GCCcore-12.3.0            75) Tk/8.6.13-GCCcore-12.3.0\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n</code></pre> Note: Besides the Nanopack/20230602-foss-2023a module you'll see a lot more is added to the list. These are all the dependencies Nanopack needs to be able to function properly.</p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#2-nanopore-data","title":"2. Nanopore data","text":"<p>Remember that each of you extracted DNA and did PCR on his/her own collectd fieldsamples in Benin last year. You can find a more detailed list with extra information in the teams General channel: <code>Document &gt; General &gt; Specieslist-overview</code> </p> <p>Before we start any Analysis on this data we first need to learn how to interprete the Nanopore sequence output and learn how to use the different tools. For this, I prepared a test data set (see below) with sequence output from 2 barcodes, i.e. 2 samples. Once you mastered all the skills you need, we can continue with a more extensive dataset.  </p> <p>To keep the running time of the analysis minimal (more data is longer runtime), I suggest to select no more than 10 Barcodes each. But first let's have a look at the data structure and start with the test data.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#21-data-structure","title":"2.1 Data structure","text":"<p>But first things first. Have a look at the content of the input data folder /ONT-SEQ_PA-Benin2024_Input-Data/. You should be able to navigate to this folder by now. Hint: you can use the <code>ls</code> command, or <code>cd</code> to access the directory and use the command <code>tree</code> to create an overview of the content. </p> <p>Main folder The structure looks like this: <pre><code>path/to/ONT-SEQ_PA-Benin2024_Input-Data/\n.\n+-- ONT-SEQ-24_PA-01\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   \\-- barcode24\n|   +-- reference.fa\n|   +-- report_AMF003_20241018_1203_c8d4ac6d.html\n|   \\-- samplesheet_ONT-SEQ-24_01-withreference.txt\n+-- ONT-SEQ-24_PA-02\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   +-- barcode24\n|   +-- reference.fa\n|   +-- report_axf308_20241022_1153_b65829bf.html\n|   \\-- samplesheet_ONT-SEQ-24_02-withreference.txt\n+-- ONT-SEQ-25_PA-04\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   +-- barcode96\n|   +-- report_FBA60703_20250120_1336_292963a6.html\n|   \\-- samplesheet_ONT-SEQ-25_04.txt\n+-- ONT-SEQ-25_PA-05\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   +-- barcode48\n|   +-- report_FBA60703_20250128_1308_e2391328.html\n|   \\-- samplesheet_ONT-SEQ-25_05.txt\n+-- information\n\\-- scripts\n</code></pre> fastq-pass folder Each <code>barcode</code> folder contains the reads for this particular barcode in <code>fastq.gz</code> format:  </p> <pre><code>../fastq_pass/barcode01$tree\n.\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_0.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_10.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_11.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_12.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_13.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_14.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_15.fastq.gz\n</code></pre> <p>4 Sequence experiments were performed on your samples: ONT-SEQ-24_PA-0X. You can find more detailed list with extra information in the teams General channel: <code>Document &gt; General &gt; Specieslist-overview</code> </p> <ol> <li>ONT-SEQ-24_PA-01: 24 amplicons - ITS+LSU sequences <code>(*)</code> </li> <li>ONT-SEQ-24_PA-02: 24 amplicons - ITS+LSU sequences <code>(*)</code> </li> <li>ONT-SEQ-24_PA-04: 96 amplicons - ITS sequences  </li> <li>ONT-SEQ-24_PA-05: 48 amplicons - ITS  </li> </ol> <p><code>(*)</code> That means both ITS and LSU amplicon products for the same sample where mixed befor sequencing. The bioinformatic tool we will able to construct both ITS and LSU sequences at the same time.  </p> <p>The 2 other experiments (5 and 6) contain only ITS sequences per specimen.</p> <p>Sequence folder </p> <p>This folder contains the following:  </p> <ul> <li><code>fastq_pass</code> the passed fastq files i.e. your sequence reads  </li> <li><code>reference.fa</code> a reference file with an ITS and a LSU sequence, only for the reads from 1st and 2nd experiment  </li> <li><code>report...html</code> a sequence report file, general info on the sequence run (including all samples)  </li> <li><code>samplesheet_....txt</code>a samplesheet (with the MBB number and sequence barcode).  </li> </ul> <pre><code>/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01$tree -L 1\n.\n+-- fastq_pass\n+-- reference.fa\n+-- report_AMF003_20241018_1203_c8d4ac6d.html\n\\-- samplesheet_ONT-SEQ-24_01-withreference.txt\n</code></pre> <p>Take a look inside the folder and the samplesheet file.  *Hint: use the command <code>cd</code>, <code>ls</code> and <code>cat</code> You will notice that opening a <code>html</code> file is not possible in the terminal. To look at this file, you'll need to download it.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#22-practice-with-test-data","title":"2.2 Practice with test data","text":"<p>Before we start with the actual field data, we first are going to run a test dataset, to see if everything runs smoothly.  </p> <p>Download the test data set </p> <ol> <li>Copy the folder <code>/ONT-SEQ_PA-Benin2024_Input-Data/test_data</code> to your home folder  </li> <li>Verify the content and check if you have the sequence folders, the samplesheet and the reference file (for variant mode).  </li> </ol> <pre><code>vsc43352@gligar09:~$ls\ntest_data\nvsc43352@gligar09:~$ cd test_data\nvsc43352@gligar09:~$/test_data$tree -L 2\n.\n+-- consensus_mode\n|\u00a0\u00a0 +-- barcode01\n|\u00a0\u00a0 +-- barcode02\n|\u00a0\u00a0 \\-- samplesheet_ONT-SEQ-25_05.txt\n\\-- variant_mode\n    +-- barcode12\n    +-- barcode14\n    +-- reference.fa\n    \\-- samplesheet_ONT-SEQ-24_02-withreference.txt\n\n6 directories, 3 files\n</code></pre>"},{"location":"Tutorial-Data-Analysis-HPC-old/#221-nanopack-quality-control","title":"2.2.1 NanoPack: Quality Control","text":"<p>Nanopack is a tool set of different long read processing and analysis tools. With these tools you can assess the quality of your sequencerun and of 'individual' sequence reads.  </p> <p>Tool: https://github.com/wdecoster/nanopack Publication: NanoPack: visualizing and processing long-read sequencing data </p> <p>Wouter De Coster, Svenn D\u2019Hert, Darrin T Schultz, Marc Cruts, Christine Van Broeckhoven, NanoPack: visualizing and processing long-read sequencing dta, Bioinformatics, Volume 34, Issue 15, August 2018, Pages 2666\u20132669, https://doi.org/10.1093/bioinformatics/bty149  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#222-run-nanopack-beginner","title":"2.2.2 Run Nanopack: Beginner","text":"<p>Exercise 1: Use the command <code>Nanostat</code> to check statistics on your sequence data. Generating Basic Statistics from a FASTQ File. You are provided with a compressed FASTQ file (<code>reads.fastq.gz</code>). Generate a statistics report and save the output in a folder called <code>statreports</code> and filname <code>nanostats.out</code>.</p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4.</li> <li>When working on the HPC you first need to \"activate\" the Software <pre><code>$ module load NanoPack/20230602-foss-2023a\n</code></pre></li> <li>Run the command. Command<pre><code>NanoStat --fastq *fastq.gz --outdir statreports -n nanostats.out\n</code></pre> Note: the \u00b4\u00b4 in the command is making sure the NanoStat command is beeing executed on all files present in this folder.</li> <li>Check the output in the newly created subfolder \u00b4statsreports\u00b4 OUTPUT<pre><code>/test_data/consensus_mode/barcode01/statreports$ls\nnanostats.out\n/test_data/consensus_mode/barcode01/statreports$cat nanostats.out \nGeneral summary:         \nMean read length:                 438.6\nMean read quality:                 11.6\nMedian read length:               425.0\nMedian read quality:               12.6\nNumber of reads:               24,302.0\nRead length N50:                  511.0\nSTDEV read length:                154.7\nTotal bases:               10,659,020.0\nNumber, percentage and megabases of reads above quality cutoffs\n&gt;Q10:   21811 (89.7%) 10.0Mb\n&gt;Q15:   2256 (9.3%) 1.4Mb\n&gt;Q20:   3 (0.0%) 0.0Mb\n&gt;Q25:   0 (0.0%) 0.0Mb\n&gt;Q30:   0 (0.0%) 0.0Mb\nTop 5 highest mean basecall quality scores and their read lengths\n1:      21.4 (530)\n2:      21.0 (705)\n3:      20.3 (585)\n4:      19.7 (274)\n5:      19.3 (467)\nTop 5 longest reads and their mean basecall quality score\n1:      999 (14.6)\n2:      970 (13.0)\n3:      943 (13.1)\n4:      902 (12.9)\n5:      888 (14.9)\n</code></pre> Hint: These commands are usefull here \u00b4cd\u00b4, \u00b4ls\u00b4, \u00b4cat\u00b4 </li> </ol> <p>Exercise 2: Use the command <code>NanoPlot</code> to create visual plots of your data. </p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4. </li> <li>Run the Command Command<pre><code>NanoPlot  --fastq *.fastq.gz -o NanoPlot-output\n</code></pre></li> <li>Check the output OUTPUT<pre><code>/test_data/consensus_mode/barcode01/NanoPlot-output$ll\ntotal 3265\n-rw-r--r-- 1 vsc43352 vsc43352  463602 Apr 24 15:07 LengthvsQualityScatterPlot_dot.html\n-rw-r--r-- 1 vsc43352 vsc43352   51397 Apr 24 15:07 LengthvsQualityScatterPlot_dot.png\n-rw-r--r-- 1 vsc43352 vsc43352  689449 Apr 24 15:07 LengthvsQualityScatterPlot_kde.html\n-rw-r--r-- 1 vsc43352 vsc43352  121391 Apr 24 15:07 LengthvsQualityScatterPlot_kde.png\n-rw-r--r-- 1 vsc43352 vsc43352    6049 Apr 24 15:07 NanoPlot_20250424_1507.log\n-rw-r--r-- 1 vsc43352 vsc43352 1362962 Apr 24 15:07 NanoPlot-report.html\n-rw-r--r-- 1 vsc43352 vsc43352     788 Apr 24 15:07 NanoStats.txt\n-rw-r--r-- 1 vsc43352 vsc43352    8079 Apr 24 15:07 Non_weightedHistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   30027 Apr 24 15:07 Non_weightedHistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352    8383 Apr 24 15:07 Non_weightedLogTransformed_HistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   31773 Apr 24 15:07 Non_weightedLogTransformed_HistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352    8102 Apr 24 15:07 WeightedHistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   28858 Apr 24 15:07 WeightedHistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352    8524 Apr 24 15:07 WeightedLogTransformed_HistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   30033 Apr 24 15:07 WeightedLogTransformed_HistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352  167169 Apr 24 15:07 Yield_By_Length.html\n-rw-r--r-- 1 vsc43352 vsc43352   37288 Apr 24 15:07 Yield_By_Length.png\n</code></pre> Note: You can download the NanoPlot-report.html file to your computer and open it locally. </li> </ol> <p>Exercise 3: Use the command <code>Chopper</code> to filter data based on read quality. </p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4. </li> <li>Locate and load the <code>chopper</code> Module. Check if is has been activated Commands to use<pre><code>ml spider chopper\nml chopper/0.9.0-GCCcore-12.3.0\nmodule list\n</code></pre></li> <li>Run the command COMMAND<pre><code>cat *.fastq.gz &gt; allreads.fastq.gz &amp;&amp; chopper -q 15 -i allreads.fastq.gz &gt; Q15_filtered.reads.fastq\n</code></pre> Where:<ul> <li><code>cat *.fastq.gz &gt; allreads.fastq.gz</code> =&gt; Concatenate all fastq.gz files in the folder to one file <code>allreads.fastq.gz</code>.</li> <li><code>&amp;&amp;</code> =&gt; this can be used to peform 2 consecutive commands one after the other, but performs the 2nd only when the 1st is finished successfully.   </li> <li><code>chopper</code> takes the file allreads.fastq.gz, performs a quality filtering of Q15 and writes the reads to a new file Q15_filtered.reads.fastq</li> </ul> </li> <li>Check the output OUTPUT<pre><code>/test_data/consensus_mode/barcode01$cat *.fastq.gz &gt; allreads.fastq.gz &amp;&amp; chopper -q 15 -i allreads.fastq.gz &gt; Q15_filtered.reads.fastq                                                                                                                                                                                            \nKept 2256 reads out of 24302 reads\n/test_data/consensus_mode/barcode01$ls\nallreads.fastq.gz                                     FBA60703_pass_barcode01_e2391328_167a61ad_3.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_7.fastq.gz  Q15_filtered.reads.fastq\nFBA60703_pass_barcode01_e2391328_167a61ad_0.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_4.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_8.fastq.gz  statreports\nFBA60703_pass_barcode01_e2391328_167a61ad_1.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_5.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_9.fastq.gz  \nFBA60703_pass_barcode01_e2391328_167a61ad_2.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_6.fastq.gz  NanoPlot-output\n</code></pre> *Note: You should by now have these files and folders in your working directory /test_data/consensus_mode/barcode01  </li> </ol> <p>Exercise 4: Use the command <code>NanoComp</code> to compare. <code>NanoComp</code> is an excellent tool to compare 2, or more, sequence files. In this example you will compare the original <code>allreads.fastq.gz</code> and the filtered reads <code>Q15_filtered.reads.fastq</code>.  </p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4. </li> <li>Run the command COMMAND<pre><code>NanoComp --fastq allreads.fastq.gz Q15_filtered.reads.fastq --outdir compare_filtered --plot violin\n</code></pre></li> <li>Check the ouput OUTPUT<pre><code>/test_data/consensus_mode/barcode01$cd compare_filtered/\n/test_data/consensus_mode/barcode01/compare_filtered$ls\nNanoComp_20250424_1550.log       NanoComp_N50.png                           NanoComp_OverlayHistogram.png                 NanoComp_OverlayLogHistogram.png            NanoComp_total_throughput.html\nNanoComp_lengths_violin.html     NanoComp_number_of_reads.html              NanoComp_OverlayHistogram_Weighted.html       NanoComp_OverlayLogHistogram_Weighted.html  NanoComp_total_throughput.png\nNanoComp_lengths_violin.png      NanoComp_number_of_reads.png               NanoComp_OverlayHistogram_Weighted.png        NanoComp_OverlayLogHistogram_Weighted.png   NanoStats.txt\nNanoComp_log_length_violin.html  NanoComp_OverlayHistogram.html             NanoComp_OverlayLogHistogram.html             NanoComp_quals_violin.html\nNanoComp_log_length_violin.png   NanoComp_OverlayHistogram_Normalized.html  NanoComp_OverlayLogHistogram_Normalized.html  NanoComp_quals_violin.png\nNanoComp_N50.html                NanoComp_OverlayHistogram_Normalized.png   NanoComp_OverlayLogHistogram_Normalized.png   NanoComp-report.html\n</code></pre> *Note: You can dowlod the file <code>NanoComp-report.html</code> and open the file on your computer. See the difference?  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC-old/#223-run-nanopack-advanced","title":"2.2.3 Run Nanopack: Advanced","text":"<p>If you want to explore more features on the <code>NanoPack</code> program, you can visit the website: https://passelma42.github.io/Field-Sequencing/Tutorial-NanoPack/.  </p> <p>!Important!:  </p> <ul> <li>Don't run the <code>conda</code> commands, no need when using the HPC  </li> <li>Focus on the Exercises and don't forget to activate the necessary modules before running the commands  </li> <li>All test data for the Advanced Nanopack exercises can be downloaded here: <code>/ONT-SEQ_PA-Benin2024_Bioinfocourse/nanopack_Advanced_exampledata</code> </li> </ul>"},{"location":"Tutorial-Data-Analysis-HPC-old/#3-epi2me-wf-amplicon","title":"3. EPI2ME: wf-amplicon","text":"<p>EPI2ME provides best practice bioinformatics analyses for nanopore sequencing. It offers bioinformatics for all levels of expertise and is designed by Oxford Nanopore especially to work on long read nanopore sequencing data. There is a desktop version available, with all necessary software packaged in a single workflow. Depending on your research there are different workflows available tailored to your needs. These workflows are also available on the command line. No need to install the bioinformatic tools yourself, except the necessary tools to load the workflows.  </p> <p>We will be using the wf-amplicon workflow. As mentioned before, when using this on your own computer, no need to install any bioinformatic tools except Nextflow, docker or singularity/apptainer.</p> <p>FYI General info: https://epi2me.nanoporetech.com/ Workflows Overview: https://epi2me.nanoporetech.com/wfindex/ Installation instructions: https://epi2me.nanoporetech.com/epi2me-docs/installation/ Amplicon Workflow: https://github.com/epi2me-labs/wf-amplicon </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#31-set-hpc-environment","title":"3.1 Set HPC environment","text":"<p>Before we can start using the <code>wf-amplicon</code> we need to prepare our HPC environment. For this you need to copy the script <code>singularity_environment_set.sh</code> and execute. These are the steps to follow:  </p> <ol> <li>Change directory to your VSC-DATA folder <pre><code>$ cd $VSC_DATA  \n</code></pre></li> <li>copy the folder scripts to your current location i.e. your VSC_DATA folder  <pre><code>$ cp -r &lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/scripts .\n</code></pre></li> <li>Change to the created folder  <pre><code>$ cd scripts  \n</code></pre></li> <li>Give executable rights to the script <pre><code>$ chmod  u+x  singularity_environment_set.sh\n</code></pre></li> <li>Execute the script using source which will allow the export commands affect your current shell environment <pre><code>$ source singularity_environment_set.sh\n</code></pre></li> <li>Check if the singularity folder has been created and also if it contains the 3 subfolders <code>cache</code>, <code>tmp</code>, and <code>image</code> <pre><code>$ ll $VSC_SCRATCH/singularity \n</code></pre></li> <li>Check if the environment is ready and set to go. These commands should give you the actual paths to the folders <pre><code>echo $SINGULARITY_CACHEDIR\necho $SINGULARITY_TMPDIR\necho $NXF_SINGULARITY_CACHEDIR\n</code></pre></li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC-old/#32-run-test-consensus-mode","title":"3.2 Run Test - Consensus mode","text":"<p>In the previous chapter you already worked on this data to pracktice the <code>Nanopack</code> tool. Now it is time to run the <code>wf-amplicon</code> pipeline.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#322-check-samplesheet","title":"3.2.2 Check samplesheet","text":"<ol> <li>Navigate to the directory <code>/test_data/consensus_mode</code> </li> <li>Take a look at the samplesheet SAMPLESHEET LAYOUT<pre><code>/test_data/consensus_mode$cat samplesheet_ONT-SEQ-25_05.txt \nbarcode,alias,type\nbarcode01,MBB-24-154_barcode01_ONT-SEQ-25_PA-05,test_sample\nbarcode02,MBB-24-156_barcode02_ONT-SEQ-25_PA-05,test_sample\n</code></pre> This shows you 3 columns <code>barcode</code>, <code>alias</code> and <code>type</code>.  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC-old/#323-run-the-program","title":"3.2.3 Run the program","text":"<p>To run the workflow the only software we need to activate is <code>Nextflow</code>. Activating all other software will be taken care of automatically while running the workflow.  Make sure you are in the folder that contains all your <code>barcodexx</code> folders. In this case it should be <code>/test_data/consensus_mode</code>.  </p> <ol> <li>Activate the nextflow software <pre><code>$ module load Nextflow/24.10.2\n</code></pre></li> <li>Navigate to the folder <code>/test_data/consensus_mode</code> which contains the <code>barcodexx</code> folders.</li> <li>Run the nextflow command to activate the <code>wf-amplicon</code> consensus_mode command<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./consensus_mode \\\n    --sample_sheet ./consensus_mode/samplesheet_ONT-SEQ-25_05.txt \\\n    --out_dir output-consensus_mode \\\n    -profile singularity\n</code></pre></li> </ol> <p>This analysis will pull the necessary containers with all needed software packages, then it will run the tools on your data. You can monitor the progress on your terminal. Because we work with a small test dataset it should not take more than 5 minutes to run.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#324-analyse-output","title":"3.2.4 Analyse output","text":"OUTPUT CONSENSUS MODE<pre><code>/test_data/output-consensus_mode$ll\ntotal 4776\n-rw-r--r-- 1 vsc43352 vsc43352    1448 Apr 30 12:05 all-consensus-seqs.fasta\n-rw-r--r-- 1 vsc43352 vsc43352     107 Apr 30 12:05 all-consensus-seqs.fasta.fai\ndrwxr-xr-x 2 vsc43352 vsc43352    4096 Apr 30 12:05 execution\ndrwxr-xr-x 4 vsc43352 vsc43352    4096 Apr 30 12:05 MBB-24-154_barcode01_ONT-SEQ-25_PA-05\ndrwxr-xr-x 4 vsc43352 vsc43352    4096 Apr 30 12:05 MBB-24-156_barcode02_ONT-SEQ-25_PA-05\n-rw-r--r-- 1 vsc43352 vsc43352    1597 Apr 30 12:01 params.json\n-rw-r--r-- 1 vsc43352 vsc43352     138 Apr 30 12:01 sample_sheet.csv\n-rw-r--r-- 1 vsc43352 vsc43352     251 Apr 30 12:05 versions.txt\n-rw-r--r-- 1 vsc43352 vsc43352 2434136 Apr 30 12:05 wf-amplicon-report.html\n</code></pre> <p>Taking a look at the output:  </p> Title File path Description Per sample or aggregated Workflow report <code>./wf-amplicon-report.html</code> Report for all samples. aggregated Sanitized reference file <code>./reference_sanitized_seqIDs.fasta</code> Some programs used by the workflow don't like special characters (like colons) in the sequence IDs in the reference FASTA file. The reference is thus \"sanitized\" by replacing these characters with underscores. Only generated in variant calling mode. aggregated Sanitized reference index file <code>./reference_sanitized_seqIDs.fasta.fai</code> FAI index for the sanitised reference FASTA file. aggregated Alignments BAM file <code>./{{ alias }}/alignments/{{ alias }}.aligned.sorted.bam</code> BAM file with alignments of input reads against the references (in variant calling mode) or the created consensus (in de-novo consensus mode). per-sample Alignments index file <code>./{{ alias }}/alignments/{{ alias }}.aligned.sorted.bam.bai</code> Index for alignments BAM file. per-sample De-novo consensus FASTQ file <code>./{{ alias }}/consensus/consensus.fastq</code> Consensus sequence file generated by de-novo consensus pipeline. per-sample Consensus FASTA file <code>./{{ alias }}/consensus/medaka.consensus.fasta</code> Consensus sequence file generated by variant calling pipeline. per-sample Variants VCF file <code>./{{ alias }}/variants/medaka.annotated.vcf.gz</code> VCF file of variants detected against the provided reference. per-sample Variants index file <code>./{{ alias }}/variants/medaka.annotated.vcf.gz.csi</code> Index for variants VCF file. per-sample Combined de-novo consensus sequences <code>./all-consensus-seqs.fasta</code> FASTA file containing all de-novo consensus sequences. aggregated Combined de-novo consensus sequences index <code>./all-consensus-seqs.fasta.fai</code> FAI index for the FASTA file with the combined de-novo consensus sequences. aggregated IGV config JSON file <code>./igv.json</code> JSON file with IGV config options to be used by the EPI2ME Desktop Application. aggregated <p>Important for screening your results is the <code>wf-amplicon-report.html</code> and for downstream applications offcourse your <code>fasta</code> files.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#33-run-test-variant-mode","title":"3.3 Run Test - Variant mode","text":"<p>Running in variant mode allows us to run the same analysis as before but since our test data contains reads from both ITS and LSU, we can bioinformatically construct consensus sequences per sample for both genes at the same time.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#332-check-samplesheet","title":"3.3.2 Check samplesheet","text":"<ol> <li>Navigate to the directory <code>/test_data/variant_mode</code> </li> <li>Take a look at the samplesheet SAMPLESHEET LAYOUT<pre><code>/test_data/variant_mode$cat samplesheet_ONT-SEQ-24_02-withreference.txt \nbarcode,alias,type,ref\nbarcode12,MBB-24-016_barcode12_ONT-SEQ-24_02,test_sample,ITS LSU\nbarcode14,MBB-24-019_barcode14_ONT-SEQ-24_02,test_sample,ITS LSU\n</code></pre> This shows you 4 columns <code>barcode</code>, <code>alias</code>, <code>type</code> and <code>ref</code>.  </li> <li>Take a look at the reference file REFERENCE FILE<pre><code>/test_data/variant_mode$cat reference.fa \n&gt;ITS\nGAAGTAAA...\n&gt;LSU\nTATCAAT...\n</code></pre> This file is a fasta file showing 2 sequences with IDs <code>ITS</code> and <code>LSU</code>, the same as in your samplesheet <code>ref</code> column. This reference file is used to \"search\" for ITS or LSU sequences in your sequence reads i.e. in the <code>barcodexx</code> folders.  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC-old/#333-run-the-program","title":"3.3.3 Run the program","text":"<p>To run the workflow the only software we need to activate is <code>Nextflow</code>. Activating all other software will be taken care of automatically while running the workflow.  Make sure you are in the folder that contains all your <code>barcodexx</code> folders. In this case it should be <code>/test_data/consensus_mode</code>.  </p> <ol> <li>Activate the nextflow software <pre><code>$ module load Nextflow/24.10.2\n</code></pre></li> <li>Run the nextflow command to activate the <code>wf-amplicon</code> consensus_mode command<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./variant_mode \\\n    --sample_sheet ./variant_mode/samplesheet_ONT-SEQ-24_02-withreference.txt \\\n    --reference ./variant_mode/reference.fa \\\n    --out_dir output-variant_mode \\\n    -profile singularity\n</code></pre></li> </ol> <p>This analysis will pull the necessary containers with all needed software packages, then it will run the tools on your data. You can monitor the progress on your terminal. Since we work with a small test dataset it should not take more than 5 minutes to run.  </p>"},{"location":"Tutorial-Data-Analysis-HPC-old/#334-analyse-output","title":"3.3.4 Analyse output","text":"<p>In variant-mode the output is slightly different. </p> OUTPUT VARIANT MODE<pre><code>/test_data/output-variant_mode$ll\ntotal 5001\ndrwxr-xr-x 2 vsc43352 vsc43352    4096 Apr 30 12:23 execution\ndrwxr-xr-x 5 vsc43352 vsc43352    4096 Apr 30 12:22 MBB-24-016_barcode12_ONT-SEQ-24_02\ndrwxr-xr-x 5 vsc43352 vsc43352    4096 Apr 30 12:22 MBB-24-019_barcode14_ONT-SEQ-24_02\n-rw-r--r-- 1 vsc43352 vsc43352    1645 Apr 30 12:21 params.json\n-rw-r--r-- 1 vsc43352 vsc43352    1666 Apr 30 12:22 reference_sanitized_seqIDs.fasta\n-rw-r--r-- 1 vsc43352 vsc43352      38 Apr 30 12:22 reference_sanitized_seqIDs.fasta.fai\n-rw-r--r-- 1 vsc43352 vsc43352     152 Apr 30 12:21 sample_sheet.csv\n-rwxr--r-- 1 vsc43352 vsc43352    2286 Apr 30 12:35 variantmode-consensus-rename2.0.sh\n-rw-r--r-- 1 vsc43352 vsc43352    7072 Apr 30 12:41 variant-mode-consensus-sequences.fasta\n-rw-r--r-- 1 vsc43352 vsc43352     251 Apr 30 12:22 versions.txt\n-rw-r--r-- 1 vsc43352 vsc43352 2529804 Apr 30 12:23 wf-amplicon-report.html\n</code></pre> <p>The main difference is that you're not only getting consensusfiles but also a vcf file. This file contains variants detected against the provided reference. Can be usefull if you are working with closely related species and you are interested in Single Nucleotid Polymorphism (SNP) information. In our case the sequences in the reference file were only a means to be able to sort out all ITS and LSU reads and assemble consensus sequences from both.  </p> <p>More info on VCF files can be found here:  https://en.wikipedia.org/wiki/Variant_Call_Format </p> <p>What we do miss here is a combined fasta file with all our ITS and LSU sequences and matching headers. This is something the workflow doesn't provide for us. For this feature we will use the script <code>variantmode-consensus-rename2.0.sh</code>.  </p> <p>Follow these instructions to run the script and build a combined fasta file for all your consensus sequences:  </p> <ol> <li>Copy the script to your outputfolder <pre><code>$ cp &lt;path-to&gt;/scripts/variantmode-consensus-rename2.0.sh &lt;path to&gt;/test_data/output-variant_mode\n</code></pre></li> <li>Change permissions to be able to execute the script <pre><code>$ cd &lt;path-to&gt;/test_data/output-variant_mode\n$ chmod u+x variantmode-consensus-rename2.0.sh\n</code></pre></li> <li>Make sure you are in the output folder and execute following command <pre><code>for dir in ./MBB*/; do ./variantmode-consensus-rename2.0.sh \"$dir\" -o variant-mode-consensus-sequences.fasta -a; done\n</code></pre> This commmand will search for all MBB-xx-xx_barcodexx_ONT-SEQ-xx_xx in the output folder, find all consensus sequences per barcode and generate one combined fasta file called <code>variant-mode-consensus-sequences.fasta</code>.  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC-old/#34-run-with-real-data","title":"3.4 Run with real data","text":"<p>Still with us? Time to put theory into practice. You can all start with your first analysis.  </p> <ol> <li>10 samples are selected for you. Check the Mycoblitz2024_specimenlist on Teams to find out which samples are selected for you.</li> <li>Find and download your samples from the <code>&lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01</code> download folder.</li> <li>Run either the consensus-mode and/or variant-mode (only possible for experiments 1 and 2)</li> <li>Check the results  </li> </ol> <p>Optional: If you finish earlier, you can try a 2nd analysis just for practice! Make sure to start a 2nd working folder Analysis2. </p> <p>The samplesheet: have a look </p> <p>Remember the structure of the dataset explained in Chapter 2.Nanopore data. Each ONT-SEQ-2X_PA-0X folder contains a different samplesheet. EXAMPLE SAMPLE SHEET<pre><code>&lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01$cat samplesheet_ONT-SEQ-24_01-withreference.txt \n\nbarcode,alias,type,ref\nbarcode01,MBB-24-001_barcode01_ONT-SEQ-24_01,test_sample,ITS LSU\nbarcode02,MBB-24-002_barcode02_ONT-SEQ-24_01,test_sample,ITS LSU\nbarcode03,MBB-24-003_barcode03_ONT-SEQ-24_01,test_sample,ITS LSU\nbarcode04,MBB-24-004_barcode04_ONT-SEQ-24_01,test_sample,ITS LSU\n</code></pre> In this samplesheet <code>ONT-SEQ-24_PA-01-withreference.txt</code> you see that <code>barcode01 = MBB-24-001</code> (for this sequence experiment only!). Now, have a look in the master species list in teams: You can find the detailed list with sample information in the teams General channel: <code>General &gt; Files &gt; Specieslist-overview &gt; Mycoblitz2024-specimenlist.xlsx</code> </p> <p>Example workflow: Copy your selection to your Analysis folder. </p> <ul> <li>I want to download samples from sequence experiment <code>ONT-SEQ-24_PA-01</code> </li> <li>The selected barcodes for me are: barcode 01 up to 10 1.Change directory to your VSC-DATA folder <pre><code>cd $VSC_DATA\n</code></pre> 2.Make a directory to copy your sequence reads to <pre><code>mkdir Analysis01\n</code></pre> 3.Change to the created folder <pre><code>cd Analysis01\n</code></pre> 4.Recursively copy the barcode01 folder to your Analysis01 folder <pre><code>cp -r &lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01/barcode01 $VSC_DATA/Analysis01\n</code></pre> 5.Check the content of the folder, the barcode01 folder should be copied to this Location <pre><code>ls $VSC_DATA/Analysis01\n</code></pre></li> </ul> <p>REPEAT FOR THE 9 OTHER BARCODEFOLDERS YOU SELECTED HINT: use a forloop to iterate over different barcode folders in one command <pre><code>for i in {01..10}; do cp -r &lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01/barcode$i \"$VSC_DATA/Analysis01\"; done\n</code></pre></p> <p>Remark: Not only copy the <code>barcode</code> folders but also the <code>samplesheet</code> from the sequencing experiment you take the <code>barcodes</code> folders from. </p> <p>When this exercise is finishe, you can run a 2nd Analysis. Just pick 10 barcode folders from the same sequencing experiment and run the exercise again. Be aware tha only experiments <code>ONT-SEQ-24_01</code> and <code>ONT-SEQ-24_02</code> allow you to run the wf-amplicon workflow in variant-mode! </p>"},{"location":"Tutorial-Data-Analysis-HPC/","title":"Tutorial Data Analysis HPC","text":"<p>Welcome to the HPC filesystem tutorial. This guide will teach you all the necessary commands and provide you with info on how to work in linux within the context of an HPC environment.  </p> <p>For an extended overview of HPC-documentation I refer to the HPC-DOCS: https://hpcugent.github.io/vsc_user_docs/ </p>"},{"location":"Tutorial-Data-Analysis-HPC/#1-introduction-to-linux-on-hpc","title":"1.  Introduction to Linux on HPC","text":""},{"location":"Tutorial-Data-Analysis-HPC/#11-getting-started","title":"1.1 Getting Started","text":"<p>To get started with the HPC-UGent infrastructure, you need to obtain a VSC account, see HPC manual. Keep in mind that you must keep your private key to yourself! You can look at your public/private key pair as a lock and a key: you give us the lock (your public key), we put it on the door, and then you can use your key to open the door and get access to the HPC infrastructure. Anyone who has your key can use your VSC account! Details on connecting to the HPC infrastructure are available in HPC manual connecting section.  </p> <p>NOTE: If you plan to work in the webbrowser interface only, then you don't need to upload public/private key. For this course, the webbrowse interface only is ok.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#12-the-prompt","title":"1.2 The prompt","text":"<p>The basic interface is the so-called shell prompt, typically ending with <code>$</code> (for bash shells).  </p> <p>NOTE: In the <code>code</code> examples below the <code>$</code> indicates your prompt. This is for information purpose only, you don't need to type this in your commands.  </p> <p>You use the shell by executing commands, and hitting <code>&lt;enter&gt;</code>. For example:</p> <p><pre><code>$ echo hello\nhello\n</code></pre> While typing (long) commands in the terminal, you can go to the start or end of the command line using <code>Ctrl-A</code> or <code>Ctrl-E</code>.</p> <p>To go through previous commands, use <code>&lt;up&gt;</code> and <code>&lt;down&gt;</code>, rather than retyping them.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#13-basic-commands","title":"1.3 Basic Commands","text":"<p>Download example data For this exercise you'll need to connect to the HPC and open a terminal. I've prepared some folders and files on which you can perform following commands.</p> <p>Start by copying the <code>/Linux_Basics_exampledata</code> to your home folder <code>~/</code>.</p> <pre><code>$ cp -r /path/to/ONT-SEQ_PA-Benin2024_Bioinfocourse/Linux_Basics_exampledata ~/  \n</code></pre> <p>Commands &amp; Examples NOTE: Make sure you are in your home folder before you start with the exercises. To go to your home folder use following command: <pre><code>$ cd ~/\n</code></pre> Example 1: Lists files and directories in the current directory. <code>$ ls -l</code></p> <p>Example 2: Changes the current directory. <code>$ cd /user/gent/433/vscXXXX/ONT-SEQ_PA-Benin2024_Bioinfocourse</code> </p> <p>Example 3: Prints the current working directory. <code>$ pwd</code></p> <p>Example 4: Creates a new directory. <code>$ mkdir my_first-folder</code></p> <p>Example 5: Deletes a file. WARNING: There is no trash, files are permanently deleted! <code>$ rm ./old_hello.py</code></p> <p>Example 6: Deletes a directory and its contents recursively. <code>$ rm -r ./old_sequences</code></p> <p>Example 7: Copies files or directories. <code>$ cp samplesheet96.txt ./my_first-folder</code></p> <p>Example 8: Moves or renames files or directories. <code>$ mv ./my_first-folder/samplesheet96.txt ./my_first-folder/samplesheet96_$(date +%Y-%m-%d).txt</code></p> <p>Example 9: Searches for a file by name in a specified path. <code>$ find ~/ -name \"samplesheet96*\"</code></p> <p>Example 10: Displays the contents of a file. <code>$ cat hello.py</code></p> <p>Example 11: Opens a file for viewing one page at a time. Press <code>q</code> to quit. <code>$ less manual_cp.txt</code></p> <p>Example 12: Shows the first 10 lines of a file. <code>$ head manual_cp.txt</code></p> <p>Example 13: Shows the last 10 lines of a file. <code>$ tail manual_cp.txt</code></p> <p>Example 14: Displays detailed information about files, including permissions. <code>$ ls -lh ~/Linux_Basics_exampledata</code></p> <p>Example 15: Shows currently running processes. <code>$ ps</code></p> <p>Example 16: Displays real-time system resource usage. <code>$ top</code></p> <p>Example 17: Interactive process viewer (if installed). <code>$ htop</code></p> <p>Example 18: Prints text to the terminal. <code>$ echo \"Hello, world!\"</code></p> <p>Example 19: Displays the current date and time. <code>$ date</code></p> <p>Example 20: Logs out or closes the terminal. <code>$ exit</code></p>"},{"location":"Tutorial-Data-Analysis-HPC/#14-manipulating-files-and-directories","title":"1.4 Manipulating files and directories","text":"<p>Changing permissions: \"chmod\" Each file and directory has particular permissions set on it, which can be queried using ls -l.</p> <p>For example: <pre><code>$ ls -l afile.txt \n-rw-rw-r-- 1 vsc40000 agroup 2929176 Apr 12 13:29 afile.txt \n</code></pre> The -rwxrw-r-- specifies both the type of file (- for files, d for directories (see first character)), and the permissions for user/group/others:</p> <ol> <li>each triple of characters indicates whether the read (r), write (w), execute (x) permission bits are set or not</li> <li>the 1st part rwx indicates that the owner \"vsc40000\" of the file has all the rights</li> <li>the 2nd part rw- indicates the members of the group \"agroup\" only have read/write permissions (not execute)</li> <li>the 3rd part r-- indicates that other users only have read permissions</li> </ol> <p>The default permission settings for new files/directories are determined by the so-called umask setting, and are by default:</p> <ol> <li>read-write permission on files for user/group (no execute), read-only for others (no write/execute)</li> <li>read-write-execute permission for directories on user/group, read/execute-only for others (no write)</li> </ol> <p>Any time you run <code>ls -l</code> you'll see a familiar line of <code>-rwx------</code> or similar combination of the letters <code>r</code>, <code>w</code>, <code>x</code> and <code>-</code> (dashes). These are the permissions for the file or directory. <pre><code>$ ls -l\ntotal 1\n-rw-r--r--. 1 vsc40000 mygroup 4283648 Apr 12 15:13 articleTable.csv\ndrwxr-x---. 2 vsc40000 mygroup 40 Apr 12 15:00 Project_GoldenDragon\n</code></pre></p> <p>Here, we see that <code>articleTable.csv</code> is a file (beginning the line with -) has read and write permission for the user <code>vsc40000</code> (<code>rw-</code>), and read permission for the group <code>mygroup</code> as well as all other users (<code>r--</code> and <code>r--</code>).</p> <p>The next entry is <code>Project_GoldenDragon</code>. We see it is a directory because the line begins with a <code>d</code>. It also has read, write, and execute permission for the <code>vsc40000</code> user (<code>rwx</code>). So that user can look into the directory and add or remove files. Users in the <code>mygroup</code> can also look into the directory and read the files. But they can't add or remove files (<code>r-x</code>). Finally, other users can read files in the directory, but other users have no permissions to look in the directory at all (<code>---</code>).</p> <p>Maybe we have a colleague who wants to be able to add files to the directory. We use <code>chmod</code> to change the modifiers to the directory to let people in the group write to the directory: <pre><code>$ chmod g+w Project_GoldenDragon\n$ ls -l\ntotal 1\n-rw-r--r--. 1 vsc40000 mygroup 4283648 Apr 12 15:13 articleTable.csv\ndrwxrwx---. 2 vsc40000 mygroup 40 Apr 12 15:00 Project_GoldenDragon\n</code></pre></p> <p>The syntax used here is <code>g+x</code> which means group was given write permission. To revoke it again, we use <code>g-w</code>. The other roles are <code>u</code> for user and <code>o</code> for other.</p> <p>You can put multiple changes on the same line: <code>chmod o-rwx,g-rxw,u+rx,u-w somefile</code>. This will take everyone's permission away except the user's ability to read or execute the file.  </p> <p>You can also use the <code>-R</code> flag to affect all the files within a directory, but this is dangerous. It's best to refine your search using find and then pass the resulting list to chmod since it's not usual for all files in a directory structure to have the same permissions.  </p> <p>Example 1: Apply chmod to all files (regular files only) within a directory This will set 644 permissions for all files (not directories) <code>find /path/to/directory -type f -exec chmod 644 {} \\;</code></p> <p>Example 2: Apply chmod to all directories within a directory  This will set 755 permissions for all directories (not files) <code>find /path/to/directory -type d -exec chmod 755 {} \\;</code></p> <p>Example 3: Apply chmod to all files modified in the last 7 days This will search for all files modified in the last 7 days and set 644 permissions <code>find /path/to/directory -type f -mtime -7 -exec chmod 644 {} \\;</code></p> <p>Example 4: Apply chmod to files with a specific extension (e.g., .txt) This will search for all .txt files and set 644 permissions <code>find /path/to/directory -type f -name \"*.txt\" -exec chmod 644 {} \\;</code></p> <p>Example 5: Apply chmod recursively with different permissions for files and directories First, set 644 permissions for all files (regular files) <code>find /path/to/directory -type f -exec chmod 644 {} \\;</code></p> <p>Then, set 755 permissions for all directories <code>find /path/to/directory -type d -exec chmod 755 {} \\;</code></p> <p>1.4.1.1 Exercises: Changing permissions with \"chmod\" </p> <p>Here are some exercises on <code>chmod</code> based on your directory structure. Try each command and observe the changes using <code>ls -l</code> before and after.</p> <p>Exercise 1: Make <code>hello.sh</code> Executable Goal: Grant execute (<code>x</code>) permission to the script so it can run as a program. <pre><code>chmod +x hello.sh\nls -l hello.sh\n</code></pre> Check if the <code>x</code> permission appears for the user (<code>rwxr--r--</code>). Now try running it: <pre><code>./hello.sh\n</code></pre></p> <p>Exercise 2: Remove Read Permissions from <code>manual_cp.txt</code> Goal: Prevent yourself and others from reading the file. <pre><code>chmod a-r manual_cp.txt\nls -l manual_cp.txt\n</code></pre> Now try opening it: <pre><code>cat manual_cp.txt  # Should give a \"Permission denied\" error\n</code></pre> Restore permissions after testing: <pre><code>chmod u+r manual_cp.txt\n</code></pre></p> <p>Exercise 3: Grant Full Access to <code>samplesheet96.txt</code> for Everyone Goal: Allow all users to read, write, and execute <code>samplesheet96.txt</code>. <pre><code>chmod 777 samplesheet96.txt\nls -l samplesheet96.txt\n</code></pre> Now everyone can modify and execute the file. This is not recommended for sensitive files!</p> <p>Exercise 4: Restrict <code>old_hello.py</code> to Read and Write for Owner Only Goal: Make the file private so only the owner can read and modify it. <pre><code>chmod 600 old_hello.py\nls -l old_hello.py\n</code></pre> Now other users cannot read or modify it.</p> <p>Exercise 5: Add a Sticky Bit to <code>sequences</code> Goal: Ensure that only the file owner can delete their own files inside <code>sequences</code>, even if others have write access. <pre><code>chmod +t sequences\nls -ld sequences  # Check for the \"t\" at the end of the permissions (drwxr-xr-t)\n</code></pre> Now, even if other users have write permissions, they cannot delete files they don't own.</p> <p>Exercise 6: Recursively Set Read &amp; Execute for All Users in <code>old_sequences</code> Goal: Ensure all files in <code>old_sequences</code> are readable and executable but not writable by others. <pre><code>chmod -R a+rx old_sequences\nls -l old_sequences/\n</code></pre> Check if all files inside now have <code>r-x</code> for everyone.</p> <p>Exercise 7: Set Group Write Permissions for <code>Tabular-file.tab</code> Goal: Allow your group members to edit the file. <pre><code>chmod g+w Tabular-file.tab\nls -l Tabular-file.tab\n</code></pre> Now, users in the same group (<code>vsc43352</code>) can modify the file.</p> <p>Exercise 8: Remove Execute Permission from <code>hello.py</code> Goal: Prevent accidental execution of a Python script. <pre><code>chmod -x hello.py\nls -l hello.py\n</code></pre> Now, you must explicitly use <code>python3 hello.py</code> instead of <code>./hello.py</code>.</p> <p>Exercise 9: Convert Between Symbolic and Octal Modes </p> <p>Goal: Change <code>samplesheet96_2025-04-02.txt</code> to the same permissions as <code>samplesheet96.txt</code> using octal mode.  </p> <ol> <li>Check the original permissions: <pre><code>ls -l samplesheet96.txt\n</code></pre></li> <li>Use <code>stat</code> to see the octal mode: <pre><code>stat -c \"%a\" samplesheet96.txt\n</code></pre>    Suppose it returns <code>644</code>.</li> <li>Apply the same mode to the new file: <pre><code>chmod 644 samplesheet96_2025-04-02.txt\n</code></pre></li> </ol> <p>Bonus Challenge: Set <code>sequences</code> to 750 Goal: Make <code>sequences</code> accessible only to the owner and group, while others have no access. <pre><code>chmod 750 sequences\nls -ld sequences\n</code></pre> Now only you and your group can access it, while others are blocked.</p> <p>Final Check After completing the exercises, list all files again to see the changes: <pre><code>ls -l\n</code></pre></p> <p>Compressing files (zip, unzip, tar) </p> <p>Files should usually be stored in a compressed file if they're not being used frequently. This means they will use less space and thus you get more out of your quota. Some types of files (e.g., CSV files with a lot of numbers) compress as much as 9:1. The most commonly used compression format on Linux is gzip. To compress a file using gzip, we use: <pre><code>$ ls -lh myfile\n-rw-r--r--. 1 vsc40000 vsc40000 4.1M Dec 2 11:14 myfile\n$ gzip myfile\n$ ls -lh myfile.gz\n-rw-r--r--. 1 vsc40000 vsc40000 1.1M Dec 2 11:14 myfile.gz\n</code></pre> Note: if you zip a file, the original file will be removed. If you unzip a file, the compressed file will be removed. To keep both, we send the data to stdout and redirect it to the target file: <pre><code>$ gzip -c myfile &gt; myfile.gz\n$ gunzip -c myfile.gz &gt; myfile\n</code></pre> \"zip\" and \"unzip\" </p> <p>Windows and macOS seem to favour the zip file format, so it's also important to know how to unpack those. We do this using unzip: <pre><code>$ unzip myfile.zip\n</code></pre> If we would like to make our own zip archive, we use zip: <pre><code>$ zip myfiles.zip myfile1 myfile2 myfile3\n</code></pre> Working with tarballs: \"tar\" </p> <p>Tar stands for \"tape archive\" and is a way to bundle files together in a bigger file.</p> <p>You will normally want to unpack these files more often than you make them. To unpack a <code>.tar</code> file you use: <pre><code>$ tar -xf tarfile.tar\n</code></pre> Often, you will find <code>gzip</code> compressed <code>.tar</code> files on the web. These are called tarballs. You can recognize them by the filename ending in <code>.tar.gz</code>. You can uncompress these using gunzip and then unpacking them using tar. But tar knows how to open them using the <code>-z</code> option: <pre><code>$ tar -zxf tarfile.tar.gz\n$ tar -zxf tarfile.tgz\n</code></pre> Order of arguments </p> <p>Note: Archive programs like <code>zip</code>, <code>tar</code> use arguments in the \"opposite direction\" of copy commands. <pre><code>$ cp source1 source2 source3 target\n$ zip zipfile.zip source1 source2 source3\n$ tar -cf tarfile.tar source1 source2 source3\n</code></pre> If you use <code>tar</code> with the source files first then the first file will be overwritten. You can control the order of arguments of tar if it helps you remember:</p> <p>$ tar -c source1 source2 source3 -f tarfile.tar Exercises: Compressing files (zip, unzip, tar)</p> <p>Here are some zip, unzip, and tar exercises based on your directory structure. Each exercise includes commands and explanations.</p> <p>Exercise 1: Create a <code>.zip</code>file from a local file. Goal: Create a <code>.zip</code> archive containing <code>manual_cp.txt</code>. <pre><code>zip manual_cp.zip manual_cp.txt\nls -l manual_cp.zip\n</code></pre> Now the file is compressed into <code>manual_cp.zip</code>.</p> <p>Exercise 2: Extract <code>manual_cp.zip</code> Goal: Extract the zipped file back to its original form. <pre><code>unzip manual_cp.zip\nls -l\n</code></pre> The extracted file should appear in the directory.</p> <p>Exercise 3: Compress Multiple Files into One ZIP Archive Goal: Create a <code>.zip</code> file containing <code>hello.py</code>, <code>hello.sh</code>, and <code>Tabular-file.tab</code>. <pre><code>zip my_archive.zip hello.py hello.sh Tabular-file.tab\nls -l my_archive.zip\n</code></pre> All three files are stored in <code>my_archive.zip</code>.</p> <p>Exercise 4: Compress the <code>sequences</code> Directory Using ZIP Goal: Create a <code>.zip</code> archive of the <code>sequences</code> directory. <pre><code>zip -r sequences.zip sequences\nls -l sequences.zip\n</code></pre> The <code>-r</code> flag ensures that all files inside the directory are included.</p> <p>Exercise 5: Extract <code>sequences.zip</code> Goal: Extract the entire directory from the <code>.zip</code> archive. <pre><code>unzip sequences.zip\nls -l\n</code></pre> The <code>sequences</code> directory is restored.</p> <p>Exercise 6: Create a Tarball (<code>.tar</code>) Without Compression Goal: Archive multiple files into a <code>.tar</code> file without compression. <pre><code>tar -cf archive.tar hello.py hello.sh Tabular-file.tab\nls -l archive.tar\n</code></pre> The <code>.tar</code> file now contains all three files but is not compressed.</p> <p>Exercise 7: Create a Gzipped Tarball (<code>.tar.gz</code>) Goal: Archive and compress the <code>old_sequences</code> directory. <pre><code>tar -czf old_sequences.tar.gz old_sequences\nls -l old_sequences.tar.gz\n</code></pre> The <code>-z</code> flag enables gzip compression, creating a smaller file.</p> <p>Exercise 8: Extract a <code>.tar.gz</code> File Goal: Extract the <code>old_sequences.tar.gz</code> archive. <pre><code>tar -xzf old_sequences.tar.gz\nls -l\n</code></pre> The <code>old_sequences</code> directory is restored.</p> <p>Exercise 9: List Contents of a <code>.tar.gz</code> File Without Extracting Goal: View files inside <code>old_sequences.tar.gz</code> without extracting. <pre><code>tar -tzf old_sequences.tar.gz\n</code></pre> This shows all files inside the tarball.</p> <p>Exercise 10: Extract Only <code>samplesheet96.txt</code> From <code>archive.tar</code> Goal: Extract a single file from a tar archive. <pre><code>tar -xf archive.tar samplesheet96.txt\nls -l samplesheet96.txt\n</code></pre> Only <code>samplesheet96.txt</code> is extracted.</p> <p>Bonus Challenge: Tar a Directory and Extract It to Another Location Goal: Archive <code>sequences</code> and extract it elsewhere. <pre><code>tar -czf sequences.tar.gz sequences\nmkdir test_restore\ntar -xzf sequences.tar.gz -C test_restore\nls test_restore/\n</code></pre> The <code>sequences</code> directory is extracted into <code>test_restore</code> instead of the current directory.</p> <p>Final Check After completing the exercises, list your directory contents: <pre><code>ls -l\n</code></pre></p>"},{"location":"Tutorial-Data-Analysis-HPC/#15-software-packages-on-hpc","title":"1.5 Software packages on HPC","text":"<p>All software on the HPC is available through <code>modules</code> which need to be activated first before you can start using them. Follow the steps below to learn how to work with these commands.  </p> <p>Exercise 1: Use the command <code>module</code> to list all activated modules </p> <ol> <li>Open a terminal and an interactive session! (see \"02-Day1-HPC-intro_2025.ppx\" if you don't remember how to do this)</li> <li>Check which modules are currently loaded/activated on your system. The command \u00b4ml\u00b4 you'll see below is short for <code>module</code>. <pre><code>ml list\n</code></pre> <pre><code># OUTPUT\nCurrently Loaded Modules:\n  1) env/vsc/doduo (S)   2) env/slurm/doduo (S)   3) env/software/doduo (S)   4) cluster/doduo (S)\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n</code></pre> Note: This tells you that you are working on the doduo cluster and all basic modules are loaded but no software packages are yet activated </li> </ol> <p>Exercise 2: Use the command <code>ml spider</code> to see if your favourite software package is available </p> <p>Run following command to check if a nanpack module is available. <pre><code>ml spider nanopack\n</code></pre></p> <p><pre><code># OUTPUT\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  NanoPack: NanoPack/20230602-foss-2023a\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Collection of long read processing and analysis tools.\n\n    You will need to load all module(s) on any one of the lines below before the \"NanoPack/20230602-foss-2023a\" module is available to load.\n\n      env/software/accelgor\n      env/software/doduo\n      env/software/donphan\n      env/software/gallade\n      env/software/joltik\n      env/software/litleo\n      env/software/shinx\n\n    Help:\n      Description\n      ===========\n      Collection of long read processing and analysis tools.\n\n\n      More information\n      ================\n       - Homepage: https://daler.github.io/pybedtools\n</code></pre> Note: Here you see the full name of the package \"NanoPack/20230602-foss-2023a\" and all the cluster on which you can activate this module/software package </p> <ol> <li>Activate the nanopack module <pre><code>ml NanoPack/20230602-foss-2023a\n</code></pre></li> <li>Check if the module is loaded   </li> </ol> <pre><code>vsc43352@gligar09:~$ml list\n</code></pre> <p><pre><code># OUTPUT\nCurrently Loaded Modules:\n  1) env/vsc/doduo                    (S)  26) foss/2023a                                 51) statsmodels/0.14.1-gfbf-2023a            76) Tkinter/3.11.3-GCCcore-12.3.0\n  2) env/slurm/doduo                  (S)  27) bzip2/1.0.8-GCCcore-12.3.0                 52) gzip/1.12-GCCcore-12.3.0                 77) NASM/2.16.01-GCCcore-12.3.0\n  3) env/software/doduo               (S)  28) ncurses/6.4-GCCcore-12.3.0                 53) lz4/1.9.4-GCCcore-12.3.0                 78) libjpeg-turbo/2.1.5.1-GCCcore-12.3.0\n  4) cluster/doduo                    (S)  29) libreadline/8.2-GCCcore-12.3.0             54) zstd/1.5.5-GCCcore-12.3.0                79) jbigkit/2.1-GCCcore-12.3.0\n  5) GCCcore/12.3.0                        30) Tcl/8.6.13-GCCcore-12.3.0                  55) ICU/73.2-GCCcore-12.3.0                  80) libdeflate/1.18-GCCcore-12.3.0\n  6) zlib/1.2.13-GCCcore-12.3.0            31) SQLite/3.42.0-GCCcore-12.3.0               56) Boost/1.82.0-GCC-12.3.0                  81) LibTIFF/4.5.0-GCCcore-12.3.0\n  7) binutils/2.40-GCCcore-12.3.0          32) libffi/3.4.4-GCCcore-12.3.0                57) snappy/1.1.10-GCCcore-12.3.0             82) giflib/5.2.1-GCCcore-12.3.0\n  8) GCC/12.3.0                            33) Python/3.11.3-GCCcore-12.3.0               58) RapidJSON/1.1.0-20230928-GCCcore-12.3.0  83) libwebp/1.3.1-GCCcore-12.3.0\n  9) numactl/2.0.16-GCCcore-12.3.0         34) gfbf/2023a                                 59) Abseil/20230125.3-GCCcore-12.3.0         84) OpenJPEG/2.5.0-GCCcore-12.3.0\n 10) XZ/5.4.2-GCCcore-12.3.0               35) cffi/1.15.1-GCCcore-12.3.0                 60) RE2/2023-08-01-GCCcore-12.3.0            85) LittleCMS/2.15-GCCcore-12.3.0\n 11) libxml2/2.11.4-GCCcore-12.3.0         36) cryptography/41.0.1-GCCcore-12.3.0         61) utf8proc/2.8.0-GCCcore-12.3.0            86) Pillow/10.0.0-GCCcore-12.3.0\n 12) libpciaccess/0.17-GCCcore-12.3.0      37) virtualenv/20.23.1-GCCcore-12.3.0          62) Arrow/14.0.1-gfbf-2023a                  87) Qhull/2020.2-GCCcore-12.3.0\n 13) hwloc/2.9.1-GCCcore-12.3.0            38) Python-bundle-PyPI/2023.06-GCCcore-12.3.0  63) Kaleido/0.2.1-GCCcore-12.3.0             88) matplotlib/3.7.2-gfbf-2023a\n 14) OpenSSL/3                             39) pybind11/2.11.1-GCCcore-12.3.0             64) NanoPlot/1.43.0-foss-2023a               89) joypy/0.2.6-foss-2023a\n 15) libevent/2.1.12-GCCcore-12.3.0        40) SciPy-bundle/2023.07-gfbf-2023a            65) nodejs/18.17.1-GCCcore-12.3.0            90) NanoComp/1.24.0-foss-2023a\n 16) UCX/1.14.1-GCCcore-12.3.0             41) nanomath/1.4.0-foss-2023a                  66) plotly-orca/1.3.1-GCCcore-12.3.0         91) libyaml/0.2.5-GCCcore-12.3.0\n 17) PMIx/4.2.4-GCCcore-12.3.0             42) Biopython/1.83-foss-2023a                  67) libpng/1.6.39-GCCcore-12.3.0             92) PyYAML/6.0-GCCcore-12.3.0\n 18) UCC/1.2.0-GCCcore-12.3.0              43) cURL/8.0.1-GCCcore-12.3.0                  68) Brotli/1.0.9-GCCcore-12.3.0              93) Pillow-SIMD/9.5.0-GCCcore-12.3.0\n 19) OpenMPI/4.1.5-GCC-12.3.0              44) Pysam/0.22.0-GCC-12.3.0                    69) freetype/2.13.0-GCCcore-12.3.0           94) tornado/6.3.2-GCCcore-12.3.0\n 20) OpenBLAS/0.3.23-GCC-12.3.0            45) nanoget/1.19.3-foss-2023a                  70) expat/2.5.0-GCCcore-12.3.0               95) bokeh/3.2.2-foss-2023a\n 21) FlexiBLAS/3.3.1-GCC-12.3.0            46) NanoStat/1.6.0-foss-2023a                  71) util-linux/2.39-GCCcore-12.3.0           96) nanoQC/0.9.4-foss-2023a\n 22) FFTW/3.3.10-GCC-12.3.0                47) NanoFilt/2.8.0-foss-2023a                  72) fontconfig/2.14.2-GCCcore-12.3.0         97) kyber/0.4.0-GCC-12.3.0\n 23) gompi/2023a                           48) minimap2/2.26-GCCcore-12.3.0               73) xorg-macros/1.20.0-GCCcore-12.3.0        98) NanoPack/20230602-foss-2023a\n 24) FFTW.MPI/3.3.10-gompi-2023a           49) NanoLyse/1.2.1-foss-2023a                  74) X11/20230603-GCCcore-12.3.0\n 25) ScaLAPACK/2.2.0-gompi-2023a-fb        50) plotly.py/5.16.0-GCCcore-12.3.0            75) Tk/8.6.13-GCCcore-12.3.0\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n</code></pre> Note: Besides the Nanopack/20230602-foss-2023a module you'll see a lot more is added to the list. These are all the dependencies Nanopack needs to be able to function properly.</p>"},{"location":"Tutorial-Data-Analysis-HPC/#2-nanopore-data","title":"2. Nanopore data","text":"<p>Remember that each of you extracted DNA and did PCR on his/her own collectd fieldsamples in Benin last year. You can find a more detailed list with extra information in the teams General channel: <code>Document &gt; General &gt; Specieslist-overview</code> </p> <p>Before we start any Analysis on this data we first need to learn how to interprete the Nanopore sequence output and learn how to use the different tools. For this, I prepared a test data set (see below) with sequence output from 2 barcodes, i.e. 2 samples. Once you mastered all the skills you need, we can continue with a more extensive dataset.  </p> <p>To keep the running time of the analysis minimal (more data is longer runtime), I suggest to select no more than 10 Barcodes each. But first let's have a look at the data structure and start with the test data.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#21-data-structure","title":"2.1 Data structure","text":"<p>But first things first. Have a look at the content of the input data folder /ONT-SEQ_PA-Benin2024_Input-Data/. You should be able to navigate to this folder by now. Hint: you can use the <code>ls</code> command, or <code>cd</code> to access the directory and use the command <code>tree</code> to create an overview of the content. </p> <p>Main folder The structure looks like this: <pre><code>path/to/ONT-SEQ_PA-Benin2024_Input-Data/\n.\n+-- ONT-SEQ-24_PA-01\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   \\-- barcode24\n|   +-- reference.fa\n|   +-- report_AMF003_20241018_1203_c8d4ac6d.html\n|   \\-- samplesheet_ONT-SEQ-24_01-withreference.txt\n+-- ONT-SEQ-24_PA-02\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   +-- barcode24\n|   +-- reference.fa\n|   +-- report_axf308_20241022_1153_b65829bf.html\n|   \\-- samplesheet_ONT-SEQ-24_02-withreference.txt\n+-- ONT-SEQ-25_PA-04\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   +-- barcode96\n|   +-- report_FBA60703_20250120_1336_292963a6.html\n|   \\-- samplesheet_ONT-SEQ-25_04.txt\n+-- ONT-SEQ-25_PA-05\n|   +-- fastq_pass\n|   |   +-- barcode01\n|   |   +-- ...\n|   |   +-- barcode48\n|   +-- report_FBA60703_20250128_1308_e2391328.html\n|   \\-- samplesheet_ONT-SEQ-25_05.txt\n+-- information\n\\-- scripts\n</code></pre> fastq-pass folder Each <code>barcode</code> folder contains the reads for this particular barcode in <code>fastq.gz</code> format:  </p> <pre><code>../fastq_pass/barcode01$tree\n.\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_0.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_10.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_11.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_12.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_13.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_14.fastq.gz\n+-- AMF003_pass_barcode01_c8d4ac6d_b8b1e3e8_15.fastq.gz\n</code></pre> <p>4 Sequence experiments were performed on your samples: ONT-SEQ-24_PA-0X. You can find more detailed list with extra information in the teams General channel: <code>Document &gt; General &gt; Specieslist-overview</code> </p> <ol> <li>ONT-SEQ-24_PA-01: 24 amplicons - ITS+LSU sequences <code>(*)</code> </li> <li>ONT-SEQ-24_PA-02: 24 amplicons - ITS+LSU sequences <code>(*)</code> </li> <li>ONT-SEQ-24_PA-04: 96 amplicons - ITS sequences  </li> <li>ONT-SEQ-24_PA-05: 48 amplicons - ITS  </li> </ol> <p><code>(*)</code> That means both ITS and LSU amplicon products for the same sample where mixed befor sequencing. The bioinformatic tool we will able to construct both ITS and LSU sequences at the same time.  </p> <p>The 2 other experiments (5 and 6) contain only ITS sequences per specimen.</p> <p>Sequence folder </p> <p>This folder contains the following:  </p> <ul> <li><code>fastq_pass</code> the passed fastq files i.e. your sequence reads  </li> <li><code>reference.fa</code> a reference file with an ITS and a LSU sequence, only for the reads from 1st and 2nd experiment  </li> <li><code>report...html</code> a sequence report file, general info on the sequence run (including all samples)  </li> <li><code>samplesheet_....txt</code>a samplesheet (with the MBB number and sequence barcode).  </li> </ul> <pre><code>/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01$tree -L 1\n.\n+-- fastq_pass\n+-- reference.fa\n+-- report_AMF003_20241018_1203_c8d4ac6d.html\n\\-- samplesheet_ONT-SEQ-24_01-withreference.txt\n</code></pre> <p>Take a look inside the folder and the samplesheet file.  *Hint: use the command <code>cd</code>, <code>ls</code> and <code>cat</code> You will notice that opening a <code>html</code> file is not possible in the terminal. To look at this file, you'll need to download it.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#22-practice-with-test-data","title":"2.2 Practice with test data","text":"<p>Before we start with the actual field data, we first are going to run a test dataset, to see if everything runs smoothly.  </p> <p>Download the test data set </p> <ol> <li>Copy the folder <code>/ONT-SEQ_PA-Benin2024_Input-Data/test_data</code> to your home folder  </li> <li>Verify the content and check if you have the sequence folders, the samplesheet and the reference file (for variant mode).  </li> </ol> <pre><code>vsc43352@gligar09:~$ls\ntest_data\nvsc43352@gligar09:~$ cd test_data\nvsc43352@gligar09:~$/test_data$tree -L 2\n.\n+-- consensus_mode\n|\u00a0\u00a0 +-- barcode01\n|\u00a0\u00a0 +-- barcode02\n|\u00a0\u00a0 \\-- samplesheet_ONT-SEQ-25_05.txt\n\\-- variant_mode\n    +-- barcode12\n    +-- barcode14\n    +-- reference.fa\n    \\-- samplesheet_ONT-SEQ-24_02-withreference.txt\n\n6 directories, 3 files\n</code></pre>"},{"location":"Tutorial-Data-Analysis-HPC/#221-nanopack-quality-control","title":"2.2.1 NanoPack: Quality Control","text":"<p>Nanopack is a tool set of different long read processing and analysis tools. With these tools you can assess the quality of your sequencerun and of 'individual' sequence reads.  </p> <p>Tool: https://github.com/wdecoster/nanopack Publication: NanoPack: visualizing and processing long-read sequencing data </p> <p>Wouter De Coster, Svenn D\u2019Hert, Darrin T Schultz, Marc Cruts, Christine Van Broeckhoven, NanoPack: visualizing and processing long-read sequencing dta, Bioinformatics, Volume 34, Issue 15, August 2018, Pages 2666\u20132669, https://doi.org/10.1093/bioinformatics/bty149  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#222-run-nanopack-beginner","title":"2.2.2 Run Nanopack: Beginner","text":"<p>Exercise 1: Use the command <code>Nanostat</code> to check statistics on your sequence data. Generating Basic Statistics from a FASTQ File. You are provided with a compressed FASTQ file (<code>reads.fastq.gz</code>). Generate a statistics report and save the output in a folder called <code>statreports</code> and filname <code>nanostats.out</code>.</p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4.</li> <li>When working on the HPC you first need to \"activate\" the Software <pre><code>$ module load NanoPack/20230602-foss-2023a\n</code></pre></li> <li>Run the command. <pre><code># Command\nNanoStat --fastq *fastq.gz --outdir statreports -n nanostats.out\n</code></pre> Note: the \u00b4\u00b4 in the command is making sure the NanoStat command is beeing executed on all files present in this folder.</li> <li>Check the output in the newly created subfolder \u00b4statsreports\u00b4 <pre><code># OUTPUT\n/test_data/consensus_mode/barcode01/statreports$ls\nnanostats.out\n/test_data/consensus_mode/barcode01/statreports$cat nanostats.out \nGeneral summary:         \nMean read length:                 438.6\nMean read quality:                 11.6\nMedian read length:               425.0\nMedian read quality:               12.6\nNumber of reads:               24,302.0\nRead length N50:                  511.0\nSTDEV read length:                154.7\nTotal bases:               10,659,020.0\nNumber, percentage and megabases of reads above quality cutoffs\n&gt;Q10:   21811 (89.7%) 10.0Mb\n&gt;Q15:   2256 (9.3%) 1.4Mb\n&gt;Q20:   3 (0.0%) 0.0Mb\n&gt;Q25:   0 (0.0%) 0.0Mb\n&gt;Q30:   0 (0.0%) 0.0Mb\nTop 5 highest mean basecall quality scores and their read lengths\n1:      21.4 (530)\n2:      21.0 (705)\n3:      20.3 (585)\n4:      19.7 (274)\n5:      19.3 (467)\nTop 5 longest reads and their mean basecall quality score\n1:      999 (14.6)\n2:      970 (13.0)\n3:      943 (13.1)\n4:      902 (12.9)\n5:      888 (14.9)\n</code></pre> Hint: These commands are usefull here \u00b4cd\u00b4, \u00b4ls\u00b4, \u00b4cat\u00b4 </li> </ol> <p>Exercise 2: Use the command <code>NanoPlot</code> to create visual plots of your data. </p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4. </li> <li>Run the Command <pre><code># Command\nNanoPlot  --fastq *.fastq.gz -o NanoPlot-output\n</code></pre></li> <li>Check the output <pre><code># OUTPUT\n/test_data/consensus_mode/barcode01/NanoPlot-output$ll\ntotal 3265\n-rw-r--r-- 1 vsc43352 vsc43352  463602 Apr 24 15:07 LengthvsQualityScatterPlot_dot.html\n-rw-r--r-- 1 vsc43352 vsc43352   51397 Apr 24 15:07 LengthvsQualityScatterPlot_dot.png\n-rw-r--r-- 1 vsc43352 vsc43352  689449 Apr 24 15:07 LengthvsQualityScatterPlot_kde.html\n-rw-r--r-- 1 vsc43352 vsc43352  121391 Apr 24 15:07 LengthvsQualityScatterPlot_kde.png\n-rw-r--r-- 1 vsc43352 vsc43352    6049 Apr 24 15:07 NanoPlot_20250424_1507.log\n-rw-r--r-- 1 vsc43352 vsc43352 1362962 Apr 24 15:07 NanoPlot-report.html\n-rw-r--r-- 1 vsc43352 vsc43352     788 Apr 24 15:07 NanoStats.txt\n-rw-r--r-- 1 vsc43352 vsc43352    8079 Apr 24 15:07 Non_weightedHistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   30027 Apr 24 15:07 Non_weightedHistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352    8383 Apr 24 15:07 Non_weightedLogTransformed_HistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   31773 Apr 24 15:07 Non_weightedLogTransformed_HistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352    8102 Apr 24 15:07 WeightedHistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   28858 Apr 24 15:07 WeightedHistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352    8524 Apr 24 15:07 WeightedLogTransformed_HistogramReadlength.html\n-rw-r--r-- 1 vsc43352 vsc43352   30033 Apr 24 15:07 WeightedLogTransformed_HistogramReadlength.png\n-rw-r--r-- 1 vsc43352 vsc43352  167169 Apr 24 15:07 Yield_By_Length.html\n-rw-r--r-- 1 vsc43352 vsc43352   37288 Apr 24 15:07 Yield_By_Length.png\n</code></pre> Note: You can download the NanoPlot-report.html file to your computer and open it locally. </li> </ol> <p>Exercise 3: Use the command <code>Chopper</code> to filter data based on read quality. </p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4. </li> <li>Locate and load the <code>chopper</code> Module. Check if is has been activated <pre><code># Commands to use\nml spider chopper\nml chopper/0.9.0-GCCcore-12.3.0\nmodule list\n</code></pre></li> <li>Run the command <pre><code># COMMAND\ncat *.fastq.gz &gt; allreads.fastq.gz &amp;&amp; chopper -q 15 -i allreads.fastq.gz &gt; Q15_filtered.reads.fastq\n</code></pre> Where:<ul> <li><code>cat *.fastq.gz &gt; allreads.fastq.gz</code> =&gt; Concatenate all fastq.gz files in the folder to one file <code>allreads.fastq.gz</code>.</li> <li><code>&amp;&amp;</code> =&gt; this can be used to peform 2 consecutive commands one after the other, but performs the 2nd only when the 1st is finished successfully.   </li> <li><code>chopper</code> takes the file allreads.fastq.gz, performs a quality filtering of Q15 and writes the reads to a new file Q15_filtered.reads.fastq</li> </ul> </li> <li>Check the output <pre><code># OUTPUT\n/test_data/consensus_mode/barcode01$cat *.fastq.gz &gt; allreads.fastq.gz &amp;&amp; chopper -q 15 -i allreads.fastq.gz &gt; Q15_filtered.reads.fastq                                                                                                                                                                                            \nKept 2256 reads out of 24302 reads\n/test_data/consensus_mode/barcode01$ls\nallreads.fastq.gz                                     FBA60703_pass_barcode01_e2391328_167a61ad_3.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_7.fastq.gz  Q15_filtered.reads.fastq\nFBA60703_pass_barcode01_e2391328_167a61ad_0.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_4.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_8.fastq.gz  statreports\nFBA60703_pass_barcode01_e2391328_167a61ad_1.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_5.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_9.fastq.gz  \nFBA60703_pass_barcode01_e2391328_167a61ad_2.fastq.gz  FBA60703_pass_barcode01_e2391328_167a61ad_6.fastq.gz  NanoPlot-output\n</code></pre> *Note: You should by now have these files and folders in your working directory /test_data/consensus_mode/barcode01  </li> </ol> <p>Exercise 4: Use the command <code>NanoComp</code> to compare. <code>NanoComp</code> is an excellent tool to compare 2, or more, sequence files. In this example you will compare the original <code>allreads.fastq.gz</code> and the filtered reads <code>Q15_filtered.reads.fastq</code>.  </p> <ol> <li>First navigate to the folder with your sequence reads in the folder \u00b4/test_data/consensus_mode/barcode01\u00b4. Hint: these commands may come in handy \u00b4cd\u00b4, \u00b4pwd\u00b4 and \u00b4ls\u00b4. </li> <li>Run the command <pre><code># COMMAND\nNanoComp --fastq allreads.fastq.gz Q15_filtered.reads.fastq --outdir compare_filtered --plot violin\n</code></pre></li> <li>Check the ouput <pre><code># OUTPUT\n/test_data/consensus_mode/barcode01$cd compare_filtered/\n/test_data/consensus_mode/barcode01/compare_filtered$ls\nNanoComp_20250424_1550.log       NanoComp_N50.png                           NanoComp_OverlayHistogram.png                 NanoComp_OverlayLogHistogram.png            NanoComp_total_throughput.html\nNanoComp_lengths_violin.html     NanoComp_number_of_reads.html              NanoComp_OverlayHistogram_Weighted.html       NanoComp_OverlayLogHistogram_Weighted.html  NanoComp_total_throughput.png\nNanoComp_lengths_violin.png      NanoComp_number_of_reads.png               NanoComp_OverlayHistogram_Weighted.png        NanoComp_OverlayLogHistogram_Weighted.png   NanoStats.txt\nNanoComp_log_length_violin.html  NanoComp_OverlayHistogram.html             NanoComp_OverlayLogHistogram.html             NanoComp_quals_violin.html\nNanoComp_log_length_violin.png   NanoComp_OverlayHistogram_Normalized.html  NanoComp_OverlayLogHistogram_Normalized.html  NanoComp_quals_violin.png\nNanoComp_N50.html                NanoComp_OverlayHistogram_Normalized.png   NanoComp_OverlayLogHistogram_Normalized.png   NanoComp-report.html\n</code></pre> *Note: You can dowlod the file <code>NanoComp-report.html</code> and open the file on your computer. See the difference?  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC/#223-run-nanopack-advanced","title":"2.2.3 Run Nanopack: Advanced","text":"<p>If you want to explore more features on the <code>NanoPack</code> program, you can visit the website: https://passelma42.github.io/Field-Sequencing/Tutorial-NanoPack/.  </p> <p>!Important!:  </p> <ul> <li>Don't run the <code>conda</code> commands, no need when using the HPC  </li> <li>Focus on the Exercises and don't forget to activate the necessary modules before running the commands  </li> <li>All test data for the Advanced Nanopack exercises can be downloaded here: <code>/ONT-SEQ_PA-Benin2024_Bioinfocourse/nanopack_Advanced_exampledata</code> </li> </ul>"},{"location":"Tutorial-Data-Analysis-HPC/#3-epi2me-wf-amplicon","title":"3. EPI2ME: wf-amplicon","text":"<p>EPI2ME provides best practice bioinformatics analyses for nanopore sequencing. It offers bioinformatics for all levels of expertise and is designed by Oxford Nanopore especially to work on long read nanopore sequencing data. There is a desktop version available, with all necessary software packaged in a single workflow. Depending on your research there are different workflows available tailored to your needs. These workflows are also available on the command line. No need to install the bioinformatic tools yourself, except the necessary tools to load the workflows.  </p> <p>We will be using the wf-amplicon workflow. As mentioned before, when using this on your own computer, no need to install any bioinformatic tools except Nextflow, docker or singularity/apptainer.</p> <p>FYI General info: https://epi2me.nanoporetech.com/ Workflows Overview: https://epi2me.nanoporetech.com/wfindex/ Installation instructions: https://epi2me.nanoporetech.com/epi2me-docs/installation/ Amplicon Workflow: https://github.com/epi2me-labs/wf-amplicon </p>"},{"location":"Tutorial-Data-Analysis-HPC/#31-set-hpc-environment","title":"3.1 Set HPC environment","text":"<p>Before we can start using the <code>wf-amplicon</code> we need to prepare our HPC environment. For this you need to copy the script <code>singularity_environment_set.sh</code> and execute. These are the steps to follow:  </p> <ol> <li>Change directory to your VSC-DATA folder <pre><code>$ cd $VSC_DATA  \n</code></pre></li> <li>copy the folder scripts to your current location i.e. your VSC_DATA folder  <pre><code>$ cp -r &lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/scripts .\n</code></pre></li> <li>Change to the created folder  <pre><code>$ cd scripts  \n</code></pre></li> <li>Give executable rights to the script <pre><code>$ chmod  u+x  singularity_environment_set.sh\n</code></pre></li> <li>Execute the script using source which will allow the export commands affect your current shell environment <pre><code>$ source singularity_environment_set.sh\n</code></pre></li> <li>Check if the singularity folder has been created and also if it contains the 3 subfolders <code>cache</code>, <code>tmp</code>, and <code>image</code> <pre><code>$ ll $VSC_SCRATCH/singularity \n</code></pre></li> <li>Check if the environment is ready and set to go. These commands should give you the actual paths to the folders <pre><code>echo $SINGULARITY_CACHEDIR\necho $SINGULARITY_TMPDIR\necho $NXF_SINGULARITY_CACHEDIR\n</code></pre></li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC/#32-run-test-consensus-mode","title":"3.2 Run Test - Consensus mode","text":"<p>In the previous chapter you already worked on this data to pracktice the <code>Nanopack</code> tool. Now it is time to run the <code>wf-amplicon</code> pipeline.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#322-check-samplesheet","title":"3.2.2 Check samplesheet","text":"<ol> <li>Navigate to the directory <code>/test_data/consensus_mode</code> </li> <li>Take a look at the samplesheet <pre><code># SAMPLESHEET LAYOUT\n/test_data/consensus_mode$cat samplesheet_ONT-SEQ-25_05.txt \nbarcode,alias,type\nbarcode01,MBB-24-154_barcode01_ONT-SEQ-25_PA-05,test_sample\nbarcode02,MBB-24-156_barcode02_ONT-SEQ-25_PA-05,test_sample\n</code></pre> This shows you 3 columns <code>barcode</code>, <code>alias</code> and <code>type</code>.  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC/#323-run-the-program","title":"3.2.3 Run the program","text":"<p>To run the workflow the only software we need to activate is <code>Nextflow</code>. Activating all other software will be taken care of automatically while running the workflow.  Make sure you are in the folder that contains all your <code>barcodexx</code> folders. In this case it should be <code>/test_data/consensus_mode</code>.  </p> <ol> <li>Activate the nextflow software <pre><code>$ module load Nextflow/24.10.2\n</code></pre></li> <li>Navigate to the folder <code>/test_data/consensus_mode</code> which contains the <code>barcodexx</code> folders.</li> <li>Run the nextflow command to activate the <code>wf-amplicon</code> <pre><code># consensus_mode command\nnextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./consensus_mode \\\n    --sample_sheet ./consensus_mode/samplesheet_ONT-SEQ-25_05.txt \\\n    --out_dir output-consensus_mode \\\n    -profile singularity\n</code></pre></li> </ol> <p>This analysis will pull the necessary containers with all needed software packages, then it will run the tools on your data. You can monitor the progress on your terminal. Because we work with a small test dataset it should not take more than 5 minutes to run.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#324-analyse-output","title":"3.2.4 Analyse output","text":"<pre><code># OUTPUT CONSENSUS MODE\n/test_data/output-consensus_mode$ll\ntotal 4776\n-rw-r--r-- 1 vsc43352 vsc43352    1448 Apr 30 12:05 all-consensus-seqs.fasta\n-rw-r--r-- 1 vsc43352 vsc43352     107 Apr 30 12:05 all-consensus-seqs.fasta.fai\ndrwxr-xr-x 2 vsc43352 vsc43352    4096 Apr 30 12:05 execution\ndrwxr-xr-x 4 vsc43352 vsc43352    4096 Apr 30 12:05 MBB-24-154_barcode01_ONT-SEQ-25_PA-05\ndrwxr-xr-x 4 vsc43352 vsc43352    4096 Apr 30 12:05 MBB-24-156_barcode02_ONT-SEQ-25_PA-05\n-rw-r--r-- 1 vsc43352 vsc43352    1597 Apr 30 12:01 params.json\n-rw-r--r-- 1 vsc43352 vsc43352     138 Apr 30 12:01 sample_sheet.csv\n-rw-r--r-- 1 vsc43352 vsc43352     251 Apr 30 12:05 versions.txt\n-rw-r--r-- 1 vsc43352 vsc43352 2434136 Apr 30 12:05 wf-amplicon-report.html\n</code></pre> <p>Taking a look at the output:  </p> <p>Check here for more info: https://github.com/epi2me-labs/wf-amplicon?tab=readme-ov-file#output-options</p> Title File path Description Per sample or aggregated Workflow report <code>./wf-amplicon-report.html</code> Report for all samples. aggregated Sanitized reference file <code>./reference_sanitized_seqIDs.fasta</code> Some programs used by the workflow don't like special characters (like colons) in the sequence IDs in the reference FASTA file. The reference is thus \"sanitized\" by replacing these characters with underscores. Only generated in variant calling mode. aggregated Sanitized reference index file <code>./reference_sanitized_seqIDs.fasta.fai</code> FAI index for the sanitised reference FASTA file. aggregated Alignments BAM file <code>./{{ alias }}/alignments/{{ alias }}.aligned.sorted.bam</code> BAM file with alignments of input reads against the references (in variant calling mode) or the created consensus (in de-novo consensus mode). per-sample Alignments index file <code>./{{ alias }}/alignments/{{ alias }}.aligned.sorted.bam.bai</code> Index for alignments BAM file. per-sample De-novo consensus FASTQ file <code>./{{ alias }}/consensus/consensus.fastq</code> Consensus sequence file generated by de-novo consensus pipeline. per-sample Consensus FASTA file <code>./{{ alias }}/consensus/medaka.consensus.fasta</code> Consensus sequence file generated by variant calling pipeline. per-sample Variants VCF file <code>./{{ alias }}/variants/medaka.annotated.vcf.gz</code> VCF file of variants detected against the provided reference. per-sample Variants index file <code>./{{ alias }}/variants/medaka.annotated.vcf.gz.csi</code> Index for variants VCF file. per-sample Combined de-novo consensus sequences <code>./all-consensus-seqs.fasta</code> FASTA file containing all de-novo consensus sequences. aggregated Combined de-novo consensus sequences index <code>./all-consensus-seqs.fasta.fai</code> FAI index for the FASTA file with the combined de-novo consensus sequences. aggregated IGV config JSON file <code>./igv.json</code> JSON file with IGV config options to be used by the EPI2ME Desktop Application. aggregated <p>Important for screening your results is the <code>wf-amplicon-report.html</code> and for downstream applications offcourse your <code>fasta</code> files.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#33-run-test-variant-mode","title":"3.3 Run Test - Variant mode","text":"<p>Running in variant mode allows us to run the same analysis as before but since our test data contains reads from both ITS and LSU, we can bioinformatically construct consensus sequences per sample for both genes at the same time.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#332-check-samplesheet","title":"3.3.2 Check samplesheet","text":"<ol> <li>Navigate to the directory <code>/test_data/variant_mode</code> </li> <li>Take a look at the samplesheet <pre><code># SAMPLESHEET LAYOUT\n/test_data/variant_mode$cat samplesheet_ONT-SEQ-24_02-withreference.txt \nbarcode,alias,type,ref\nbarcode12,MBB-24-016_barcode12_ONT-SEQ-24_02,test_sample,ITS LSU\nbarcode14,MBB-24-019_barcode14_ONT-SEQ-24_02,test_sample,ITS LSU\n</code></pre> This shows you 4 columns <code>barcode</code>, <code>alias</code>, <code>type</code> and <code>ref</code>.  </li> <li>Take a look at the reference file <pre><code># REFERENCE FILE\n/test_data/variant_mode$cat reference.fa \n&gt;ITS\nGAAGTAAA...\n&gt;LSU\nTATCAAT...\n</code></pre> This file is a fasta file showing 2 sequences with IDs <code>ITS</code> and <code>LSU</code>, the same as in your samplesheet <code>ref</code> column. This reference file is used to \"search\" for ITS or LSU sequences in your sequence reads i.e. in the <code>barcodexx</code> folders.  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC/#333-run-the-program","title":"3.3.3 Run the program","text":"<p>To run the workflow the only software we need to activate is <code>Nextflow</code>. Activating all other software will be taken care of automatically while running the workflow.  Make sure you are in the folder that contains all your <code>barcodexx</code> folders. In this case it should be <code>/test_data/consensus_mode</code>.  </p> <ol> <li>Activate the nextflow software <pre><code>$ module load Nextflow/24.10.2\n</code></pre></li> <li>Run the nextflow command to activate the <code>wf-amplicon</code> <pre><code># consensus_mode command\nnextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./variant_mode \\\n    --sample_sheet ./variant_mode/samplesheet_ONT-SEQ-24_02-withreference.txt \\\n    --reference ./variant_mode/reference.fa \\\n    --out_dir output-variant_mode \\\n    -profile singularity\n</code></pre></li> </ol> <p>This analysis will pull the necessary containers with all needed software packages, then it will run the tools on your data. You can monitor the progress on your terminal. Since we work with a small test dataset it should not take more than 5 minutes to run.  </p>"},{"location":"Tutorial-Data-Analysis-HPC/#334-analyse-output","title":"3.3.4 Analyse output","text":"<p>In variant-mode the output is slightly different. </p> <pre><code># OUTPUT VARIANT MODE\n/test_data/output-variant_mode$ll\ntotal 5001\ndrwxr-xr-x 2 vsc43352 vsc43352    4096 Apr 30 12:23 execution\ndrwxr-xr-x 5 vsc43352 vsc43352    4096 Apr 30 12:22 MBB-24-016_barcode12_ONT-SEQ-24_02\ndrwxr-xr-x 5 vsc43352 vsc43352    4096 Apr 30 12:22 MBB-24-019_barcode14_ONT-SEQ-24_02\n-rw-r--r-- 1 vsc43352 vsc43352    1645 Apr 30 12:21 params.json\n-rw-r--r-- 1 vsc43352 vsc43352    1666 Apr 30 12:22 reference_sanitized_seqIDs.fasta\n-rw-r--r-- 1 vsc43352 vsc43352      38 Apr 30 12:22 reference_sanitized_seqIDs.fasta.fai\n-rw-r--r-- 1 vsc43352 vsc43352     152 Apr 30 12:21 sample_sheet.csv\n-rwxr--r-- 1 vsc43352 vsc43352    2286 Apr 30 12:35 variantmode-consensus-rename2.0.sh\n-rw-r--r-- 1 vsc43352 vsc43352    7072 Apr 30 12:41 variant-mode-consensus-sequences.fasta\n-rw-r--r-- 1 vsc43352 vsc43352     251 Apr 30 12:22 versions.txt\n-rw-r--r-- 1 vsc43352 vsc43352 2529804 Apr 30 12:23 wf-amplicon-report.html\n</code></pre> <p>The main difference is that you're not only getting consensusfiles but also a vcf file. This file contains variants detected against the provided reference. Can be usefull if you are working with closely related species and you are interested in Single Nucleotid Polymorphism (SNP) information. In our case the sequences in the reference file were only a means to be able to sort out all ITS and LSU reads and assemble consensus sequences from both.  </p> <p>More info on VCF files can be found here:  https://en.wikipedia.org/wiki/Variant_Call_Format </p> <p>What we do miss here is a combined fasta file with all our ITS and LSU sequences and matching headers. This is something the workflow doesn't provide for us. For this feature we will use the script <code>variantmode-consensus-rename2.0.sh</code>.  </p> <p>Follow these instructions to run the script and build a combined fasta file for all your consensus sequences:  </p> <ol> <li>Copy the script to your outputfolder <pre><code>$ cp &lt;path-to&gt;/scripts/variantmode-consensus-rename2.0.sh &lt;path to&gt;/test_data/output-variant_mode\n</code></pre></li> <li>Change permissions to be able to execute the script <pre><code>$ cd &lt;path-to&gt;/test_data/output-variant_mode\n$ chmod u+x variantmode-consensus-rename2.0.sh\n</code></pre></li> <li>Make sure you are in the output folder and execute following command <pre><code>for dir in ./MBB*/; do ./variantmode-consensus-rename2.0.sh \"$dir\" -o variant-mode-consensus-sequences.fasta -a; done\n</code></pre> This commmand will search for all MBB-xx-xx_barcodexx_ONT-SEQ-xx_xx in the output folder, find all consensus sequences per barcode and generate one combined fasta file called <code>variant-mode-consensus-sequences.fasta</code>.  </li> </ol>"},{"location":"Tutorial-Data-Analysis-HPC/#34-run-with-real-data","title":"3.4 Run with real data","text":"<p>Still with us? Time to put theory into practice. You can all start with your first analysis.  </p> <ol> <li>10 samples are selected for you. Check the Mycoblitz2024_specimenlist on Teams to find out which samples are selected for you.</li> <li>Find and download your samples from the <code>&lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01</code> download folder.</li> <li>Run either the consensus-mode and/or variant-mode (only possible for experiments 1 and 2)</li> <li>Check the results  </li> </ol> <p>Optional: If you finish earlier, you can try a 2nd analysis just for practice! Make sure to start a 2nd working folder Analysis2. </p> <p>The samplesheet: have a look </p> <p>Remember the structure of the dataset explained in Chapter 2.Nanopore data. Each ONT-SEQ-2X_PA-0X folder contains a different samplesheet. <pre><code># EXAMPLE SAMPLE SHEET\n&lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01$cat samplesheet_ONT-SEQ-24_01-withreference.txt \n\nbarcode,alias,type,ref\nbarcode01,MBB-24-001_barcode01_ONT-SEQ-24_01,test_sample,ITS LSU\nbarcode02,MBB-24-002_barcode02_ONT-SEQ-24_01,test_sample,ITS LSU\nbarcode03,MBB-24-003_barcode03_ONT-SEQ-24_01,test_sample,ITS LSU\nbarcode04,MBB-24-004_barcode04_ONT-SEQ-24_01,test_sample,ITS LSU\n</code></pre> In this samplesheet <code>ONT-SEQ-24_PA-01-withreference.txt</code> you see that <code>barcode01 = MBB-24-001</code> (for this sequence experiment only!). Now, have a look in the master species list in teams: You can find the detailed list with sample information in the teams General channel: <code>General &gt; Files &gt; Specieslist-overview &gt; Mycoblitz2024-specimenlist.xlsx</code> </p> <p>Example workflow: Copy your selection to your Analysis folder. </p> <ul> <li>I want to download samples from sequence experiment <code>ONT-SEQ-24_PA-01</code> </li> <li>The selected barcodes for me are: barcode 01 up to 10 1.Change directory to your VSC-DATA folder <pre><code>cd $VSC_DATA\n</code></pre> 2.Make a directory to copy your sequence reads to <pre><code>mkdir Analysis01\n</code></pre> 3.Change to the created folder <pre><code>cd Analysis01\n</code></pre> 4.Recursively copy the barcode01 folder to your Analysis01 folder <pre><code>cp -r &lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01/barcode01 $VSC_DATA/Analysis01\n</code></pre> 5.Check the content of the folder, the barcode01 folder should be copied to this Location <pre><code>ls $VSC_DATA/Analysis01\n</code></pre></li> </ul> <p>REPEAT FOR THE 9 OTHER BARCODEFOLDERS YOU SELECTED HINT: use a forloop to iterate over different barcode folders in one command <pre><code>for i in {01..10}; do cp -r &lt;path to&gt;/ONT-SEQ_PA-Benin2024_Input-Data/ONT-SEQ-24_PA-01/barcode$i \"$VSC_DATA/Analysis01\"; done\n</code></pre></p> <p>Remark: Not only copy the <code>barcode</code> folders but also the <code>samplesheet</code> from the sequencing experiment you take the <code>barcodes</code> folders from. </p> <p>When this exercise is finishe, you can run a 2nd Analysis. Just pick 10 barcode folders from the same sequencing experiment and run the exercise again. Be aware tha only experiments <code>ONT-SEQ-24_01</code> and <code>ONT-SEQ-24_02</code> allow you to run the wf-amplicon workflow in variant-mode! </p>"},{"location":"Tutorial-NanoPack/","title":"Tutorial NanoPack","text":"<p>Nanopack is a tool set of different long read processing and analysis tools. Below you'll find elaborate exercises explaining all the different faeatures of this toolset. The publischer also provided a test-data set which can be downloaded in preparation of this tutorial.</p> <p>Tool: https://github.com/wdecoster/nanopack Download test data: https://github.com/wdecoster/nanotest Publication: NanoPack: visualizing and processing long-read sequencing data </p> <p>Wouter De Coster, Svenn D\u2019Hert, Darrin T Schultz, Marc Cruts, Christine Van Broeckhoven, NanoPack: visualizing and processing long-read sequencing dta, Bioinformatics, Volume 34, Issue 15, August 2018, Pages 2666\u20132669, https://doi.org/10.1093/bioinformatics/bty149  </p>"},{"location":"Tutorial-NanoPack/#preparations","title":"Preparations","text":""},{"location":"Tutorial-NanoPack/#installation","title":"Installation","text":"<p>Please follow the installation instructions on the nanopack github website for installing all the individual tools and modules. If you are using conda on your local machine for installing scientific software you can use a dedicated <code>nanopack.yml</code> file and follow the instructions below.  </p> <p>Notes</p> <p>For more information on how to install miniconda on your system have a look here: https://docs.anaconda.com/miniconda/ </p> <p>Following these instructions should create a dedicated environment and install all necessary dependencies:  </p> <ol> <li>Download environment file     <pre><code>wget https://raw.githubusercontent.com/passelma42/conda-envs-VM/main/nanopack.yml\n</code></pre></li> <li>Create the environment and install all dependencies <pre><code>conda env create -f nanopack.yml\n</code></pre></li> <li>Activate the env <pre><code>conda activate Nanopack\n</code></pre></li> <li>Verify installation <pre><code>conda list\n</code></pre></li> </ol>"},{"location":"Tutorial-NanoPack/#download-test-data","title":"Download test data","text":"<p>Clone the git repo. Example<pre><code>git clone git@github.com:wdecoster/nanotest.git\n</code></pre></p> <ul> <li><code>NanoStat</code>: https://github.com/wdecoster/nanotest</li> <li><code>Cramino</code>: https://github.com/wdecoster/cramino/tree/master/test-data </li> <li><code>Chopper</code>: https://github.com/wdecoster/chopper/tree/master/test-data </li> </ul>"},{"location":"Tutorial-NanoPack/#nanostat","title":"NanoStat","text":"<p>Tool for basic statistics. </p> <p>Exercise 1 Generating Basic Statistics from a FASTQ File. You are provided with a compressed FASTQ file (<code>reads.fastq.gz</code>). Generate a statistics report and save the output in a folder called <code>statreports</code> and filnema <code>nanostats.out</code>.</p> <p>Command: <pre><code>NanoStat --fastq reads.fastq.gz --outdir statreports -n nanostats.out\n</code></pre></p> <p>Objective: - Learn how to calculate and store basic statistics from a FASTQ file. - Explore the statistical metrics NanoStat provides for sequencing data.</p> <p>Exercise 2: Working with Summary Files You are given a sequencing summary files (<code>sequencing_summary.txt</code>). Generate a report from this summary files.</p> <p>Command: <pre><code>NanoStat --summary sequencing_summary.txt --outdir statreports -n nanostats-seqsummary.out\n</code></pre></p> <p>Objective: - Learn how to calculate statistics from multiple summary files and filter based on specific read types (1D2 in this case). - Understand how NanoStat handles multiple input files and the read type filtering option.</p> <p>Exercise 3: Calculating Statistics for BAM Files You are working with multiple BAM files (<code>alignment.bam</code>). Generate statistics for aligned reads and print the results to the terminal.</p> <p>Command: <pre><code>NanoStat --bam alignment.bam\n</code></pre></p> <p>Objective: - Explore how NanoStat calculates statistics from BAM files containing aligned reads. - Understand the metrics provided for aligned data and how to handle BAM files.</p> <p>Exercise 4: Exporting Statistics as TSV You are provided with a FASTA file (<code>reads.fasta.gz</code>). Generate a statistics report and export it as a TSV (Tab-Separated Values) file.</p> <p>Command: <pre><code>NanoStat --fasta reads.fasta.gz --tsv --outdir tsv_report -n nanostat-tsv.out\n</code></pre></p> <p>Objective: - Learn how to generate and export statistics in TSV format, which can be useful for downstream analysis. - Explore how to work with FASTA files as input data.</p>"},{"location":"Tutorial-NanoPack/#chopper","title":"Chopper","text":"<p>A rust implementation combining NanoLyse and NanoFilt into one faster tool for filtering, trimming, and removing contaminants. </p> <p>Download test data: https://github.com/wdecoster/chopper/tree/master/test-data</p> <p>Exercise 1: Basic Quality Filtering You are provided with a FASTQ file (<code>reads.fastq</code>) and need to filter out reads with a Phred average quality score below 20. Use the default settings for other parameters.</p> <p>Command: <pre><code>chopper -q 20 -i ./nanotest/reads.fastq.gz &gt; Q20_filtered.reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on minimum quality score. - Understand the impact of quality filtering on read data. - Get statistics from your reads before and after filtering and compare.</p> <p>Exercise 2: Length-Based Filtering You have a FASTQ file (<code>reads.fastq</code>) and need to filter out reads shorter than 100 base pairs and longer than 1000 base pairs. Use the default settings for other parameters.</p> <p>Command: <pre><code>chopper -l 10000 --maxlength 250000 -i reads.fastq &gt; length_filtered_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on length. - Understand how length-based filtering affects the dataset.</p> <p>Exercise 3: Quality and Length Filtering with Trimming You are working with a FASTQ file (<code>reads.fastq</code>). Set a minimum quality score of 30 and filter reads between 200 and 800 base pairs in length. Trim 10 nucleotides from both the start and end of each read.</p> <p>Command: <pre><code>chopper -q 12 -l 200 --maxlength 250000 --headcrop 10 --tailcrop 10 -i reads.fastq.gz &gt; filtered_trimmed_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to combine quality and length filtering with trimming. - Explore the effects of trimming on read length and quality.</p> <p>Exercise 4: Contaminant Filtering You have a FASTQ file (<code>test.fastq</code>) and a contaminant FASTA file (<code>random_contam.fa</code>). Filter out reads that match contaminants using Chopper.</p> <p>Command: <pre><code>chopper -c chopper-random_contam.fa -i chopper.test.fastq &gt; chopper_clean_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on contamination against a FASTA file. - Understand the process of removing contaminant sequences.</p> <p>Exercise 5: Inverse Filtering You are provided with a FASTQ file (<code>reads.fastq</code>) and need to keep only the reads that do not match the contaminants in <code>contaminants.fasta</code>.</p> <p>Command: <pre><code>chopper -c chopper-random_contam.fa --inverse -i chopper.test.fastq &gt; chopper_clean_reads_inverse.fastq\n</code></pre></p> <p>Objective: - Learn how to use inverse filtering to retain non-contaminant reads. - Understand the difference between standard and inverse filtering.</p> <p>Exercise 6: GC Content Filtering You have a FASTQ file (<code>reads.fastq</code>) and want to filter out reads with GC content outside the range of 30% to 60%.</p> <p>Command: <pre><code>chopper --mingc 0.3 --maxgc 0.6 -i testGC.fastq &gt; chopper_gc_filtered_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to filter reads based on GC content. - Understand the implications of GC content filtering on read data.</p> <p>Exercise 7 - Optional: Multi-threaded Processing When you have a large FASTQ file (<code>large_reads.fastq</code>) and want to speed up processing by using 8 threads. Filter reads with a minimum Phred quality score of 25 and a length between 100 and 1000 base pairs.</p> <p>Notes</p> <p>You can check the number of threads available on your system in various ways depending on your OS. For Linux type the command <code>lscpu</code>.</p> <p>Command: <pre><code>chopper -q 25 -l 100 --maxlength 1000 -t 8 -i large_reads.fastq &gt; filtered_reads.fastq\n</code></pre></p> <p>Objective: - Learn how to utilize multi-threading for faster processing. - Explore the effect of parallel processing on large datasets.</p>"},{"location":"Tutorial-NanoPack/#nanocomp","title":"NanoComp","text":"<p>A tool for comparing multiple runs on read length and quality based on reads (fastq), alignments (bam) or albacore summary files. </p> <p>Exercise 1: Comparing BAM Files Compare multiple BAM files generated from different sequencing runs to assess read length distribution and other metrics.</p> <p>Command: <pre><code>NanoComp --bam alignment.bam small-test-phased.bam --outdir compare_bams --plot box --names run1 run2\n</code></pre> Make sure you downloaded the test data in the Download test data section.  </p> <p>Objective: - You will be able to visualize and interpret differences in read length distribution across sequencing runs.</p> <p>Exercise 2: Filtering FASTQ Files by Read Length Compare read lengths of FASTQ files while filtering out reads that fall outside a specific length range.  </p> <p>Command:   <pre><code>NanoComp --fastq reads.fastq.gz chopper.test.fastq --minlength 1000 --maxlength 50000 --outdir compare_filtered --plot violin\n</code></pre></p> <p>Objectives: - Learn how filtering by read length can influence data visualization and results. - Compare without filtering.</p> <p>Exercise 4: Using Different Plot Types Explore different visualization methods by generating various types of plots. - Command:   <pre><code>NanoComp --fastq reads.fastq.gz chopper.test.fastq --names run1 run2 --plot ridge --outdir compare_plots\n</code></pre></p> <p>Objectives: - Learn how different visualization techniques can highlight various aspects of sequencing data. - Generate different plots (ridge plot, violin plot, box plot) by adjusting the <code>--plot</code> parameter.</p> <p>Use Case: Generating a TSV Summary of Multiple Sequencing Files Objective: Create a summary TSV file with statistics for multiple sequencing files. - Command:   <pre><code>  NanoComp --fastq ./nanotest/reads.fastq.gz ./nanotest/chopper.test.fastq --names run1 run2  --tsv_stats --outdir tvs_summary\n</code></pre> - Objective: - Learn how to generate tabular summaries that can be used in downstream analysis or reports. This can be particularly useful when comparing several datasets in a systematic way. - To view tab delimited files issue this commant: <code>column -s $'\\t' -t &lt; NanoStats.txt</code> - Get quick info out of NanoStats on longest read and it's Q score: <code>column -s $'\\t' -t &lt; NanoStats.txt | grep \"longest_read_(with_Q):1 \"</code></p>"},{"location":"Tutorial-NanoPack/#cramino","title":"Cramino","text":"<p>A rust replacement for NanoStat - much quicker summary creation of BAM or CRAM files. </p> <p>Download test data: https://github.com/wdecoster/cramino/tree/master/test-data</p> <p>Exercise 1: Extract Basic QC Metrics from a BAM File You are provided with a BAM file (<code>alignment.bam</code>). Use Cramino to extract the basic QC metrics and use 4 parallel decompression threads (default). </p> <p>Command: <pre><code>cramino alignment.bam\n</code></pre></p> <p>Objective: - Learn how to extract quality control metrics from a BAM file. - Explore the default output provided by Cramino.</p> <p>Exercise 2: Generate Read Length Histograms You are given a BAM file (<code>alignment.bam</code>) and need to generate histograms of the read lengths.</p> <p>Command: <pre><code>cramino alignment.bam --hist\n</code></pre></p> <p>Objective: - Learn how to generate histograms of read lengths using Cramino. - Visualize the distribution of read lengths within the dataset.</p> <p>Exercise 3: Filter Reads by Minimum Length You are provided with a CRAM file (<code>alignment.cram</code>) and only want to include reads that are at least 500 base pairs long.  </p> <p>Command: <pre><code>cramino alignment.cram --min-read-len 500\n</code></pre></p> <p>Objective: - Learn how to filter reads by minimum length when extracting QC metrics. - Understand how length filtering affects the output. - Compare histograms with and without filtering</p>"},{"location":"Tutorial-NanoPack/#nanoplot","title":"NanoPlot","text":"<p>A tool for visualizing read data. </p> <p>Exercise 1: Plot Read Length Distribution from FASTQ Data.</p> <p>Command: <pre><code>NanoPlot --fastq chopper.test.fastq -o output_ex1/\n</code></pre></p> <p>Objective: Learn how to visualize the distribution of read lengths from a FASTQ file using NanoPlot.</p> <p>Exercise 2: Analyzing BAM File Alignments. This exercise will teach you how to visualize the read lengths for aligned reads from a BAM file using NanoPlot.  </p> <p>Command: <pre><code>NanoPlot --bam small-test-phased.bam --alength -o output_ex2/ --format pdf\n</code></pre> Objective: Visualize the aligned read lengths from BAM files.</p> <p>Exercise 3: Logarithmic Transformation of Read Lengths In this exercise, you'll explore how NanoPlot can transform read length distributions using a logarithmic scale.  </p> <p>Command: <pre><code>NanoPlot --fastq chopper.test.fastq --loglength -o output_ex3/\n</code></pre></p> <p>Objective: Learn how to apply logarithmic scaling to read length distributions.</p> <p>Exercise 4: Plotting Quality Scores This exercise focuses on generating plots to visualize the quality scores of sequencing reads.  </p> <p>Command: <pre><code>NanoPlot --fastq filtered_trimmed_reads.fastq --percentqual -o output_ex4/ --format png\n</code></pre></p> <p>Objective: Visualize quality scores from sequencing reads.  </p> <p>Exercise 5: Downsampling Large Datasets In this exercise, you will use NanoPlot to downsample a large FASTQ file to reduce the dataset size for faster plotting.  </p> <p>Command: <pre><code>NanoPlot --fastq reads.fastq.gz --downsample 10000 -o output_ex5/\n</code></pre></p> <p>Objective: Explore the downsampling feature of NanoPlot to work with large datasets.</p> <p>Exercise 6: Filtering Reads by Quality Learn how to filter reads by their average quality scores before plotting.  </p> <p>Command: <pre><code>NanoPlot --fastq Q10_filtered.reads.fastq --minqual 15 -o output_ex6/\n</code></pre></p> <p>Objective: Filter reads based on average quality scores and visualize the results.</p>"},{"location":"Tutorial-NanoPack/#plot-twists","title":"Plot twists","text":"<p>Wouter is like the David Lynch of plots. Please find below a short description on each plot and what it tells you.  </p> <p>Weighted Histogram of Read Lengths </p> <ul> <li>Description: This plot displays the distribution of read lengths, where each read is assigned a weight based on its abundance, quality, or other criteria. It helps to visualize which lengths contribute most significantly to the dataset.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Weighted frequency (number of reads multiplied by their weight).</li> <li>Example: Imagine a sequencing run where 500 reads are 150 bp long and 50 reads are 10,000 bp long. In a weighted histogram, the long reads would have a high weight due to their length, leading to a taller bar for the 10,000 bp category even though there are fewer of them. This shows that while there are many short reads, the long reads contribute significantly to the total base pair yield.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_1","title":"Tutorial NanoPack","text":"<p>Weighted Histogram of Read Lengths After Log Transformation </p> <ul> <li>Description: Similar to the previous plot, this histogram displays read lengths after applying a logarithmic transformation. Log transformation is useful for visualizing data that spans several orders of magnitude.</li> <li>X-axis: Log-transformed read length (log10 of the length).</li> <li>Y-axis: Weighted frequency.</li> <li>Example: If your dataset contains reads ranging from 100 bp to 10,000 bp, the log transformation will allow you to visualize the distribution more clearly. In this case, short reads (100-200 bp) might dominate the histogram, but you can still see the contribution of long reads (which would appear much smaller on a linear scale).  </li> </ul>"},{"location":"Tutorial-NanoPack/#_2","title":"Tutorial NanoPack","text":"<p>Non-Weighted Histogram of Read Lengths </p> <ul> <li>Description: This plot shows the distribution of read lengths without applying any weighting, simply counting the number of reads for each length category.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Frequency (number of reads).</li> <li>Example: If you have 700 reads of 150 bp and 300 reads of 300 bp, the histogram will show a higher bar for the 150 bp length. This representation is straightforward but doesn\u2019t account for the potential importance of longer reads.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_3","title":"Tutorial NanoPack","text":"<p>Non-Weighted Histogram of Read Lengths After Log Transformation </p> <ul> <li>Description: A non-weighted histogram that uses log transformation on read lengths to accommodate large variances in length.</li> <li>X-axis: Log-transformed read length (log10 of the length).</li> <li>Y-axis: Frequency.</li> <li>Example: This histogram could show that a large number of reads are concentrated around the short lengths (e.g., 100-500 bp, appearing as a single peak on a log scale) while still revealing how many long reads exist. The log transformation helps mitigate the influence of extremely long reads that might skew the data on a linear scale.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_4","title":"Tutorial NanoPack","text":"<p>Yield by Length </p> <ul> <li>Description: This plot illustrates how much sequencing data (yield) is generated for each read length category, providing insight into which lengths contribute most to the overall sequencing effort.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Yield (total base pairs sequenced).</li> <li>Example: If the sequencing effort produced many short reads but only a few long reads that cover a significant amount of bases, the yield might be higher for long reads despite their lower count. This plot would visually show that longer reads contribute more to total yield, helping researchers decide on read length strategies.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_5","title":"Tutorial NanoPack","text":"<p>Read Lengths vs. Average Read Quality Plot </p> <ul> <li>Description: This plot compares the lengths of reads against their average quality scores (typically represented as Phred scores). It provides insights into how read length affects quality.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Average read quality score (e.g., Phred score).</li> <li>Example: If you find that shorter reads tend to have higher average quality (e.g., many are Q30 or higher), while longer reads fall to Q20 or lower, this might indicate that the longer reads are more prone to errors, possibly due to the sequencing technology used.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_6","title":"Tutorial NanoPack","text":"<p>Aligned Read Lengths vs. Sequence Read Length </p> <ul> <li>Description: This plot compares the lengths of reads that have been aligned to a reference genome against their original lengths, highlighting the differences caused by alignment (e.g., clipping).</li> <li>X-axis: Aligned read length (length after alignment).</li> <li>Y-axis: Original read length (sequence read length).</li> <li>Example: Points clustering along the diagonal indicate that reads aligned well without significant clipping. Points above the diagonal suggest soft-clipped reads, while points below might indicate that reads were trimmed significantly or poorly aligned.</li> </ul>"},{"location":"Tutorial-NanoPack/#_7","title":"Tutorial NanoPack","text":"<p>Read Mapping Quality vs. Average Basecall Quality Plot </p> <ul> <li>Description: This plot assesses the relationship between read mapping quality (confidence that the read is correctly aligned) and average base quality (confidence in the accuracy of the base calls).</li> <li>X-axis: Read mapping quality (often denoted as MAPQ score).</li> <li>Y-axis: Average base quality score.</li> <li>Example: A cluster of points in the upper right indicates high-quality reads that map confidently. If many low mapping quality reads also show high average base quality, it might suggest alignment issues, even if the bases are accurate.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_8","title":"Tutorial NanoPack","text":"<p>Read Length vs. Read Mapping Quality </p> <ul> <li>Description: This plot explores how read length correlates with mapping quality, indicating whether longer reads tend to align better or worse than shorter ones.</li> <li>X-axis: Read length (in base pairs).</li> <li>Y-axis: Read mapping quality (MAPQ score).</li> <li>Example: You might observe that shorter reads cluster in the lower mapping quality range, indicating they map ambiguously, while longer reads often show higher mapping quality, reflecting their capacity to provide unique alignments.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_9","title":"Tutorial NanoPack","text":"<p>Percent Identity vs. Average Base Quality Plot </p> <ul> <li>Description: This plot compares percent identity (how well the read matches the reference genome) against the average quality score of the bases in the read.</li> <li>X-axis: Percent identity (percentage of bases matching the reference).</li> <li>Y-axis: Average base quality score.</li> <li>Example: A trend where high percent identity corresponds with high average base quality indicates that better-quality reads tend to align more accurately. If some low-quality reads also have high percent identity, it might suggest they are mismatches that coincidentally align well.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_10","title":"Tutorial NanoPack","text":"<p>Aligned Length vs. Percent Identity </p> <ul> <li>Description: This plot compares the length of reads that have been aligned to the reference genome against their percent identity, highlighting how alignment length relates to alignment quality.</li> <li>X-axis: Aligned read length (length after alignment).</li> <li>Y-axis: Percent identity (percentage of matches).</li> <li>Example: Points in the upper right show long aligned reads that match well with the reference. If some long reads have low percent identity, it might indicate challenges in aligning due to errors or complex regions in the genome.  </li> </ul>"},{"location":"Tutorial-NanoPack/#_11","title":"Tutorial NanoPack","text":"<p>Dynamic Histogram of Percent Identity </p> <ul> <li>Description: An interactive histogram that shows how the distribution of percent identity changes based on applied filters (like read length or mapping quality). Users can adjust parameters in real-time to observe shifts in the data.</li> <li>X-axis: Percent identity (percentage).</li> <li>Y-axis: Frequency (or weighted frequency).</li> <li>Example: You might start with a general view showing a broad range of percent identities. When you apply a filter to exclude low-quality reads, the histogram updates to show a concentration of higher percent identities, helping you understand the impact of quality filtering on your data.  </li> </ul> <p> </p>"},{"location":"Tutorial-NanoPack/#addendum","title":"Addendum","text":""},{"location":"Tutorial-NanoPack/#pid-vs-mapq","title":"PID VS MAPQ","text":"<p>Percent Identity VS Mapping Quality. Both are important metrics used in analyzing sequencing data. But are NOT the same.</p> <p>Percent Identity (PID) </p> <ul> <li>Definition: Percent Identity refers to the proportion of identical bases between the read and the reference sequence in an alignment. It is calculated as the number of matching bases divided by the total number of aligned bases, often expressed as a percentage.</li> <li>Purpose: PID indicates how similar the aligned read is to the reference, providing a measure of alignment accuracy. A higher PID means the read matches the reference more closely.</li> <li>Example: If a read aligns to the reference with 98% identical bases, the PID is 98%. PID is often used to assess how well the reads align to a reference at the sequence level.</li> </ul> <p>Mapping Quality (MAPQ) </p> <ul> <li>Definition: Mapping Quality is a score assigned by an alignment algorithm (such as BWA, minimap2, etc.) that represents the confidence in the placement of the read in the genome. It quantifies how likely it is that the read is aligned to the correct location. The MAPQ score is usually derived from both the uniqueness of the alignment and the quality of the alignment itself.</li> <li>Purpose: MAPQ reflects the reliability of the alignment in terms of positional accuracy. It considers whether the read could have aligned equally well to other locations in the genome (ambiguity), as well as the general quality of the alignment.</li> <li>Example: A MAPQ score of 60 typically indicates that the read is uniquely aligned with high confidence, while lower MAPQ values (e.g., below 30) suggest that the read may have multiple possible alignment locations or lower confidence in its placement.</li> </ul> <p>Key Differences </p> <p>Scope:  </p> <ul> <li>PID measures the sequence similarity between the read and the reference, focusing on how many bases match.</li> <li>MAPQ measures the confidence in the read's placement in the genome, focusing on alignment ambiguity and positional accuracy.</li> </ul> <p>Calculation:  </p> <ul> <li>PID is calculated from the alignment by dividing matching bases by the total number of aligned bases.</li> <li>MAPQ is calculated by the aligner based on the alignment score and the likelihood that the read could be placed elsewhere.</li> </ul> <p>Interpretation:  </p> <ul> <li>A high PID means the read is very similar to the reference in terms of its base sequence.</li> <li>A high MAPQ means there is high confidence that the read is placed in the correct position in the genome, indicating a unique or highly confident alignment.</li> </ul> <p>Example: </p> <p>A read could have a high PID (e.g., 99% identical to the reference) but a low MAPQ (e.g., 20), which would suggest that while the read sequence closely matches the reference, the read might have multiple valid alignment locations due to repetitive regions in the genome.</p> <p>Both metrics are used together to assess different aspects of sequencing data quality: PID for sequence matching and MAPQ for alignment confidence.</p>"},{"location":"Tutorial-ONTdata/","title":"Tutorial ONTdata","text":""},{"location":"Tutorial-ONTdata/#intro","title":"Intro","text":"<p>At UGent Botanic Garden we organized a field sequencing experiment. The setup was to mimick field conditions i.e. sampling fungi, DNA extraction and PCR on the bentolab followed by live sequencing and basecalling on the mk1B from Oxford nanopore. Below you'll find an outline of the data analysis part of this setup including downloadlinks to the data icluding runcommands, so you can rerun the analysis and practice at your own pace.  </p>"},{"location":"Tutorial-ONTdata/#the-data","title":"The data","text":"<p>The original data file consists of reads for 27 accessions collected from the area around Campus Sterra at UGhent. All the QC examples below are taken from this data set. If you want to test the workflow I have provided a toy data set with reads for 3 accessions. You can download the toy-data here: <pre><code>wget https://github.com/passelma42/Field-Sequencing/raw/main/toy-data.tar.gz\n</code></pre></p> <p>Notes</p> <p>The example is ~20MB and contains reads for 2 samples: Barcode41 and Barcode63. Analysis time was 30 minutes on i7 Intel, 8 core, 16Gb Ram computer.</p>"},{"location":"Tutorial-ONTdata/#the-sequencing-run","title":"The Sequencing Run","text":"<p>Method: Rapid Barcoding SQK-RBK114.96 Data Analysis: wf-amplicon EPI2ME </p> <p>Notes</p> <p>The data and output described below was generated on the full data set. Because this is too large, I cannot share it on this platform. When you run the toy-data though you'll get for the analyzed barcodes the same output as you'll find them in the below plots.  </p> <p>We chose the RBK114 kit because this kit is best applicable in the field if you don't have access to a freezer or fridge. The 'Field sequencing kit' from nanopore is legacy at this point (2024) and was replaced by the Rapid Sequencing kits. In this post you can find results of a stability experiment of the kit kept at ambient temperature.</p> <p>The sequencing data folder </p> <pre><code>20240506_1258_MN35631_ASX408_61292243/\n        \u251c\u2500\u2500 barcode_alignment_ASX408_61292243_dc2466ae.tsv\n        \u251c\u2500\u2500 fastq_fail\n        \u251c\u2500\u2500 fastq_pass\n        \u251c\u2500\u2500 fastq_pass.tar.gz\n        \u251c\u2500\u2500 final_summary_ASX408_61292243_dc2466ae.txt\n        \u251c\u2500\u2500 other_reports\n        \u251c\u2500\u2500 pod5_fail\n        \u251c\u2500\u2500 pod5_pass\n        \u251c\u2500\u2500 pore_activity_ASX408_61292243_dc2466ae.csv\n        \u251c\u2500\u2500 report_ASX408_20240506_1302_61292243.json\n        \u251c\u2500\u2500 report_ASX408_20240506_1302_61292243.md\n        \u251c\u2500\u2500 sample_sheet_ASX408_20240506_1302_61292243.csv\n        \u251c\u2500\u2500 sequencing_summary_ASX408_61292243_dc2466ae.txt\n        \u2514\u2500\u2500 throughput_ASX408_61292243_dc2466ae.csv\n</code></pre> <p>Fastq_pass This folder is where you'll find your sequence reads. Basecalling at SUP means that all reads in this folder are q10 or higher.  </p> <pre><code>20240506_1258_MN35631_ASX408_61292243/fastq_pass/\n            \u251c\u2500\u2500 barcode41\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_0.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_1.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_10.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_11.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_12.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode41_61292243_dc2466ae_13.fastq.gz\n            \u251c\u2500\u2500 barcode42\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_0.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_1.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_10.fastq.gz\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ASX408_pass_barcode42_61292243_dc2466ae_11.fastq.gz\n</code></pre>"},{"location":"Tutorial-ONTdata/#qc-sequence-run","title":"QC sequence run","text":"<p>Validating your sequencerun requires you to look at a lot of data. Max throughput of a minion flowcell is 50Gbases! Not hard to imagine that doing this manually is not the way to go. For this job we need dedicated sotware! There are different tools for visualizing HTS data. Nanopack is such a tool, opzimized for longread data. For validating the sequence run we can use the sequencing_summary_ASX408_61292243_dc2466ae.txt file which is generated by Minknow.  </p>"},{"location":"Tutorial-ONTdata/#tools-and-commands","title":"Tools and Commands","text":"<p>Tool: Nanopack Nanoplot Command<pre><code>NanoPlot --summary sequencing_summary_ASX408_61292243_dc2466ae.txt -o summary-plots\n</code></pre> Output </p> <p>NanoPlot gives you a set of .png plots, .txt statistics and also a handy interactive Report.html file. We'll take a closer look at some of the output to get grips on what we have sequenced.  </p> Metric Value Active channels 88.0 Mean read length 527.2 Mean read quality 12.2 Median read length 478.0 Median read quality 15.3 Number of reads 499,919.0 Read length N50 594.0 STDEV read length 2,745.3 Total bases 263,532,800.0 &gt;Q5 492,143 (98.4%), 252.2Mb &gt;Q7 472,784 (94.6%), 238.4Mb &gt;Q10 434,979 (87.0%), 218.4Mb &gt;Q12 392,967 (78.6%), 198.9Mb &gt;Q15 266,363 (53.3%), 137.8Mb <p>Number of reads over time </p> <p>Active pores over time </p> <p>Quality VS sequencing speed </p> <p>Time vs Speed </p> <p>Quality VS length log scaled (min 200bp - max 1000bp) </p> <p>Summary From these plots and stats we can confirm that we have nearly 500.000 reads with 87% of the reads Q10 and higher. Be aware that the RB114 kit performs tagmentation to add barcodes and adapers hence the 'low' mean read length. FYI our amplicons designed with ITS1f-ITS are about 900bp of length.  </p>"},{"location":"Tutorial-ONTdata/#the-data-analysis","title":"The Data Analysis","text":""},{"location":"Tutorial-ONTdata/#qc-data","title":"QC data","text":"<p>In the previous chapter we performed a QC on the sequencerun to validate the success of failure of the minion sequencerun. Now it is time to have a deeper look because we want to know how well our samples have performed. For this experiment we did ITS pcr on 27 samples collected at campus Sterre at UGhent. These samples have been assigned a barcode during the libraryprep and multiplexed into 1 single library to be sequenced. This means the data has to be demultiplexed, each unique Barcode sequence is retreived bioinformatically and assigned to a designated barcode folder (see topology above). Equimolar pooling of our barcodes during the libraryprep allows for an equal amount of reads per barcode. At least in theory. In real life, 'Numbers tell the table', we are going to measure this. We will again use the nanopack tool but now instead of the NanoPlot command we'll use the NanoComp command. In this way all the barcode stats will lign up in a nice overview. We will also need to prep the data a little bit and run this command in a 'for loop' script. We don't want to tire ourselves too much. If this doesn't ring a bell, don't care too much, just follow the instructions below.</p> <p>Tool: Nanopack Script: NanoComp-allfastq.sh </p> <ol> <li>Download the script. The script will loop over your barcode folders containing the demultiplexed fastq files. Download and test the script<pre><code>$ wget https://raw.githubusercontent.com/passelma42/scrippets/main/nanocomp-allfastq.sh\n$ chmod u+x nanocomp-allfastq.sh        # Set run permissions for the user\n$ ./nanocomp-allfastq.sh -h             # Display help function\n</code></pre></li> <li>Run NanoComp over your samples The script will concatenate all fastq files per barcode folder and run nanocomp. After the analysis has finished you'll find a .nanocompout/ folder inside the input folder you issued in the command below (under the -d flag). Command<pre><code>./nanocomp-allfastq.sh -d ./fastq_pass/ -c\n</code></pre> Output<pre><code>(nanopack) passelma@grover:~/Analyses/wf-amplicon/ONT-field2-greenhouse/benin-field2-greenhouse-run/testrunQC/fastq_pass/.nanocompout$ tree\n.\n\u251c\u2500\u2500 NanoComp-report.html\n\u251c\u2500\u2500 NanoComp_20240823_1521.log\n\u251c\u2500\u2500 NanoComp_N50.html\n\u251c\u2500\u2500 NanoComp_N50.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Normalized.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Normalized.png\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Weighted.html\n\u251c\u2500\u2500 NanoComp_OverlayHistogram_Weighted.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Normalized.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Normalized.png\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Weighted.html\n\u251c\u2500\u2500 NanoComp_OverlayLogHistogram_Weighted.png\n\u251c\u2500\u2500 NanoComp_lengths_violin.html\n\u251c\u2500\u2500 NanoComp_lengths_violin.png\n\u251c\u2500\u2500 NanoComp_log_length_violin.html\n\u251c\u2500\u2500 NanoComp_log_length_violin.png\n\u251c\u2500\u2500 NanoComp_number_of_reads.html\n\u251c\u2500\u2500 NanoComp_number_of_reads.png\n\u251c\u2500\u2500 NanoComp_quals_violin.html\n\u251c\u2500\u2500 NanoComp_quals_violin.png\n\u251c\u2500\u2500 NanoComp_total_throughput.html\n\u251c\u2500\u2500 NanoComp_total_throughput.png\n\u2514\u2500\u2500 NanoStats.txt\n</code></pre> NanoComp Number of reads per barcode Amongst other plots and reports we will have a look at number of reads per barcode. Although all samples have been equimolary pooled, we can already see that BC063 is underrepresented. If only this won't give us coverage issues in downstream applications :-o ?? Take a closer look at the file NanoComp-report.html for more details on other stats.  NanoComp_quals_violin In this violin plot we can confirm all our reads per barcode are of good quality. All the reads are Q10 and above. Except again BC63 is acting up... . </li> </ol>"},{"location":"Tutorial-ONTdata/#build-a-consensus","title":"Build a consensus","text":"<p>Now that we verified that we have sufficient reads per barcode and they are of good quality, it is time to start with our downstream analysis. It is time to get to buisiness! Off course there are many workflows to have a go at this type of data, but in this tutorial we will be showcasing the EPI2ME tool called wf-amplicon.. All necessary info on how to setup the necessary software and tools to run this workflow can be found on the 'installation' page of this website. In this section we will walk you through the process on how use the tool to build your consensus sequences from your raw nanopore reads.  </p> <p>Tool: wf-amplicon </p> <p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo. Running the tool in 'variant' mode it is possible to run more than one target (in this case genespecific amplicon) for the same species. But for this tutorial we will run in the de novo mode.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p> <p>Run the workflow The workflow can handle different folder layouts. If you followed the above sections of this tutorial, we are working with structure (iii).  data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre></p> <ol> <li> <p>Prepare a sample sheet This is a comma delimited file with 3 (or 4 if you run in variant mode) sections. The sample sheet is a CSV file with, minimally, columns named barcode and alias. Extra columns are allowed. In variant calling mode, a ref column can be added to tell the workflow which reference sequences should be used for which sample Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode01,BC01,test_sample,ref1\nbarcode02,BC02,test_sample\nbarcode03,BC03,test_sample,ref2\n</code></pre></p> </li> <li> <p>Prepare reference file when running variant mode When using a reference file called reference.fa it need to look like this. Each ref sequence would the represent a genespecific consensus sequence.  This reference will then be used to fish for sequences in your reads to catch and separate per gene. For instance if you amplified ITS and rpb2, mix them per sample and used this as a template to build your library, this tool could give you 2 consensus sequences per species: one ITS consensus and one rpb2 consensus. Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> </li> <li>Run the wf-amplicon on your samples The command below will run a singularity container in a nextflow run. The container beeing epi2me-labs/wf-amplicon and the command flags issued by the nextflow tool. Rember you have some prepping to do to get your system up and running to be able to run EPI2ME workflows. But believe me, this way is more easier than installing the tools yourself and writing designated scripts to stitch together all data input and oututs to feed into the next program. Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\\n    -profile singulairy\n</code></pre><pre><code>--fastq: Path to Input folder which holds fastq files or barcode directories  \n--reference: Including this line invoces running in variant mode, delete if you want to run in de novo mode  \n--sample_sheet: Path to sample sheet file  \n-c: path to config file, see 'Installation' section of this website  \n-profile: We are running the container under the singularity flag, not docker because docker doesn't play nice with HPC\n</code></pre> </li> </ol> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Make sure to delete the ref column in the sample sheet and leave out the --reference flag from the command in that case.  </p>"},{"location":"Tutorial-ONTdata/#get-your-data-out-of-here","title":"Get your data out of here","text":"<p>Once the workflow has finished, the output can be found in a folder called output-wf-apmlicon. Folder layout<pre><code>.\n\u251c\u2500\u2500 output-wf-amplicon\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 barcode41\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 alignments\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned.sorted.bam\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 aligned.sorted.bam.bai\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 consensus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 consensus.fastq\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 barcode42\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 alignments\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned.sorted.bam\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 aligned.sorted.bam.bai\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 consensus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 consensus.fastq\n</code></pre> Offcourse it would be too much to ask if the consensus sequence is given to us in a nice way i.e. having a sensible discriptive name like barcode41.consensus.fastq. Or something similar. \"Alas poor Yorick\". We'll have to fidle ourselves. Script: rename-consensus.sh This script will digg into each individual barcode folder and rename the consensus.fastq file accordingly and finally copy it to output-wf-ampmicon folder. Isn't that neat!  </p> <ol> <li>Download the script and set runpermissions <pre><code>wget https://raw.githubusercontent.com/passelma42/scrippets/main/rename-consensus.sh\nchmod u+x rename-consensus.sh       # Set run permissions for the user\n</code></pre></li> <li>Execute the script <pre><code>./rename-consensus.sh\n</code></pre></li> </ol> <p>Warning</p> <p>Verify you run the script from the output-wf-amplicon folder  </p> <p>Layout folder after rename script<pre><code>/output-wf-amlicon-nosubsample$ tree -L 1\n    .\n    \u251c\u2500\u2500 barcode41\n    \u251c\u2500\u2500 barcode41.consensus.fastq\n    \u251c\u2500\u2500 barcode42\n    \u251c\u2500\u2500 barcode42.consensus.fastq\n</code></pre> This final output gives you one consensus fastq per barcode which now you can use for blast or other alignment tools, phylogenetics,... .  </p>"},{"location":"blast/","title":"Blast","text":""},{"location":"blast/#blast-cli","title":"Blast-CLI","text":""},{"location":"blast/#intro","title":"Intro","text":"<p>The BLAST algorithm, which stands for Basic Local Alignment Search Tool, is a widely used and highly effective algorithm in bioinformatics and computational biology. It is used to compare biological sequences, such as DNA, RNA, or protein sequences, to identify regions of similarity or homology. The primary goal of the BLAST algorithm is to find local alignments between a query sequence and a database of sequences, which can help researchers identify genes, functional domains, and evolutionary relationships between sequences. Blast can be run at the NCBI website. Or a Command Line Interface can be downloaded to your own system.</p> <p>Here are the key components and steps of the BLAST algorithm:  </p> <ol> <li>Query Sequence: BLAST starts with a query sequence provided by the user, which is the sequence you want to compare against a database of other sequences.  </li> <li>Database: BLAST searches against a database containing a collection of sequences. This can be a database of known genes, genomes, proteins, or any other relevant biological sequences.  </li> <li>Scoring System: BLAST uses a scoring system to assign a numerical score to alignments between the query sequence and sequences in the database. Common scoring systems include the substitution matrix (e.g., BLOSUM or PAM matrices) for protein sequences and scoring schemes like match/mismatch scores and gap penalties for nucleotide sequences.  </li> <li>Seed Search: BLAST starts by identifying short, exact matches (seeds) between the query sequence and sequences in the database. These seeds serve as starting points for potential alignments.  </li> <li>Extension: Once seeds are identified, BLAST extends these seed matches in both directions along the sequences, using dynamic programming techniques to find the optimal alignment that maximizes the alignment score. The algorithm also considers gap penalties to account for insertions and deletions.  </li> <li>Scoring and Filtering: BLAST scores the extended alignments based on the chosen scoring system. It then applies a statistical significance threshold (usually based on E-values) to filter out alignments that could occur by chance.  </li> <li>Reporting Hits: BLAST reports the significant alignments, known as \"hits,\" to the user. These hits represent regions of similarity between the query sequence and sequences in the database.  </li> </ol> <p>BLAST is highly configurable, allowing users to adjust parameters such as the scoring system, gap penalties, and statistical significance thresholds to customize the search based on their specific needs. There are different versions of BLAST, including BLASTn (for nucleotide sequences), BLASTp (for protein sequences), BLASTx (translating nucleotide sequences to proteins), and more, each tailored to different types of sequence data.  </p> <p>Overall, BLAST is an essential tool for sequence comparison and is widely used in genomics, proteomics, and other areas of biological research. It enables researchers to quickly and effectively identify similarities between sequences and gain insights into the functional and evolutionary relationships of biological molecules.  </p> <p>Check out the NCBI-blast page for more information (and online applications)</p>"},{"location":"blast/#local-blast","title":"Local Blast","text":"<p>For more info take a look at the Download section @ NCBI website.</p>"},{"location":"blast/#installation","title":"Installation","text":"<p>Running blast form the webinterface has it's limitations. If you want to include your blast analysis in your own pipelines, Blast+ is the way to go. You can download the Blast+ executables and install them on your own machine.  </p> <ol> <li>Download the latest executable</li> <li>Unpack the download file: <pre><code>tar xzvf yourfile.tar.gz\n</code></pre> NOTE: It is always good pracktice to install bioinformatic software (if installed from source) to a designated bioinfo folder i.e. /usr/local/bionfo.           From here you can create a symlink to a folder in your $PATH f.i. /usr/local/bin</li> </ol>"},{"location":"blast/#database-download","title":"Database Download","text":"<p>BLAST databases are updated daily and may be downloaded via FTP. Database sets may be retrieved automatically with update_blastdb.pl, which is part of the BLAST+ suite. (run this command in working directory, probably best in same directory at your $PATH of the database.)</p> <p>Please refer to the BLAST database documentation for more details.</p> <ol> <li>Pre-fromatted BLAST db are archived here</li> <li>Download all numbered files for your database of choice using the basename (eg nt.000.tar.gz =&gt; \"nt\"):       Each of these files represents a subset (volume) of that database,       and all of them are needed to reconstitute the database.</li> <li>Extract: <pre><code>for file in *.gz; do tar -xzvf \"$file\"; done\n</code></pre></li> <li>After extraction, there is no need to concatenate the resulting files:       Call the database with the base name, for nr database files, use \"-db nt\".</li> </ol> <p>NOTE: For easy download, use the update_blastdb.pl script from the blast+ package.      Run following command in your db folder: <pre><code>perl update_blastdb.pl --decompress nt # or any other prefix for you db of coice\n</code></pre> NOTE: if the ncbi server runs slow, the update command could rise connection issues.   Sometimes it is easier to download manually from the ftp db website: <pre><code>wget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz\nwget https://ftp.ncbi.nlm.nih.gov/blast/db/nt.???.tar.gz.md5\n</code></pre></p>"},{"location":"blast/#build-local-db","title":"Build Local DB","text":"<p>If you want to build your own database based on fasta sequences take a look here for more details. Example<pre><code>$ makeblastdb -in test.fsa -parse_seqids -blastdb_version 5 -taxid_map test_map.txt -title \"Cookbook demo\" -dbtype prot\n\n\nBuilding a new DB, current time: 02/06/2019 17:08:14\nNew DB name:   test.fsa\nNew DB title:  Cookbook demo\nSequence type: Protein\nKeep MBits: T\nMaximum file size: 1000000000B\nAdding sequences from FASTA; added 6 sequences in 0.00222588 seconds.\n$\n</code></pre></p>"},{"location":"blast/#configuration","title":"Configuration","text":"<p>To set the env for blastn you can make a configuration file named .ncbirc (on Unix-like platforms) or ncbi.ini (on Windows) in your home directory  </p> <pre><code>touch ~/.ncbirc &amp;&amp;\necho [BLAST] &gt;&gt; ~/.ncbirc\necho [BLASTDB]=/path/to/databases &gt;&gt; ~/.ncbirc\n</code></pre> <p>More available parameters can be set in the config file. More detailed info on configurateion can be found here </p>"},{"location":"blast/#command-line","title":"Command line","text":""},{"location":"blast/#quick-run","title":"Quick run","text":"<pre><code>blastn \u2013db nt \u2013query nt.fsa \u2013out results.out\n</code></pre>"},{"location":"blast/#personalized-outputformat","title":"Personalized outputformat","text":"<pre><code>blastn -db nt \\ # nt is the name (prefix) you gave your db\n  -query /path/to/fastafiles \\\n  -out /path/to/output/out \\\n    -max_target_seqs 5 \\\n  -outfmt \"\"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore staxids sscinames sskingdoms qcovs\" \\\n  -num_trheads $(THREADS)\n</code></pre> <ul> <li> <p><code>1. qseqid</code>: query or source (gene) sequence id</p> </li> <li> <p><code>2. sseqid</code>: subject or target (reference genome) sequence id</p> </li> <li> <p><code>3. pident</code>: percentage of identical positions</p> </li> <li> <p><code>4. length</code>: alignment length (sequence overlap)</p> </li> <li> <p><code>5. mismatch</code>: number of mismatches</p> </li> <li> <p><code>6. gapopen</code>: number of gap openings</p> </li> <li> <p><code>7. qstart</code>: start of alignment in query</p> </li> <li> <p><code>8. qend</code>: end of alignment in query</p> </li> <li> <p><code>9. sstart</code>: start of alignment in subject</p> </li> <li> <p><code>10. send</code>: end of alignment in subject</p> </li> <li> <p><code>11. evalue</code>: expect value</p> </li> <li> <p><code>12. bitscore</code>: bit score</p> </li> </ul>"},{"location":"blast/#setup-blast-hpc-ugent","title":"Setup Blast @ HPC-UGent","text":"<p>Blast modules are available on the HPC.  Databases on the other hand are installed 'localy. ALERT!: don't install databases locally, they take up a lot of space. Please contact Pieter if you need a database which is not installed yet.</p>"},{"location":"blast/#db-location","title":"DB location","text":"<p>A dedicated folder is available for our VO where we can 'locally' make pre installed databases available for us all.  If you need another database, please avoid installing it into your own account folder, it will take up a lot of space and others who are part of our VO cannot access.  </p> <p>Location: /data/gent/vo/001/gvo00142/data_share_group/databases_blast/  Configuration db: Create a ncbirc file (see above for instructions) and save in your home folder (/user/gent/433/yourvscnumber)  </p> <p>Alternative: add this line to your .bashrc file:  </p> <pre><code>export BLASTDB=/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre>"},{"location":"blast/#updates","title":"Updates","text":"<p>Instllation dates are always part of the folder where you can find the databasefiles, if you feel it is not up to date, give a shout out to Pieter or make an update yourself if you know how to. When you need another type of db wchich you frequently use and others might benefit from, ask for a local installation.  </p>"},{"location":"blast/#usage","title":"Usage","text":"<ol> <li>Make sure your env value for $BLASTDB is set correctly. <pre><code>vsc43352@node3206:~$echo $BLASTDB                                                    \n/data/gent/vo/001/gvo00142/data_share_group/databases_blast/\n</code></pre></li> <li>Use ml spider blast to list the installed blast modules</li> <li>Use the latest module installed in your jobscript</li> <li>run blast following the blast module (for instpiration look to section Quick run above)      or RTFM.</li> </ol>"},{"location":"github/","title":"Github","text":"<p>For a more in depth overview go to the official documentation.  </p>"},{"location":"github/#1-setting-up-a-local-git-repository","title":"1. Setting Up a Local Git Repository","text":"<ol> <li> <p>Initialize a new Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\n</code></pre> <ul> <li><code>cd /path/to/your/project</code>: Change directory to your project folder.</li> <li><code>git init</code>: Initialize a new Git repository in the current directory.</li> </ul> </li> <li> <p>Add files to the repository:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all files in the current directory for the next commit.</li> </ul> </li> <li> <p>Commit the files:</p> <pre><code>git commit -m \"Initial commit\"\n</code></pre> <ul> <li><code>git commit -m \"Initial commit\"</code>: Commit the staged files with a message.</li> </ul> </li> </ol>"},{"location":"github/#2-authenticating-using-ssh-key","title":"2. Authenticating Using SSH Key","text":"<ol> <li> <p>Generate an SSH key pair (if you don't have one):</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <ul> <li><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"</code>: Generate a new SSH key with RSA encryption.</li> </ul> </li> <li> <p>Add the SSH key to your SSH agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <ul> <li><code>eval \"$(ssh-agent -s)\"</code>: Start the SSH agent.</li> <li><code>ssh-add ~/.ssh/id_rsa</code>: Add your SSH private key to the SSH agent.</li> </ul> </li> <li> <p>Add the SSH key to your GitHub/GitLab/Bitbucket account:</p> <ul> <li> <p>Copy the SSH key to your clipboard:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> </li> <li> <p>Go to your Git hosting service and add the SSH key to your account settings.</p> </li> </ul> </li> </ol>"},{"location":"github/#3-pushing-the-local-repository-to-a-remote-repository","title":"3. Pushing the Local Repository to a Remote Repository","text":"<ol> <li> <p>Add the remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\n</code></pre> <ul> <li><code>git remote add origin git@github.com:username/repo.git</code>: Add a remote repository with the alias <code>origin</code>.</li> </ul> </li> <li> <p>Push the local repository to the remote repository:</p> <pre><code>git push -u origin main\n</code></pre> <ul> <li><code>git push -u origin main</code>: Push the local <code>main</code> branch to the <code>origin</code> remote repository and set it as the default upstream branch.</li> </ul> </li> </ol>"},{"location":"github/#4-making-changes-and-syncing-them","title":"4. Making Changes and Syncing Them","text":"<ol> <li> <p>Make changes to your files.</p> </li> <li> <p>Stage the changes:</p> <pre><code>git add .\n</code></pre> <ul> <li><code>git add .</code>: Stage all changed files for the next commit.</li> </ul> </li> <li> <p>Commit the changes:</p> <pre><code>git commit -m \"Describe your changes\"\n</code></pre> <ul> <li><code>git commit -m \"Describe your changes\"</code>: Commit the staged changes with a descriptive message.</li> </ul> </li> <li> <p>Push the changes to the remote repository:</p> <pre><code>git push\n</code></pre> <ul> <li><code>git push</code>: Push the committed changes to the remote repository.</li> </ul> </li> </ol>"},{"location":"github/#5-keeping-your-local-repository-in-sync-with-the-remote","title":"5. Keeping Your Local Repository in Sync with the Remote","text":"<ol> <li> <p>Fetch the latest changes from the remote repository:</p> <pre><code>git fetch origin\n</code></pre> <ul> <li><code>git fetch origin</code>: Fetch the latest changes from the <code>origin</code> remote repository.</li> </ul> </li> <li> <p>Merge the fetched changes into your local branch:</p> <pre><code>git merge origin/main\n</code></pre> <ul> <li><code>git merge origin/main</code>: Merge the fetched <code>main</code> branch into your local <code>main</code> branch.</li> </ul> </li> <li> <p>Pull the latest changes (fetch + merge):</p> <pre><code>git pull\n</code></pre> <ul> <li><code>git pull</code>: Fetch and merge the latest changes from the remote repository into your local branch.</li> </ul> </li> </ol>"},{"location":"github/#full-command-summary","title":"Full Command Summary","text":"<ol> <li> <p>Initialize Git repository:</p> <pre><code>cd /path/to/your/project\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> </li> <li> <p>Generate and add SSH key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub\n# Add the key to your Git hosting account settings\n</code></pre> </li> <li> <p>Add and push to remote repository:</p> <pre><code>git remote add origin git@github.com:username/repo.git\ngit push -u origin main\n</code></pre> </li> <li> <p>Make changes and push:</p> <pre><code># Make changes to your files\ngit add .\ngit commit -m \"Describe your changes\"\ngit push\n</code></pre> </li> <li> <p>Sync with remote repository:</p> <pre><code>git pull\n</code></pre> </li> </ol> <p>By following these steps and using the commands provided, you can effectively manage your local and remote Git repositories.</p>"},{"location":"installation/","title":"Installation","text":"<p>EPI2ME Labs maintains a collection of bioinformatics workflows tailored to Oxford Nanopore Technologies long-read sequencing data. They are curated and actively maintained by experts in long-read sequence analysis. Read all about EPI2ME workflows here.  </p>"},{"location":"installation/#installation-guide","title":"Installation guide","text":""},{"location":"installation/#java","title":"JAVA","text":"<pre><code>sudo apt-get update\nsudo apt-get install default-jre #install java on ubuntu\n</code></pre>"},{"location":"installation/#nextflow","title":"NEXTFLOW","text":"<p>Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages.  </p> <p>More information can be found on the NEXTFLOW WEBSITE. Download and install nextflow<pre><code>curl -s https://get.nextflow.io | bash # install nextflow\n</code></pre></p>"},{"location":"installation/#go","title":"GO","text":"<p>Singularity 3.0 is written primarily in Go, and you will need Go installed to compile it from source.  </p> <p>More information can be found on the Singularity installation guide website.  </p> <pre><code># Install\n    export VERSION=1.11 OS=linux ARCH=amd64 &amp;&amp; \\\n    wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz &amp;&amp; \\\n    rm go$VERSION.$OS-$ARCH.tar.gz\n\n# setup GO environment\n    echo 'export GOPATH=${HOME}/go' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    source ~/.bashrc\n\n# install dep for dependency resolution\n    go get -u github.com/golang/dep/cmd/dep\n</code></pre>"},{"location":"installation/#singularity","title":"Singularity","text":"<p>Singularity is a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Singularity on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system and system.  </p> <p>More information can be found on the Singularity introduction page.  </p> <p>Installation <pre><code>export VERSION=3.0.3 &amp;&amp; # adjust this as necessary \\\n    mkdir -p $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    cd $GOPATH/src/github.com/sylabs &amp;&amp; \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz &amp;&amp; \\\n    tar -xzf singularity-${VERSION}.tar.gz &amp;&amp; \\\n    cd ./singularity &amp;&amp; \\\n    ./mconfig\n</code></pre> Compile <pre><code>./mconfig &amp;&amp; \\\n    make -C ./builddir &amp;&amp; \\\n    sudo make -C ./builddir install\n</code></pre> Singularity cache folder </p> <p>Save storage space on your computer. When you run the singularity container (wf-amplicon for instance), a work directory will be created in your project folder. The wf-amplicon uses different containers without you noticing. All container images will be downloaded each time you run this wf. And this takes up space on your HD. </p> <p>To avoid this, you can create a cache folder where singularity can store these .img files and use these instead of downloding them each time you run the wf. The cache foler can then be specified in a nextflow.config file.  OR you can do as the error message expalains is to set the NXF_SINGULARITY_CACHEDI variable in you .bashrc file. Both options will be explained below.  </p> <p>OPTION 1: create nextflow.config file Follow these steps to do just this:  </p> <ol> <li> <p>Run the workflow for the 1st time     Run the workflow for the firs time and don't bother about this warning:</p> <p>Warning</p> <p>If the cache folder is not set you get this warning: WARN: Singularity cache directory has not been defined -- Remote image will be stored in the path: /home/path-to/work/singularity -- Use the environment variable NXF_SINGULARITY_CACHEDIR to specify a different location </p> </li> <li> <p>Locate the .img files     The wf-amplicon will create a <code>./work/singularity</code> folder in your projcet folder.      <pre><code>pieter@UGENT-PC:~/Analyses/wf-amplicon/work\n$ ls\n0f  14  24  2d  3f  46  49  54  5b  63  6d  77  7b  80  85  93  9b  9f  a1  a4  af  b5  bc  cc  d6  e5  e8  singularity\n11  1e  2b  3c  40  47  4c  56  5d  6b  71  78  7f  82  89  96  9c  a0  a3  ac  b0  b6  c8  d0  db  e6  ee\n(base)\npieter@UGENT-PC~/Analyses/wf-amplicon/work\n$ ls singularity/\nontresearch-medaka-sha61a9438d745a78030738352e084445a2db3daa2a.img       ontresearch-wf-common-sha91452ece4f647f62b32dac3a614635a6f0d7f8b5.img\nontresearch-wf-amplicon-sha3b6a08c2ed47efd85b17b98762951fffdeee5ba9.img\n</code></pre>     In this folder you'll find the <code>.img</code> files you need later.  </p> </li> <li> <p>Copy the <code>.img</code> files to a local folder     To avoid this create cache folder to store downloaded images when running epi2me labs. For each new analysis you run with epi2me make sure to put the image in this folder. <pre><code>mkdir /path/to/singularity-img\ncp ~/Analyses/wf-amplicon/work/singularity/*.img /path/to/singularity-img # this will copy all img files to your folder created in previou step, don't forget to personalize the paths  \n</code></pre> Create nextflow config file Now it is time to configure nextflow which is used to stitch all the singularity containers from wf-amplicon together. We will create a file that will be checked running each wf and redirects singularity to find the images stored locally rather than download them each time. More info on nextflow configuration file. Process: <pre><code>mkdir /home/sequencing/nextflow-config # /home/sequencing is in this case an example of your user's home directory. User is called *sequencer* in this example.  \ntouch /home/sequencing/nextflow-config/nextflow.config\n</code></pre> nextflow.config<pre><code>singularity { \n    enabled = true\n    cacheDir = /path/to/singularity-img\n} \n</code></pre></p> </li> </ol> <p>OPTION2: Set the environmental variable  </p> <p>The warning message indicates that the Singularity cache directory has not been set correctly, even though you specified it in your custom Nextflow configuration. To resolve this, you can explicitly set the Singularity cache directory using the <code>NXF_SINGULARITY_CACHEDIR</code> environment variable.</p> <p>Steps to Set the Singularity Cache Directory </p> <ol> <li>Set the Environment Variable:</li> <li>You can set the environment variable in your shell before running Nextflow. For example:      <pre><code>export NXF_SINGULARITY_CACHEDIR=/home/pieter/singularity-img\n</code></pre></li> <li>Set the Environment Variable Permanently (Optional):</li> <li>To make this change permanent, you can add the <code>export</code> command to your shell\u2019s configuration file (e.g., <code>~/.bashrc</code> or <code>~/.bash_profile</code> for bash):      <pre><code>echo \"export NXF_SINGULARITY_CACHEDIR=/home/pieter/singularity-img\" &gt;&gt; ~/.bashrc\n</code></pre></li> <li> <p>Reload your shell configuration:      <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>Verify the Configuration:</p> </li> <li>Run the pipeline again to ensure the warning disappears:      <pre><code>nextflow run epi2me-labs/wf-amplicon --fastq ./fastq --sample_sheet ./sample-sheet.csv --porechop --discard_middle --out_dir output-wf-amlicon-toydata-bis\n</code></pre></li> </ol> <p>This should direct Nextflow to use the specified cache directory for Singularity images, avoiding the warning and ensuring images are stored in your preferred location.</p>"},{"location":"installation/#wf-amplicon","title":"wf-amplicon","text":"<p>This workflow performs the analysis of reads generated from PCR amplicons. After some pre-processing, reads are either aligned to a reference (containing the expected sequence for each amplicon) for variant calling or the amplicon\u2019s consensus sequence is generated de novo.  </p> <p>Installation guides for wf-amplicon can be found here: wf-amplicon Installation guide.  </p>"},{"location":"installation/#run-the-workflow","title":"Run the workflow","text":"<p>data layout<pre><code>(i)                     (ii)                 (iii)\ninput_reads.fastq   \u2500\u2500\u2500 input_directory  \u2500\u2500\u2500 input_directory\n                        \u251c\u2500\u2500 reads0.fastq     \u251c\u2500\u2500 barcode01\n                        \u2514\u2500\u2500 reads1.fastq     \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u2514\u2500\u2500 reads1.fastq\n                                             \u251c\u2500\u2500 barcode02\n                                             \u2502   \u251c\u2500\u2500 reads0.fastq\n                                             \u2502   \u251c\u2500\u2500 reads1.fastq\n                                             \u2502   \u2514\u2500\u2500 reads2.fastq\n                                             \u2514\u2500\u2500 barcode03\n                                              \u2514\u2500\u2500 reads0.fastq\n</code></pre> Command to run<pre><code>nextflow run epi2me-labs/wf-amplicon \\\n    --fastq ./fastq \\\n    --reference ./reference.fa \\\n    --sample_sheet ./sample-sheet.csv\n    -c /home/sequencing/nextflow-config/nextflow.config \\ #this can be omitted if you set the variable in your .bashrc\n    -profile singulairy\n</code></pre> Example: sample-sheet.csv<pre><code>barcode,alias,type,ref\nbarcode41,BC41,test_sample,ref1\nbarcode42,BC42,test_sample\nbarcode43,BC43,test_sample,ref2\n</code></pre> Example: reference.fa<pre><code>&gt;ref1\nATGC\n&gt;ref2\nCGTA\n</code></pre></p> <p>Notes</p> <p>When not running variant mode a reference file is not necessary. Also make sure to delete the ref column in that case.</p>"},{"location":"installation/#links","title":"Links","text":"<ul> <li>Installation EPI2ME</li> <li>Installation Nextflow </li> <li>Installation Singularity </li> <li>Installation Docker</li> <li>Installation GO</li> </ul>"},{"location":"linuxcommands/","title":"Essential Linux Commands","text":""},{"location":"linuxcommands/#file-and-directory-operations","title":"File and Directory Operations","text":""},{"location":"linuxcommands/#listing-and-navigating","title":"Listing and Navigating","text":"<ul> <li><code>ls</code>: List the contents of a directory.   <pre><code>ls /path/to/directory\n</code></pre></li> <li><code>cd</code>: Change the current directory.   <pre><code>cd /path/to/directory\n</code></pre></li> <li><code>pwd</code>: Display the current working directory.   <pre><code>pwd\n</code></pre></li> </ul>"},{"location":"linuxcommands/#creating-and-removing","title":"Creating and Removing","text":"<ul> <li><code>mkdir</code>: Create a new directory.   <pre><code>mkdir new_directory\n</code></pre></li> <li><code>rmdir</code>: Remove an empty directory.   <pre><code>rmdir empty_directory\n</code></pre></li> <li><code>rm</code>: Remove files or directories.   <pre><code>rm file_name   # for files\nrm -r directory_name   # for directories\n</code></pre></li> </ul>"},{"location":"linuxcommands/#copying-and-moving","title":"Copying and Moving","text":"<ul> <li><code>cp</code>: Copy files or directories.   <pre><code>cp /path/to/source/file /path/to/destination/\n</code></pre>   Example:   <pre><code>cp /home/user/file.txt /home/user/Documents/\n</code></pre></li> <li>Copy a directory and its contents:   <pre><code>cp -r /path/to/source/directory /path/to/destination/\n</code></pre>   Example:   <pre><code>cp -r /home/user/folder /home/user/Documents/\n</code></pre></li> <li><code>mv</code>: Move or rename files or directories.   <pre><code>mv /path/to/source/file /path/to/destination/\n</code></pre>   Example:   <pre><code>mv /home/user/file.txt /home/user/Documents/\n</code></pre></li> </ul>"},{"location":"linuxcommands/#file-manipulation","title":"File Manipulation","text":"<ul> <li><code>touch</code>: Create an empty file or update the timestamp of an existing file.   <pre><code>touch file_name\n</code></pre></li> <li><code>cat</code>: Concatenate and display the content of files.   <pre><code>cat file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#advanced-copying-with-rsync","title":"Advanced Copying with <code>rsync</code>","text":"<ul> <li><code>rsync</code>: Synchronize directories and copy large amounts of data efficiently.   <pre><code>rsync -av /path/to/source/ /path/to/destination/\n</code></pre></li> <li><code>-a</code>: Archive mode, preserves permissions, times, symbolic links, etc.</li> <li><code>-v</code>: Verbose, shows the progress of the transfer.</li> </ul>"},{"location":"linuxcommands/#file-permissions-and-ownership","title":"File Permissions and Ownership","text":"<ul> <li><code>chmod</code>: Change the permissions of a file or directory.   <pre><code>chmod 755 file_name\n</code></pre></li> <li><code>chown</code>: Change the ownership of a file or directory.   <pre><code>chown user:group file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#file-viewing-and-editing","title":"File Viewing and Editing","text":"<ul> <li><code>nano</code>/<code>vim</code>: Text editors for creating and editing files directly in the terminal.   <pre><code>nano file_name\nvim file_name\n</code></pre></li> <li><code>less</code>: Display the content of a file one screen at a time.   <pre><code>less file_name\n</code></pre></li> <li><code>head</code>: Show the first few lines of a file.   <pre><code>head -n 10 file_name   # shows the first 10 lines\n</code></pre></li> <li><code>tail</code>: Show the last few lines of a file.   <pre><code>tail -n 10 file_name   # shows the last 10 lines\n</code></pre></li> <li><code>grep</code>: Search for a specific pattern within files.   <pre><code>grep \"pattern\" file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#system-information","title":"System Information","text":"<ul> <li><code>uname</code>: Display basic information about the system.   <pre><code>uname -a   # shows all system information\n</code></pre></li> <li><code>df</code>: Show disk space usage.   <pre><code>df -h   # human-readable format\n</code></pre></li> <li><code>du</code>: Display the disk usage of files and directories.   <pre><code>du -sh directory_name\n</code></pre></li> <li><code>free</code>: Show the amount of free and used memory in the system.   <pre><code>free -h\n</code></pre></li> <li><code>top</code>: Display real-time system processes and resource usage.   <pre><code>top\n</code></pre></li> <li><code>ps</code>: Display the currently running processes.   <pre><code>ps aux\n</code></pre></li> </ul>"},{"location":"linuxcommands/#network-operations","title":"Network Operations","text":"<ul> <li><code>ping</code>: Test connectivity to a network host.   <pre><code>ping google.com\n</code></pre></li> <li><code>ifconfig</code>: Display or configure network interfaces.   <pre><code>ifconfig   # use `ip addr` in newer systems\n</code></pre></li> <li><code>netstat</code>: Show network connections, routing tables, and more.   <pre><code>netstat -tuln   # shows listening ports\n</code></pre></li> <li><code>ssh</code>: Connect to another machine securely over the network.   <pre><code>ssh user@hostname\n</code></pre></li> </ul>"},{"location":"linuxcommands/#process-management","title":"Process Management","text":"<ul> <li><code>kill</code>: Send a signal to terminate a process.   <pre><code>kill PID   # where PID is the process ID\n</code></pre></li> <li><code>killall</code>: Terminate all processes by name.   <pre><code>killall process_name\n</code></pre></li> <li><code>bg</code>: Resume a suspended job in the background.   <pre><code>bg job_number\n</code></pre></li> <li><code>fg</code>: Bring a job to the foreground.   <pre><code>fg job_number\n</code></pre></li> </ul>"},{"location":"linuxcommands/#user-and-group-management","title":"User and Group Management","text":"<ul> <li><code>adduser</code>/<code>useradd</code>: Add a new user to the system.   <pre><code>adduser username\nuseradd username\n</code></pre></li> <li><code>passwd</code>: Change a user\u2019s password.   <pre><code>passwd username\n</code></pre></li> <li><code>su</code>: Switch to another user account.   <pre><code>su - username\n</code></pre></li> <li><code>sudo</code>: Execute a command with superuser privileges.   <pre><code>sudo command\n</code></pre></li> </ul>"},{"location":"linuxcommands/#package-management-example-debian-based-systems","title":"Package Management (Example: Debian-based systems)","text":"<ul> <li><code>apt-get update</code>: Update the package lists.   <pre><code>sudo apt-get update\n</code></pre></li> <li><code>apt-get install</code>: Install a new package.   <pre><code>sudo apt-get install package_name\n</code></pre></li> <li><code>apt-get upgrade</code>: Upgrade all installed packages.   <pre><code>sudo apt-get upgrade\n</code></pre></li> </ul>"},{"location":"linuxcommands/#text-manipulation","title":"Text Manipulation","text":"<ul> <li><code>echo</code>: Display a line of text.   <pre><code>echo \"Hello, World!\"\n</code></pre></li> <li><code>grep</code>: Search for text patterns in files.   <pre><code>grep \"text\" file_name\n</code></pre></li> <li><code>sed</code>: Stream editor for filtering and transforming text.   <pre><code>sed 's/old/new/' file_name\n</code></pre></li> <li><code>awk</code>: A powerful text processing language.   <pre><code>awk '{print $1}' file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#expanded-section-on-text-manipulation-for-fasta-fastq-and-bam-files","title":"Expanded Section on Text Manipulation for FASTA, FASTQ, and BAM Files","text":"<p>When working with bioinformatics data formats like FASTA, FASTQ, and BAM files, text manipulation tools like <code>echo</code>, <code>grep</code>, <code>sed</code>, and <code>awk</code> become essential. Below are examples and explanations of how these tools can be used effectively.</p>"},{"location":"linuxcommands/#fasta-files","title":"FASTA Files","text":"<p>FASTA files store nucleotide or protein sequences with a header line starting with <code>&gt;</code> followed by sequence data. </p> <p>Examples: </p> <ul> <li> <p><code>grep</code>: Extract sequence identifiers (headers) from a FASTA file.   <pre><code>grep \"^&gt;\" sequences.fasta\n</code></pre>   This command searches for lines starting with <code>&gt;</code> and outputs them, giving you all the headers in the file.</p> </li> <li> <p><code>awk</code>: Count the number of sequences in a FASTA file.   <pre><code>awk '/^&gt;/ {count++} END {print count}' sequences.fasta\n</code></pre>   This <code>awk</code> script increments a counter for each header line and prints the total number of sequences at the end.</p> </li> <li> <p><code>sed</code>: Remove description from headers in a FASTA file.   <pre><code>sed 's/\\s.*$//' sequences.fasta &gt; cleaned_sequences.fasta\n</code></pre>   This command keeps only the identifier in each header line by deleting everything after the first space.</p> </li> </ul>"},{"location":"linuxcommands/#fastq-files","title":"FASTQ Files","text":"<p>FASTQ files contain sequence data with quality scores, commonly used in next-generation sequencing. </p> <p>Examples: </p> <ul> <li> <p><code>grep</code>: Extract all sequence headers (lines starting with <code>@</code>).   <pre><code>grep \"^@\" sequences.fastq\n</code></pre>   This command will output all sequence headers in a FASTQ file.</p> </li> <li> <p><code>awk</code>: Calculate the total number of reads in a FASTQ file.   <pre><code>awk '{s++} END {print s/4}' sequences.fastq\n</code></pre>   Since each read spans four lines (header, sequence, plus, quality), dividing the total line count by four gives the number of reads.</p> </li> <li> <p><code>sed</code>: Convert sequence headers to uppercase (useful if headers have mixed cases).   <pre><code>sed -e '1~4s/.*/\\U&amp;/' sequences.fastq &gt; uppercased_headers.fastq\n</code></pre>   This command converts the headers (every 4th line, starting from line 1) to uppercase.</p> </li> </ul>"},{"location":"linuxcommands/#bam-files","title":"BAM Files","text":"<p>BAM files are binary versions of SAM files and store aligned sequence data. While binary, text manipulation often applies to the SAM format (BAM converted to SAM via <code>samtools view</code>).</p> <p>Examples: </p> <ul> <li> <p><code>samtools</code> &amp; <code>awk</code>: Extract read names and count the total number of unique reads.   <pre><code>samtools view -h file.bam | awk '{if($1 !~ /^@/) print $1}' | sort | uniq | wc -l\n</code></pre>   This command extracts the read names from a BAM file, sorts them, filters out duplicates, and counts the unique entries.</p> </li> <li> <p><code>grep</code>: Filter reads mapped to a specific chromosome.   <pre><code>samtools view file.bam | grep \"^chr1\" &gt; chr1_reads.sam\n</code></pre>   This filters out reads that are mapped to chromosome 1.</p> </li> <li> <p><code>awk</code>: Extract and count reads with a specific mapping quality.   <pre><code>samtools view file.bam | awk '$5 &gt;= 30' | wc -l\n</code></pre>   This command counts the number of reads with a mapping quality of 30 or higher.</p> </li> </ul>"},{"location":"linuxcommands/#combining-tools-for-advanced-manipulation","title":"Combining Tools for Advanced Manipulation","text":"<p>Combining <code>grep</code>, <code>awk</code>, and <code>sed</code> can provide powerful data extraction and manipulation. Here\u2019s an example:</p> <ul> <li>Extracting and summarizing GC content from FASTA sequences:   <pre><code>awk '/^&gt;/ {if (seqlen){print gc/seqlen*100}; print; gc=0; seqlen=0; next} {gc+=gsub(/[GgCc]/,\"\"); seqlen+=length($0)} END {print gc/seqlen*100}' sequences.fasta\n</code></pre>   This script calculates the GC content for each sequence and prints it after each sequence header.</li> </ul> <p>By mastering these commands, you can efficiently manipulate and analyze bioinformatics data files. Each tool offers specific advantages depending on the task, allowing for streamlined data processing in a command-line environment.</p>"},{"location":"linuxcommands/#archiving-and-compression","title":"Archiving and Compression","text":""},{"location":"linuxcommands/#tar","title":"<code>tar</code>","text":"<p>Archive files into a tarball, which can be compressed using various algorithms.   - Create a tarball:      <pre><code>tar -cvf archive_name.tar directory_name\n</code></pre>   - Create a compressed tarball with gzip:      <pre><code>tar -czvf archive_name.tar.gz directory_name\n</code></pre>   - Create a compressed tarball with bzip2:      <pre><code>tar -cjvf archive_name.tar.bz2 directory_name\n</code></pre>   - Create a compressed tarball with xz:      <pre><code>tar -cJvf archive_name.tar.xz directory_name\n</code></pre>   - Extract a tarball:      <pre><code>tar -xvf archive_name.tar\n</code></pre>   - Extract a gzip-compressed tarball:      <pre><code>tar -xzvf archive_name.tar.gz\n</code></pre>   - Extract a bzip2-compressed tarball:      <pre><code>tar -xjvf archive_name.tar.bz2\n</code></pre>   - Extract an xz-compressed tarball:      <pre><code>tar -xJvf archive_name.tar.xz\n</code></pre></p>"},{"location":"linuxcommands/#gzip","title":"<code>gzip</code>","text":"<p>Compress files using the gzip algorithm.   - Compress a file:      <pre><code>gzip file_name\n</code></pre>   - Decompress a file:      <pre><code>gunzip file_name.gz\n</code></pre>   - Compress a file while keeping the original:      <pre><code>gzip -c file_name &gt; file_name.gz\n</code></pre>   - List the contents of a gzip-compressed file:      <pre><code>zcat file_name.gz\n</code></pre></p>"},{"location":"linuxcommands/#bzip2","title":"<code>bzip2</code>","text":"<p>Compress files using the bzip2 algorithm, which typically achieves better compression than gzip.   - Compress a file:      <pre><code>bzip2 file_name\n</code></pre>   - Decompress a file:      <pre><code>bunzip2 file_name.bz2\n</code></pre>   - Compress a file while keeping the original:      <pre><code>bzip2 -c file_name &gt; file_name.bz2\n</code></pre>   - List the contents of a bzip2-compressed file:      <pre><code>bzcat file_name.bz2\n</code></pre></p>"},{"location":"linuxcommands/#zip","title":"<code>zip</code>","text":"<p>Create a ZIP archive, which can include multiple files and directories.   - Create a ZIP archive:      <pre><code>zip archive_name.zip file1 file2 directory_name\n</code></pre>   - Add files to an existing ZIP archive:      <pre><code>zip archive_name.zip newfile\n</code></pre>   - Extract a ZIP archive:      <pre><code>unzip archive_name.zip\n</code></pre>   - List the contents of a ZIP archive:      <pre><code>unzip -l archive_name.zip\n</code></pre></p>"},{"location":"linuxcommands/#7z","title":"<code>7z</code>","text":"<p>Use the 7-Zip compression tool for high compression ratios and support for many formats.   - Create a 7z archive:      <pre><code>7z a archive_name.7z file1 file2 directory_name\n</code></pre>   - Extract a 7z archive:      <pre><code>7z x archive_name.7z\n</code></pre>   - List the contents of a 7z archive:      <pre><code>7z l archive_name.7z\n</code></pre></p> <p>These tools and commands cover a broad range of archiving and compression needs, from simple file compression to handling more complex multi-file archives.</p>"},{"location":"linuxcommands/#finding-files","title":"Finding Files","text":"<ul> <li><code>find</code>: Search for files in a directory hierarchy.   <pre><code>find /path -name \"file_name\"\n</code></pre></li> <li><code>locate</code>: Quickly find files by name using an indexed database.   <pre><code>locate file_name\n</code></pre></li> </ul>"},{"location":"linuxcommands/#system-monitoring-and-performance","title":"System Monitoring and Performance","text":"<ul> <li><code>htop</code>: Interactive process viewer (more user-friendly than <code>top</code>).   <pre><code>htop\n</code></pre></li> <li><code>vmstat</code>: Report virtual memory statistics.   <pre><code>vmstat 1   # refreshes every second\n</code></pre></li> <li><code>iostat</code>: Report CPU and I/O statistics.   <pre><code>iostat\n</code></pre></li> </ul>"},{"location":"linuxtutorial/","title":"Linuxtutorial","text":"<p>All credits for this HANDS-ON go to Mag Selwa from KULeuven. Check out the whole repo including slides.</p>"},{"location":"linuxtutorial/#hands-on-1","title":"HANDS-ON 1","text":"Task Hint 1. Open the VM and play Linux (Ubuntu) Show Hint Check the Applications menu. Run the command line (Show applications -&gt; Terminal). 2. Identify if you are a normal user or a superuser Show Hint Try to switch to root (superuser) and report what happens. <code>whoami</code>, <code>su</code> 3. Identify your shell Show Hint <code>echo $SHELL</code> 4. Check the kernel version, system distribution, and glibc version Show Hint <code>uname -r</code>, <code>uname -a</code>, <code>ls -l /lib/i386-linux-gnu/libc.so*</code>"},{"location":"linuxtutorial/#hands-on-2","title":"HANDS-ON 2","text":"Task Hint 1. Start Terminal and display the list of your files and directories Show Hint <code>ls</code> 2. Check more options about the <code>ls</code> command and display all files Show Hint <code>ls --help</code>, <code>man ls</code>, <code>info ls</code>, <code>ls -a</code> 3. Clear the screen Show Hint <code>clear</code> 4. Compare the information given by different kinds of help about <code>pwd</code> command Show Hint <code>whatis pwd</code>, <code>help pwd</code>, <code>man pwd</code>, <code>info pwd</code> 5. Navigate directories: Downloads, Desktop, and back to home Show Hint <code>cd Downloads</code>, <code>ls</code>, <code>cd ../Desktop</code>, <code>ls</code>, <code>cd ../Downloads</code>, <code>cd ~</code>, <code>cd</code> 6. Review a few used commands with arrow and compare with output from history Show Hint <code>history</code> 7. Print the date on the screen and save it to a file Show Hint <code>date</code>, <code>date &gt; date.txt</code>, <code>date &gt;&gt; date.txt</code> 8. Exit the shell Show Hint <code>exit</code> 9. Start terminal again and perform directory operations Show Hint <code>pwd</code>, <code>cd Desktop</code>, <code>pwd</code>, <code>cd ..</code>, <code>pwd</code>, <code>cd /tmp</code>, <code>pwd</code>, <code>cd /home/student</code>, <code>ls /</code> 10. Display more information about <code>date.txt</code> file Show Hint <code>ls -l d*</code>, <code>stat date.txt</code>"},{"location":"linuxtutorial/#hands-on-3","title":"HANDS-ON 3","text":"Task Hint 1. Create and manipulate directories and files Show Hint <code>mkdir CourseLinux</code>, <code>cp -r -v -i /usr/share/icons/ ./CourseLinux</code>, <code>mkdir CourseLinux/test</code>, <code>mkdir CourseLinux/test1</code>, <code>mv CourseLinux/test1 CourseLinux/test2</code>, <code>cp -r -v -i /usr/share/icons/* CourseLinux/test</code>, <code>mv CourseLinux/test/unity-icon-theme/apps/128/photos.svg CourseLinux/test/fl.svg</code>, <code>cp -v -i CourseLinux/test/fl.svg fl2.svg</code>, <code>display fl2.svg</code>, <code>ln -s CourseLinux/test/fl.svg mylink2file</code>, <code>display mylink2file</code>, <code>rm -i CourseLinux/test/fl.svg</code>, <code>display mylink2file</code> 2. Clear the screen and perform file operations Show Hint <code>clear</code>, <code>cd CourseLinux</code>, <code>wget https://raw.githubusercontent.com/hpculeuven/Linux-intro/master/tabel.dat</code>, <code>wget https://raw.githubusercontent.com/hpculeuven/Linux-intro/master/matstats.log</code>, <code>cat tabel.dat</code>, <code>less matstats.log</code>, <code>tail -30 matstats.log</code>, <code>head matstats.log</code>, <code>grep ardaa matstats.log</code>, <code>grep ardbb matstats.log</code>, <code>ls -al | less</code>, <code>ls -al | grep tabel.dat</code> 3. Create a file called <code>test4edit</code> and edit it Show Hint <code>touch test4edit</code>, <code>gedit test4edit</code>, <code>nano test4edit</code>, <code>vi test4edit</code>"},{"location":"linuxtutorial/#hands-on-4","title":"HANDS-ON 4","text":"Task Hint 1. Show the path of gcc command and perform various operations Show Hint <code>which gcc</code>, <code>whereis gcc | grep lib</code>, <code>whoami</code>, <code>echo \"I like Linux\" | tr -d 'I' | tr a-z A-Z</code>, <code>bc</code>, <code>date &gt; date.txt</code>, <code>date &gt;&gt; date.txt</code>, <code>cp date.txt date1.txt</code>, <code>echo \"I like Linux\" &gt;&gt; date1.txt</code>, <code>echo \"And I do not\" &gt;&gt; date.txt</code>, <code>diff date.txt date1.txt</code>, <code>du -kah CourseLinux</code>, <code>wc date.txt</code> 2. Clear the screen, archive and unpack directories Show Hint <code>clear</code>, <code>cd ~/CourseLinux</code>, <code>ls ico*</code>, <code>tar -cvf ico.tar icons/</code>, <code>tar -czvf ico.tar.gz icons/</code>, <code>mkdir newtest</code>, <code>cd newtest</code>, <code>cp ../ico.tar.gz .</code>, <code>tar -xzvf ico.tar.gz</code> 3. Work with permissions and directory contents Show Hint <code>cd ~/CourseLinux</code>, <code>mkdir testfiles</code>, <code>cd testfiles</code>, <code>touch file1</code>, <code>gedit file1</code>, <code>cp file1 file2</code>, <code>chmod u-w file2</code>, <code>ls -la</code>, <code>gedit file2</code>, <code>cd ..</code>, <code>chmod u-w testfiles</code>, <code>ls -la</code>, <code>cp testfiles/file1 testfiles/file3</code>, <code>chmod o-rwx testfiles</code>, <code>ls -la</code>, <code>ls testfiles</code>, <code>chmod u-r testfiles</code>, <code>ls -la</code>, <code>cd testfiles</code>, <code>cd ..</code>, <code>chmod u-x testfiles</code>, <code>ls -la</code>, <code>cd testfiles</code> 4. Manage processes and run applications Show Hint <code>clear</code>, <code>ps -aux</code>, <code>gedit</code>, <code>ps -u student | grep gedit</code>, <code>kill &lt;pid&gt;</code>, <code>gedit &amp;</code>"},{"location":"linuxtutorial/#summary","title":"Summary","text":"<ul> <li>All the Hint sections have been updated with <code>&lt;details&gt;</code> and <code>&lt;summary&gt;</code> tags for collapsible content.</li> <li>Users can click on \"Show Hint\" to reveal the hint and click again to collapse it.</li> </ul> <p>Make sure to rebuild your MkDocs project after updating the Markdown file to see the changes.</p>"},{"location":"nanopore-tools/","title":"Nanopore Tools","text":"<p>All nanopore proprietary software can be found in the software &amp; Download section on the website of nanopore.   </p>"},{"location":"nanopore-tools/#sequencing-minkow","title":"Sequencing: Minkow","text":"<p>The basic setup for sequencing in the software package called <code>Minknow</code>. Minknow can both operate the sequencing device as well as running real time basecalling. The installation guide provided in the the software &amp; Download section gives you an overview of all necessary commands to issue for installing the software.  </p>"},{"location":"nanopore-tools/#default-installation-directories","title":"Default installation directories","text":"<ol> <li>For the MinKNOW software: <code>/opt/ont/minknow</code> </li> <li>For the MinKNOW user interface: <code>/opt/ont/minknow-ui</code> </li> <li>Location of the reads folder: <code>/var/lib/minknow/data</code> </li> <li>Location of the log files: The MinKNOW logs are located in <code>/var/log/minknow</code> The Dorado basecaller logs are located in <code>/var/log/dorado</code> </li> </ol>"},{"location":"nanopore-tools/#run-minknow-as-root","title":"Run Minknow as root","text":"<p>When running minknow you'll notice that when issuing another path for your reads folder, minkow will issue an error. Redirecting output from the standard <code>/var/lib/minknow/data</code> might be necessary when you run out of diskspace or simply when you prefer to output your data elsewhere. To enable this, you'll need to give the <code>minknow.service</code> writing privileges i.e. make this root. Make minknow.service root<pre><code>        $ sudo service minknow stop\n        $ sudo perl -i -pe 's/(User|Group)=minknow/$1=root/'/lib/systemd/system/minknow.service\n        $ sudo systemctl daemon-reload\n        $ sudo service minknow start\n</code></pre></p>"},{"location":"nanopore-tools/#basecalling-dorado","title":"Basecalling: Dorado","text":"<p>Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads. The tool can be installed as a stand alone tool which provides more functionalities than basecalling in Minknow. You can find the Dorado github repo here.  </p>"},{"location":"nanopore-tools/#dorado-features","title":"Dorado features","text":"<ol> <li>Download model modules are not packed in the software <pre><code>dordado download --list\n</code></pre></li> <li>Basecaller <pre><code>dorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ &gt; output.bam\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-fastq &gt; output.fastq\ndorado basecaller dna_r10.4.1_e82_400bps_fast@v4.1.0 ./input_pod5/ --emit-sam &gt; output.sam\n</code></pre></li> <li>Aligner <pre><code>dorado basecaller -x cuda:0 --reference .your-reference.fasta dna_r10.4.1_e8_400bps_fast@v4.1.0 ./pod5 &gt; aligned.bam\n</code></pre></li> <li>Sequencing summary generation <pre><code>dorado summary ./reads.bam &gt; sequencing_summary.txt\n</code></pre></li> </ol>"},{"location":"system-build/","title":"System setup","text":""},{"location":"system-build/#operating-system","title":"Operating system","text":"<p>Release: Ubuntu 22.04.4 LTS (Jammy Jellyfish) Follow this link to learn all you need to know on how to install ubuntu.</p>"},{"location":"system-build/#gpu-cuda","title":"GPU &amp; CUDA","text":"<p>GPU computational power increased ONT basecalling speed and enabled real-time data analysis. Upon writing this documentation dorado 0.7 is the current basecaller which is also incorperated in the sequencing operating software Minknow. Before we get to the installation of dedicated sequencing software and other data analys packages we will focus on installing a GPU and the CUDA toolkit.  </p>"},{"location":"system-build/#installation-on-ubuntu","title":"Installation on Ubuntu","text":"<ul> <li>Add graphics drivers ppa repo <pre><code>sudo add-apt-repository ppa:graphics-drivers/ppa\n</code></pre></li> <li>Install Ubuntu drivers app <pre><code>sudo apt install ubuntu-drivers-common\n</code></pre></li> <li>Check available GPUs <pre><code>ubuntu-drivers devices\n</code></pre> Example output<pre><code>== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==\nmodalias : pci:v000010DEd000027E0sv00001025sd0000166Cbc03sc00i00\nvendor   : NVIDIA Corporation\ndriver   : nvidia-driver-545-open - distro non-free\ndriver   : nvidia-driver-555-open - third-party non-free\ndriver   : nvidia-driver-550 - third-party non-free\ndriver   : nvidia-driver-550-open - third-party non-free\ndriver   : nvidia-driver-545 - distro non-free\ndriver   : nvidia-driver-535 - third-party non-free\ndriver   : nvidia-driver-555 - third-party non-free recommended\ndriver   : nvidia-driver-535-server - distro non-free\ndriver   : nvidia-driver-535-open - distro non-free\ndriver   : nvidia-driver-535-server-open - distro non-free\ndriver   : nvidia-driver-525 - third-party non-free\ndriver   : xserver-xorg-video-nouveau - distro free builtin\n</code></pre></li> <li>Install latest Nvidia driver (change <code>555</code> with latest version available in your case) <pre><code>sudo apt install nvidia-driver-525\n</code></pre></li> <li>Reboot your computer  </li> <li>Check if Nvidia driver and CUDA is available after reboot <pre><code>nvidia-smi\n</code></pre> Example output<pre><code>+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4080 ...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   45C    P8               1W /  90W |   1032MiB / 12282MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n</code></pre></li> </ul> <p>This output tells us we have a NVIDIA GeForce RTX 4080 running (12GB virtual RAM) and CUDA Version 12.3 has been co-installed.  </p>"},{"location":"system-build/#purge-nvidia","title":"Purge Nvidia","text":"<p>On some occasions you'll need to perform a fresh installation of nvidia drivers. It is important to purge all currently installed NVIDIA/CUDA before installing anew. Here is a step by step guide on how to do this:  </p> <ul> <li>List all packages The dpkg -l command is used in Debian-based Linux distributions to list all the installed packages. This command provides detailed information about each package, including its status, name, version, and a brief description. <pre><code>dpkg -l\n</code></pre> The output of dpkg -l, the columns represent:  </li> </ul> <ol> <li>Package status (e.g., \"ii\" for installed, \"un\" for uninstalled, etc.)  </li> <li>Package name  </li> <li>Version  </li> <li>Architecture  </li> <li>Description  </li> </ol> <ul> <li>List and purge all NVIDIA/CUDA  </li> </ul> <p><pre><code>sudo apt-get purge $(dpkg -l | grep '^ii.*nvidia'| awk '{print $2}')\n</code></pre> Description<pre><code>dpkg -l | grep '^ii.*nvidia'    # This command lists all installed packages (output of dpkg -l) that have \"nvidia\" in their names and are in the \"installed\" state (lines starting with \"ii\").  \n\nawk '{print $2}'                # This command uses awk to print the second column (package names) from the output of the previous command.  \n\nsudo apt-get purge $(...)       # This command uses command substitution to execute the inner command and pass its output (list of package names) as arguments to apt-get purge, which removes the specified packages.  \n</code></pre></p> <p>.</p>"},{"location":"system-build/#system-software","title":"System Software","text":"<p>On Ubuntu system applications can be installed using the graphical user interface (GUI). More information on how to install software on Ubuntu click here.  </p>"},{"location":"system-build/#bioinformatic-software","title":"Bioinformatic software","text":"<p>Usually bioinformatic software are opensource packages with installation instructions described on Github repositories. First thing you can do is get yourself a github account and get familiarized with github. Executable scripts are often found in the <code>/bin</code>folder, or can be installed in <code>/usr/local/bin</code> or in a dedicated bioinfo folder <code>/usr/local/bioinf/</code>. In the next section I will explain how to install the basecalling tool Dorado in the <code>/bioinf</code>folder and put the script on <code>PATH</code>.  </p> <ul> <li>Goto DORADO GITHUB </li> <li>Download the relevant installer for your platform (like described in the instructions)  </li> <li>Extract the archive to the desired location Extract and put on PATH<pre><code>1. mkdir /usr/local/bioinf/\n2. cd /usr/local/bioinf/\n3. wget https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.7.2-linux-x64.tar.gz\n4. tar -xzvf dorado-0.7.2-linux-x64.tar.gz\n    # after unzipping, the main folder will contain a /bin and /lib folder.\n5. cd bin  \n6. ls\n    # /usr/local/bioinf/dorado/bin/dorado =&gt; this is the Executable file \n7. sudo ln -s /usr/local/bioinf/dorado/bin/dorado /usr/local/bin/dorado\n    # this will symlink the dorado script to /usr/local/bin/dorado\n8. echo 'export PATH=$PATH:/usr/local/bin' | sudo tee -a /etc/profile &amp;&amp; export PATH=$PATH:/usr/local/bin\n    # put in path if not by default, you can check by `echo $PATH`\n</code></pre> Following these instructions you should have downloaded the dorado script and it's libraries in the designated <code>/usr/local/bioinf</code> folder. To make the executable file accessible for all users you need to put it in PATH.</li> </ul>"},{"location":"system-build/#python-environments","title":"Python environments","text":"<p>When you need to install packages using the command <code>pip</code> it is adviced to do so in a separate python environment to avoid issues with the already existing python installation supporting your operating system. Detailed information on python venv can be found here.  </p>"},{"location":"system-build/#conda","title":"Conda","text":"<p>Yet another packaging and environmanagement tool is Conda. A lot of bioinformatic software has been made available in dedicated conda environments. Read all there is to know on how to use conda here. </p>"}]}